import type { QuizQuestion } from '@/lib/db/schema';

export const quizQuestions: QuizQuestion[] = [
  {
    id: 'q-1.1-1',
    lessonId: '1.1',
    question: 'Which best describes the relationship between AI, ML, and deep learning?',
    type: 'multiple-choice',
    options: ['They are three independent fields with no overlap', 'Deep learning is a subset of ML, which is a subset of AI', 'AI is a subset of deep learning', 'ML and deep learning are the same thing'],
    correctAnswer: 'Deep learning is a subset of ML, which is a subset of AI',
    explanation: 'These fields are nested: AI is the broadest (automating intellectual tasks), ML is a specific approach within AI (learning from data), and deep learning is a specific technique within ML (using layered representations).',
    relatedCardIds: ['rc-1.1-1'],
    order: 1,
  },
  {
    id: 'q-1.1-2',
    lessonId: '1.1',
    question: 'In the ML paradigm, a programmer writes explicit rules for the system to follow.',
    type: 'true-false',
    options: ['True', 'False'],
    correctAnswer: 'False',
    explanation: 'The key paradigm shift of ML is that systems learn rules from data (input + expected output), rather than having a programmer manually code the rules. This distinguishes ML from classical symbolic AI.',
    relatedCardIds: ['rc-1.1-2'],
    order: 2,
  },
  {
    id: 'q-1.1-3',
    lessonId: '1.1',
    question: 'Symbolic AI was the dominant paradigm from the 1950s to the 1980s. What was its primary limitation?',
    type: 'multiple-choice',
    options: ['It required too much computing power', 'It could not handle problems requiring handcrafted rules for complex, fuzzy domains', 'It was too slow to train on GPUs', 'It could not store enough data in memory'],
    correctAnswer: 'It could not handle problems requiring handcrafted rules for complex, fuzzy domains',
    explanation: 'Symbolic AI relied on human experts encoding explicit rules. For complex tasks like image recognition or natural language understanding, writing comprehensive rules proved intractable. ML solved this by learning patterns from data instead.',
    relatedCardIds: ['rc-1.1-1', 'rc-1.1-3'],
    order: 3,
  },
  {
    id: 'q-1.2-1',
    lessonId: '1.2',
    question: 'What does the "deep" in deep learning refer to?',
    type: 'multiple-choice',
    options: ['The depth of mathematical understanding required', 'The number of successive layers of representations', 'The size of the training dataset', 'The complexity of the loss function'],
    correctAnswer: 'The number of successive layers of representations',
    explanation: '"Deep" refers to successive layers of representations. Modern deep networks may have tens or hundreds of layers, each transforming data into a slightly more abstract and useful representation.',
    relatedCardIds: ['rc-1.2-1'],
    order: 1,
  },
  {
    id: 'q-1.2-2',
    lessonId: '1.2',
    question: 'A _____ is a different way to look at or encode data, such as representing color as RGB versus HSV.',
    type: 'fill-blank',
    correctAnswer: 'representation',
    explanation: 'A representation is a particular encoding or view of the data. Finding the right representation can make a problem trivially solvable -- in polar coordinates a circle is just a single value (radius).',
    relatedCardIds: ['rc-1.2-2'],
    order: 2,
  },
  {
    id: 'q-1.2-3',
    lessonId: '1.2',
    question: 'What is "information distillation" in deep learning?',
    type: 'multiple-choice',
    options: ['Compressing the training dataset to save memory', 'Each layer refining data into increasingly meaningful representations for the task', 'Removing outliers from the input data', 'Reducing the number of parameters in the model'],
    correctAnswer: 'Each layer refining data into increasingly meaningful representations for the task',
    explanation: 'Information distillation describes how each layer transforms its input into a slightly more useful representation. Raw pixels become edges, edges become textures, textures become parts, and parts become objects.',
    relatedCardIds: ['rc-1.2-1', 'rc-1.2-3'],
    order: 3,
  },
  {
    id: 'q-1.3-1',
    lessonId: '1.3',
    question: 'What is the correct order of steps in a single training loop iteration?',
    type: 'multiple-choice',
    options: ['Backward pass, forward pass, weight update, loss computation', 'Forward pass, loss computation, backward pass, weight update', 'Loss computation, forward pass, backward pass, weight update', 'Weight update, forward pass, backward pass, loss computation'],
    correctAnswer: 'Forward pass, loss computation, backward pass, weight update',
    explanation: 'The training loop: (1) forward pass to compute predictions, (2) loss computation to measure error, (3) backward pass to compute gradients via backpropagation, (4) weight update using the optimizer.',
    relatedCardIds: ['rc-1.3-1'],
    order: 1,
  },
  {
    id: 'q-1.3-2',
    lessonId: '1.3',
    question: 'Self-supervised learning requires manually labeled data.',
    type: 'true-false',
    options: ['True', 'False'],
    correctAnswer: 'False',
    explanation: 'Self-supervised learning generates its own labels from the input data itself, for example by predicting the next word in a sentence. This removes the need for expensive manual labeling and unlocks vastly larger datasets.',
    relatedCardIds: ['rc-1.3-2'],
    order: 2,
  },
  {
    id: 'q-1.3-3',
    lessonId: '1.3',
    question: 'The _____ measures how far the network\'s output is from the expected output.',
    type: 'fill-blank',
    correctAnswer: 'loss function',
    explanation: 'The loss function (also called objective or cost function) quantifies the difference between predicted and expected outputs. It provides the feedback signal the optimizer uses to adjust weights during training.',
    relatedCardIds: ['rc-1.3-3'],
    order: 3,
  },
  {
    id: 'q-1.4-1',
    lessonId: '1.4',
    question: 'What triggered the resurgence of deep learning around 2012?',
    type: 'multiple-choice',
    options: ['The invention of the perceptron algorithm', 'The convergence of large datasets, GPU computing power, and algorithmic improvements', 'A breakthrough in symbolic AI reasoning', 'The creation of the Python programming language'],
    correctAnswer: 'The convergence of large datasets, GPU computing power, and algorithmic improvements',
    explanation: 'The deep learning revolution was enabled by three simultaneous developments: massive datasets (especially ImageNet), powerful GPU hardware for parallel computation, and key algorithmic improvements like ReLU activations and better weight initialization.',
    relatedCardIds: ['rc-1.4-1'],
    order: 1,
  },
  {
    id: 'q-1.4-2',
    lessonId: '1.4',
    question: 'AI has gone through multiple cycles of hype followed by disappointment (AI winters).',
    type: 'true-false',
    options: ['True', 'False'],
    correctAnswer: 'True',
    explanation: 'AI has experienced at least two major "AI winters" -- periods where overpromising led to funding cuts and disillusionment. Understanding this history helps set realistic expectations about current AI capabilities.',
    relatedCardIds: ['rc-1.4-2'],
    order: 2,
  },
  {
    id: 'q-1.4-3',
    lessonId: '1.4',
    question: 'Which event is widely considered the catalyst for the modern deep learning era?',
    type: 'multiple-choice',
    options: ['IBM Deep Blue defeating Kasparov in chess (1997)', 'AlexNet winning the ImageNet competition (2012)', 'The release of TensorFlow (2015)', 'GPT-3 being released (2020)'],
    correctAnswer: 'AlexNet winning the ImageNet competition (2012)',
    explanation: 'AlexNet\'s dramatic victory in the 2012 ImageNet competition demonstrated that deep convolutional networks could vastly outperform traditional computer vision methods, sparking massive interest and investment in deep learning.',
    relatedCardIds: ['rc-1.4-1', 'rc-1.4-3'],
    order: 3,
  },
  {
    id: 'q-1.5-1',
    lessonId: '1.5',
    question: 'In the MNIST task, what shape does the input data have after preprocessing?',
    type: 'multiple-choice',
    options: ['28x28 grayscale images kept as 2D arrays', 'Flattened vectors of 784 values normalized to [0, 1]', '32x32 RGB images', 'One-hot encoded vectors of length 10'],
    correctAnswer: 'Flattened vectors of 784 values normalized to [0, 1]',
    explanation: 'MNIST images are 28x28 pixels. For a simple dense network, they are reshaped into flat vectors of 784 (28*28) values and normalized from [0, 255] to [0, 1] by dividing by 255.',
    relatedCardIds: ['rc-1.5-1'],
    order: 1,
  },
  {
    id: 'q-1.5-2',
    lessonId: '1.5',
    question: 'Why is softmax used in the output layer of MNIST classification?',
    type: 'multiple-choice',
    options: ['It compresses inputs to the range [-1, 1]', 'It produces a probability distribution over the 10 digit classes that sums to 1', 'It speeds up training by normalizing gradients', 'It prevents overfitting by adding noise'],
    correctAnswer: 'It produces a probability distribution over the 10 digit classes that sums to 1',
    explanation: 'Softmax converts the raw logits into a valid probability distribution where all values are positive and sum to 1. Each output represents the model\'s confidence that the input belongs to that class.',
    relatedCardIds: ['rc-1.5-2'],
    order: 2,
  },
  {
    id: 'q-1.5-3',
    lessonId: '1.5',
    question: 'The loss function typically used for multiclass classification with one-hot labels is _____.',
    type: 'fill-blank',
    correctAnswer: 'categorical crossentropy',
    explanation: 'Categorical crossentropy measures the distance between the predicted probability distribution and the true one-hot label. It penalizes confident wrong predictions heavily, making it ideal for multiclass tasks.',
    relatedCardIds: ['rc-1.5-3'],
    order: 3,
  },
  {
    id: 'q-1.6-1',
    lessonId: '1.6',
    question: 'A scalar (single number) is a tensor of how many dimensions?',
    type: 'multiple-choice',
    options: ['0 dimensions (rank-0 tensor)', '1 dimension (rank-1 tensor)', '2 dimensions (rank-2 tensor)', 'Scalars are not tensors'],
    correctAnswer: '0 dimensions (rank-0 tensor)',
    explanation: 'A scalar is a rank-0 tensor with zero axes. A vector is rank-1, a matrix is rank-2. The rank (ndim) of a tensor is the number of axes it has.',
    relatedCardIds: ['rc-1.6-1'],
    order: 1,
  },
  {
    id: 'q-1.6-2',
    lessonId: '1.6',
    question: 'A batch of 32 color images of size 256x256 would be stored as a rank-4 tensor with shape (32, 256, 256, 3).',
    type: 'true-false',
    options: ['True', 'False'],
    correctAnswer: 'True',
    explanation: 'The four axes represent: batch size (32), height (256), width (256), and color channels (3 for RGB). Image batches are one of the most common rank-4 tensors in deep learning.',
    relatedCardIds: ['rc-1.6-2'],
    order: 2,
  },
  {
    id: 'q-1.6-3',
    lessonId: '1.6',
    question: 'The three key attributes that define a tensor are its rank (ndim), its _____, and its dtype.',
    type: 'fill-blank',
    correctAnswer: 'shape',
    explanation: 'Every tensor is characterized by: (1) rank/ndim (number of axes), (2) shape (size along each axis, e.g. (32, 256, 256, 3)), and (3) dtype (data type, typically float32).',
    relatedCardIds: ['rc-1.6-3'],
    order: 3,
  },
  {
    id: 'q-1.7-1',
    lessonId: '1.7',
    question: 'What does "broadcasting" allow in tensor operations?',
    type: 'multiple-choice',
    options: ['Sending tensors across multiple GPUs simultaneously', 'Operating on tensors of different shapes by virtually expanding the smaller tensor', 'Converting tensors from one dtype to another', 'Splitting a tensor into smaller batches'],
    correctAnswer: 'Operating on tensors of different shapes by virtually expanding the smaller tensor',
    explanation: 'Broadcasting allows element-wise operations between tensors of different shapes. The smaller tensor is conceptually repeated (without copying memory) to match the larger. For example, adding a vector (10,) to a matrix (5, 10) broadcasts across all 5 rows.',
    relatedCardIds: ['rc-1.7-1'],
    order: 1,
  },
  {
    id: 'q-1.7-2',
    lessonId: '1.7',
    question: 'Why is data processed in batches rather than one sample at a time?',
    type: 'multiple-choice',
    options: ['Individual samples are too small for GPU memory', 'Batches enable GPU parallelism and provide more stable gradient estimates', 'Batches are required by the Python language', 'Processing one at a time gives identical but unsupported results'],
    correctAnswer: 'Batches enable GPU parallelism and provide more stable gradient estimates',
    explanation: 'Batching serves two purposes: (1) GPUs are massively parallel and most efficient on many samples simultaneously, and (2) averaging gradients over a batch produces more stable updates than using a single noisy sample.',
    relatedCardIds: ['rc-1.7-2'],
    order: 2,
  },
  {
    id: 'q-1.7-3',
    lessonId: '1.7',
    question: 'A dot product between two vectors produces a scalar.',
    type: 'true-false',
    options: ['True', 'False'],
    correctAnswer: 'True',
    explanation: 'The dot product of two vectors sums their element-wise products, producing a single number (scalar). This is distinct from element-wise multiplication which produces another vector of the same length.',
    relatedCardIds: ['rc-1.7-3'],
    order: 3,
  },
  {
    id: 'q-1.8-1',
    lessonId: '1.8',
    question: 'What does the gradient of the loss with respect to a weight tell us?',
    type: 'multiple-choice',
    options: ['The exact value the weight should be set to', 'The direction and magnitude of the steepest increase of the loss', 'Whether the weight should be removed', 'The probability that the weight is correct'],
    correctAnswer: 'The direction and magnitude of the steepest increase of the loss',
    explanation: 'The gradient points in the direction of steepest ascent. To minimize loss, we move weights in the opposite direction (gradient descent). The magnitude tells us how sensitive the loss is to changes in that weight.',
    relatedCardIds: ['rc-1.8-1'],
    order: 1,
  },
  {
    id: 'q-1.8-2',
    lessonId: '1.8',
    question: 'If the learning rate is too large, what typically happens?',
    type: 'multiple-choice',
    options: ['Training converges slowly but reliably', 'The loss oscillates wildly or diverges instead of decreasing', 'The model automatically reduces it', 'The gradients become zero'],
    correctAnswer: 'The loss oscillates wildly or diverges instead of decreasing',
    explanation: 'A learning rate that is too large causes the optimizer to overshoot the minimum, bouncing back and forth across the loss landscape or even diverging. The key is finding a rate small enough for stability but large enough for speed.',
    relatedCardIds: ['rc-1.8-2'],
    order: 2,
  },
  {
    id: 'q-1.8-3',
    lessonId: '1.8',
    question: 'In stochastic gradient descent, the term "stochastic" refers to gradients computed on _____.',
    type: 'fill-blank',
    correctAnswer: 'random mini-batches',
    explanation: 'The word "stochastic" means random. In SGD, instead of computing the gradient over the entire dataset, we estimate it using a randomly sampled mini-batch. This introduces noise but makes training feasible for large datasets.',
    relatedCardIds: ['rc-1.8-3'],
    order: 3,
  },
  {
    id: 'q-1.9-1',
    lessonId: '1.9',
    question: 'Backpropagation computes gradients efficiently by applying which mathematical principle?',
    type: 'multiple-choice',
    options: ['The distributive property', 'The chain rule of calculus', 'Bayes\' theorem', 'The central limit theorem'],
    correctAnswer: 'The chain rule of calculus',
    explanation: 'Backpropagation is the chain rule applied systematically. Since a neural network is a composition of functions, the chain rule decomposes the overall gradient into a product of local gradients computed at each layer.',
    relatedCardIds: ['rc-1.9-1'],
    order: 1,
  },
  {
    id: 'q-1.9-2',
    lessonId: '1.9',
    question: 'During backpropagation, gradients flow from the output layer backward toward the input layer.',
    type: 'true-false',
    options: ['True', 'False'],
    correctAnswer: 'True',
    explanation: 'Backpropagation ("backward propagation of errors") starts at the loss function and propagates gradients backward through the network layer by layer, from output to input -- the reverse of the forward pass.',
    relatedCardIds: ['rc-1.9-2'],
    order: 2,
  },
  {
    id: 'q-1.9-3',
    lessonId: '1.9',
    question: 'Automatic differentiation frameworks record operations on a computational _____ to enable backpropagation.',
    type: 'fill-blank',
    correctAnswer: 'graph',
    explanation: 'Frameworks build a computational graph (or tape) that records all operations on tensors during the forward pass. This graph is then traversed in reverse to compute gradients via the chain rule automatically.',
    relatedCardIds: ['rc-1.9-3'],
    order: 3,
  },
  {
    id: 'q-1.10-1',
    lessonId: '1.10',
    question: 'When reimplementing a dense layer from scratch, what two learnable parameters does each layer maintain?',
    type: 'multiple-choice',
    options: ['Learning rate and momentum', 'Weights (W) and biases (b)', 'Mean and standard deviation', 'Input shape and output shape'],
    correctAnswer: 'Weights (W) and biases (b)',
    explanation: 'A dense layer computes output = activation(dot(input, W) + b). The weight matrix W and bias vector b are the learnable parameters updated during training.',
    relatedCardIds: ['rc-1.10-1'],
    order: 1,
  },
  {
    id: 'q-1.10-2',
    lessonId: '1.10',
    question: 'Implementing a neural network from scratch helps build intuition but is not practical for production models.',
    type: 'true-false',
    options: ['True', 'False'],
    correctAnswer: 'True',
    explanation: 'From-scratch implementations build intuition about forward passes, gradient computation, and weight updates. Production code should use optimized frameworks that handle GPU acceleration, automatic differentiation, and numerical stability.',
    relatedCardIds: ['rc-1.10-2'],
    order: 2,
  },
  {
    id: 'q-1.10-3',
    lessonId: '1.10',
    question: 'Weights are typically initialized with small _____ values rather than zeros to break symmetry.',
    type: 'fill-blank',
    correctAnswer: 'random',
    explanation: 'If all weights start at zero, every neuron computes the same gradient and updates identically, making multiple neurons redundant. Small random values break this symmetry so each neuron learns different features.',
    relatedCardIds: ['rc-1.10-3'],
    order: 3,
  },
  {
    id: 'q-2.1-1',
    lessonId: '2.1',
    question: 'Keras 3 is best described as:',
    type: 'multiple-choice',
    options: ['A standalone deep learning framework', 'A high-level API that works on top of TensorFlow, PyTorch, or JAX', 'A replacement for PyTorch', 'A data preprocessing library'],
    correctAnswer: 'A high-level API that works on top of TensorFlow, PyTorch, or JAX',
    explanation: 'Keras is a unified, user-friendly API that runs on multiple backends, letting you write code once and run it on TensorFlow, PyTorch, or JAX.',
    relatedCardIds: ['rc-2.1-1'],
    order: 1,
  },
  {
    id: 'q-2.1-2',
    lessonId: '2.1',
    question: 'All three major deep learning frameworks share which core capability?',
    type: 'multiple-choice',
    options: ['Graph-only execution', 'Automatic differentiation and GPU-accelerated tensor computation', 'Built-in data labeling tools', 'Exclusive support for computer vision'],
    correctAnswer: 'Automatic differentiation and GPU-accelerated tensor computation',
    explanation: 'TensorFlow, PyTorch, and JAX all provide GPU tensor computation and autodiff. They differ in API design, ecosystem, and deployment tooling.',
    relatedCardIds: ['rc-2.1-2'],
    order: 2,
  },
  {
    id: 'q-2.1-3',
    lessonId: '2.1',
    question: 'In eager execution, operations run _____ when called, like normal Python code.',
    type: 'fill-blank',
    correctAnswer: 'immediately',
    explanation: 'Eager mode runs operations on the spot, enabling interactive debugging. Graph mode compiles operations first, then runs them as a batch.',
    relatedCardIds: ['rc-2.1-3'],
    order: 3,
  },
  {
    id: 'q-2.2-1',
    lessonId: '2.2',
    question: 'The correct order for the three main steps in building a Keras model:',
    type: 'multiple-choice',
    options: ['Compile, Build, Fit', 'Build, Fit, Compile', 'Build, Compile, Fit', 'Fit, Build, Compile'],
    correctAnswer: 'Build, Compile, Fit',
    explanation: 'First build the model (define layers), then compile (specify loss, optimizer, metrics), then fit (train on data).',
    relatedCardIds: ['rc-2.2-1'],
    order: 1,
  },
  {
    id: 'q-2.2-2',
    lessonId: '2.2',
    question: 'For regression predicting house prices, which loss function is standard?',
    type: 'multiple-choice',
    options: ['binary_crossentropy', 'categorical_crossentropy', 'mean_squared_error', 'sparse_categorical_crossentropy'],
    correctAnswer: 'mean_squared_error',
    explanation: 'MSE is the standard loss for regression tasks predicting continuous values. Crossentropy losses are for classification.',
    relatedCardIds: ['rc-2.2-2'],
    order: 2,
  },
  {
    id: 'q-2.2-3',
    lessonId: '2.2',
    question: 'Layers are the _____ of deep learning: they store weights and perform data transformations.',
    type: 'fill-blank',
    correctAnswer: 'building blocks',
    explanation: 'Layers are modular processing units that hold learnable weights and transform input data. Complex models are assembled by composing layers.',
    relatedCardIds: ['rc-2.2-3'],
    order: 3,
  },
  {
    id: 'q-2.3-1',
    lessonId: '2.3',
    question: 'Training accuracy is 99% but validation accuracy is 72%. The likely issue is:',
    type: 'multiple-choice',
    options: ['Underfitting', 'Overfitting', 'Learning rate too small', 'Batch size too large'],
    correctAnswer: 'Overfitting',
    explanation: 'A large gap between training and validation performance is the hallmark of overfitting -- the model has memorized training data but fails to generalize.',
    relatedCardIds: ['rc-2.3-1'],
    order: 1,
  },
  {
    id: 'q-2.3-2',
    lessonId: '2.3',
    question: 'model.fit() with validation_split=0.2 uses what percentage of data for training?',
    type: 'multiple-choice',
    options: ['20%', '100%', '80%', '50%'],
    correctAnswer: '80%',
    explanation: 'The validation_split parameter holds out 20% of the data for validation, leaving 80% for training.',
    relatedCardIds: ['rc-2.3-2'],
    order: 2,
  },
  {
    id: 'q-2.3-3',
    lessonId: '2.3',
    question: 'During _____, some layers like Dropout behave differently than during inference.',
    type: 'fill-blank',
    correctAnswer: 'training',
    explanation: 'Dropout randomly zeroes outputs during training (regularization) but is disabled during inference (prediction). The model.training flag controls this behavior.',
    relatedCardIds: ['rc-2.3-3'],
    order: 3,
  },
  {
    id: 'q-2.4-1',
    lessonId: '2.4',
    question: 'For binary sentiment classification, the output layer should have:',
    type: 'multiple-choice',
    options: ['2 units with softmax', '1 unit with sigmoid', '10 units with softmax', '1 unit with relu'],
    correctAnswer: '1 unit with sigmoid',
    explanation: 'Binary classification uses 1 output unit with sigmoid activation, producing a probability between 0 and 1 for the positive class.',
    relatedCardIds: ['rc-2.4-1'],
    order: 1,
  },
  {
    id: 'q-2.4-2',
    lessonId: '2.4',
    question: 'Multi-hot encoding discards which information about the input text?',
    type: 'multiple-choice',
    options: ['Which words appear', 'The order and frequency of words', 'The sentiment', 'Nothing -- it preserves all information'],
    correctAnswer: 'The order and frequency of words',
    explanation: 'Multi-hot encoding only indicates presence/absence of each word. Word order and how many times a word appears are lost.',
    relatedCardIds: ['rc-2.4-2'],
    order: 2,
  },
  {
    id: 'q-2.4-3',
    lessonId: '2.4',
    question: 'A sigmoid output of 0.87 means the model estimates an 87% probability the review is _____.',
    type: 'fill-blank',
    correctAnswer: 'positive',
    explanation: 'Sigmoid output directly represents the probability of the positive class. Values above 0.5 predict positive; below 0.5 predict negative.',
    relatedCardIds: ['rc-2.4-3'],
    order: 3,
  },
  {
    id: 'q-2.5-1',
    lessonId: '2.5',
    question: 'For classifying newswires into 46 topics, the output layer should have:',
    type: 'multiple-choice',
    options: ['1 unit with sigmoid', '2 units with softmax', '46 units with softmax', '46 units with relu'],
    correctAnswer: '46 units with softmax',
    explanation: 'One output unit per class with softmax activation produces a probability distribution over all 46 classes.',
    relatedCardIds: ['rc-2.5-1'],
    order: 1,
  },
  {
    id: 'q-2.5-2',
    lessonId: '2.5',
    question: 'An intermediate Dense layer with only 4 units in a 46-class classifier creates:',
    type: 'multiple-choice',
    options: ['A regularization effect', 'An information bottleneck that harms performance', 'A speedup in training', 'Better generalization'],
    correctAnswer: 'An information bottleneck that harms performance',
    explanation: '4 units cannot encode enough information to distinguish 46 classes. Information is permanently lost, creating a bottleneck that degrades classification accuracy.',
    relatedCardIds: ['rc-2.5-2'],
    order: 2,
  },
  {
    id: 'q-2.5-3',
    lessonId: '2.5',
    question: 'Softmax outputs for 3 classes [0.2, 0.3, 0.5] must satisfy: each value is between 0 and 1 and they sum to _____.',
    type: 'fill-blank',
    correctAnswer: '1.0',
    explanation: 'Softmax produces a valid probability distribution: all values are non-negative and sum to exactly 1.0.',
    relatedCardIds: ['rc-2.5-3'],
    order: 3,
  },
  {
    id: 'q-2.6-1',
    lessonId: '2.6',
    question: 'For regression predicting house prices, the output layer should have:',
    type: 'multiple-choice',
    options: ['1 unit with sigmoid', '1 unit with no activation function', '10 units with softmax', '1 unit with relu'],
    correctAnswer: '1 unit with no activation function',
    explanation: 'Regression needs unrestricted continuous output. No activation (linear output) allows the network to predict any value.',
    relatedCardIds: ['rc-2.6-1'],
    order: 1,
  },
  {
    id: 'q-2.6-2',
    lessonId: '2.6',
    question: 'Feature normalization is important because:',
    type: 'multiple-choice',
    options: ['It reduces the number of features', 'It puts all features on the same scale, helping gradient descent converge', 'It removes noisy features', 'It increases the dataset size'],
    correctAnswer: 'It puts all features on the same scale, helping gradient descent converge',
    explanation: 'Features with vastly different scales cause inefficient gradient descent. Normalization (mean=0, std=1) equalizes feature contributions.',
    relatedCardIds: ['rc-2.6-2'],
    order: 2,
  },
  {
    id: 'q-2.6-3',
    lessonId: '2.6',
    question: 'In 4-fold cross-validation with 400 samples, each fold has _____ validation samples and the model is trained _____ times.',
    type: 'fill-blank',
    correctAnswer: '100; 4',
    explanation: '400 / 4 = 100 samples per validation fold. The model trains once per fold rotation, so 4 times total. Final score is the average.',
    relatedCardIds: ['rc-2.6-3'],
    order: 3,
  },
  {
    id: 'q-2.7-1',
    lessonId: '2.7',
    question: 'To compute gradients in TensorFlow, you use:',
    type: 'multiple-choice',
    options: ['torch.autograd', 'tf.GradientTape', 'tf.gradient()', 'keras.gradient()'],
    correctAnswer: 'tf.GradientTape',
    explanation: 'GradientTape records operations for automatic differentiation. After the forward pass, call tape.gradient() to compute gradients.',
    relatedCardIds: ['rc-2.7-1'],
    order: 1,
  },
  {
    id: 'q-2.7-2',
    lessonId: '2.7',
    question: 'In PyTorch, after loss.backward(), gradients are stored:',
    type: 'multiple-choice',
    options: ['Returned by backward()', 'In the loss object', 'In each parameter\'s .grad attribute', 'Printed to the console'],
    correctAnswer: 'In each parameter\'s .grad attribute',
    explanation: 'PyTorch accumulates gradients in tensor.grad for each tensor with requires_grad=True. You must zero them before each new backward pass.',
    relatedCardIds: ['rc-2.7-2'],
    order: 2,
  },
  {
    id: 'q-2.7-3',
    lessonId: '2.7',
    question: 'Both TensorFlow and PyTorch provide _____ execution, meaning tensor operations run immediately.',
    type: 'fill-blank',
    correctAnswer: 'eager',
    explanation: 'Both frameworks now support eager mode where operations execute immediately like normal Python, enabling interactive debugging.',
    relatedCardIds: ['rc-2.7-3'],
    order: 3,
  },
  {
    id: 'q-2.8-1',
    lessonId: '2.8',
    question: 'In JAX, jax.grad(loss_fn) returns:',
    type: 'multiple-choice',
    options: ['The gradient values directly', 'A new function that computes gradients of loss_fn', 'The loss value', 'A compiled version of loss_fn'],
    correctAnswer: 'A new function that computes gradients of loss_fn',
    explanation: 'jax.grad is a function transform: it takes a function and returns a new function that computes its gradient. This is a fundamentally different API from TensorFlow or PyTorch.',
    relatedCardIds: ['rc-2.8-1'],
    order: 1,
  },
  {
    id: 'q-2.8-2',
    lessonId: '2.8',
    question: 'JAX arrays are immutable. To modify element 0, you use:',
    type: 'multiple-choice',
    options: ['x[0] = value', 'x.at[0].set(value)', 'jax.modify(x, 0, value)', 'x.update(0, value)'],
    correctAnswer: 'x.at[0].set(value)',
    explanation: 'JAX arrays cannot be modified in place. The .at[].set() syntax returns a new array with the change, preserving immutability for functional purity.',
    relatedCardIds: ['rc-2.8-2'],
    order: 2,
  },
  {
    id: 'q-2.8-3',
    lessonId: '2.8',
    question: 'JAX\'s three key function transformations are jit, grad, and _____.',
    type: 'fill-blank',
    correctAnswer: 'vmap',
    explanation: 'jit (compilation), grad (differentiation), and vmap (vectorization/auto-batching) are JAX\'s composable transforms that can be combined arbitrarily.',
    relatedCardIds: ['rc-2.8-3'],
    order: 3,
  },
  {
    id: 'q-2.9-1',
    lessonId: '2.9',
    question: 'An image can have multiple simultaneous tags (indoor, cat, sunny). What classification type is this?',
    type: 'multiple-choice',
    options: ['Binary classification', 'Multiclass classification', 'Multi-label classification', 'Regression'],
    correctAnswer: 'Multi-label classification',
    explanation: 'Multiple labels can apply simultaneously. Use sigmoid per label with binary crossentropy, not softmax which forces mutually exclusive classes.',
    relatedCardIds: ['rc-2.9-1'],
    order: 1,
  },
  {
    id: 'q-2.9-2',
    lessonId: '2.9',
    question: 'Your regression model\'s loss is very high and will not decrease. The first thing to check:',
    type: 'multiple-choice',
    options: ['Whether features are normalized', 'Whether you have enough GPU memory', 'Whether the batch size is large enough', 'Whether you are using the right programming language'],
    correctAnswer: 'Whether features are normalized',
    explanation: 'Unnormalized features with vastly different scales are the most common cause of regression training failure. Always normalize inputs.',
    relatedCardIds: ['rc-2.9-2'],
    order: 2,
  },
  {
    id: 'q-2.9-3',
    lessonId: '2.9',
    question: 'Match output activations to problem types: sigmoid = binary, softmax = multiclass, no activation = _____.',
    type: 'fill-blank',
    correctAnswer: 'regression',
    explanation: 'Each problem type has a standard activation for the output layer. Regression uses no activation (linear output) to allow unrestricted continuous predictions.',
    relatedCardIds: ['rc-2.9-3'],
    order: 3,
  },
  {
    id: 'q-2.10-1',
    lessonId: '2.10',
    question: 'The test set should be used:',
    type: 'multiple-choice',
    options: ['During training to monitor progress', 'To tune hyperparameters', 'Only once, for final evaluation after all tuning is done', 'Before training to select architecture'],
    correctAnswer: 'Only once, for final evaluation after all tuning is done',
    explanation: 'The test set provides an unbiased final performance estimate. Using it during development would cause indirect overfitting to the test data.',
    relatedCardIds: ['rc-2.10-1'],
    order: 1,
  },
  {
    id: 'q-2.10-2',
    lessonId: '2.10',
    question: 'Which is NOT a common preprocessing step?',
    type: 'multiple-choice',
    options: ['Normalizing features to mean=0, std=1', 'Converting labels to the format expected by the loss function', 'Deleting 50% of training data to speed up training', 'Reshaping inputs to match model expectations'],
    correctAnswer: 'Deleting 50% of training data to speed up training',
    explanation: 'More data generally helps. Normalizing, reshaping, and formatting labels are all standard preprocessing steps.',
    relatedCardIds: ['rc-2.10-2'],
    order: 2,
  },
  {
    id: 'q-2.10-3',
    lessonId: '2.10',
    question: 'ML development is _____ -- you train, evaluate, adjust, and repeat.',
    type: 'fill-blank',
    correctAnswer: 'iterative',
    explanation: 'ML development is a loop of experimentation. Rarely does the first attempt work perfectly; success comes from systematic iteration.',
    relatedCardIds: ['rc-2.10-3'],
    order: 3,
  },
  {
    id: 'q-3.1-1',
    lessonId: '3.1',
    question: 'Overfitting occurs when:',
    type: 'multiple-choice',
    options: ['Training loss and validation loss are both high', 'Training loss is low but validation loss is much higher', 'Both losses are low and similar', 'Validation loss is lower than training loss'],
    correctAnswer: 'Training loss is low but validation loss is much higher',
    explanation: 'A large gap between low training loss and high validation loss shows the model has memorized training data but fails on unseen data.',
    relatedCardIds: ['rc-3.1-1'],
    order: 1,
  },
  {
    id: 'q-3.1-2',
    lessonId: '3.1',
    question: 'Which is an example of a spurious correlation a model might learn?',
    type: 'multiple-choice',
    options: ['Edges are useful for digit classification', 'A rare word appearing once in a negative review predicts negative sentiment', '28x28 pixel images represent digits', 'Cats and dogs have different shapes'],
    correctAnswer: 'A rare word appearing once in a negative review predicts negative sentiment',
    explanation: 'One occurrence provides no statistical basis. The model memorizes a coincidence that will not generalize to new data.',
    relatedCardIds: ['rc-3.1-2'],
    order: 2,
  },
  {
    id: 'q-3.1-3',
    lessonId: '3.1',
    question: 'You cannot directly optimize _____; you can only fit the model to training data and hope it follows.',
    type: 'fill-blank',
    correctAnswer: 'generalization',
    explanation: 'Generalization (performance on unseen data) is the real goal, but we can only directly optimize training performance. The gap between them is the central challenge of ML.',
    relatedCardIds: ['rc-3.1-3'],
    order: 3,
  },
  {
    id: 'q-3.2-1',
    lessonId: '3.2',
    question: 'The manifold hypothesis suggests that:',
    type: 'multiple-choice',
    options: ['All pixel combinations are valid images', 'Real-world data occupies a low-dimensional surface within high-dimensional space', 'Deep learning models always extrapolate well', 'More parameters always mean more overfitting'],
    correctAnswer: 'Real-world data occupies a low-dimensional surface within high-dimensional space',
    explanation: 'Not all possible inputs are valid data. Real data has structure that lies on lower-dimensional manifolds within the full space of possibilities.',
    relatedCardIds: ['rc-3.2-1'],
    order: 1,
  },
  {
    id: 'q-3.2-2',
    lessonId: '3.2',
    question: 'Deep learning models generalize well partly because gradient descent acts as:',
    type: 'multiple-choice',
    options: ['A data augmentation technique', 'An implicit regularizer biasing toward smooth solutions', 'A feature engineering tool', 'A hyperparameter optimizer'],
    correctAnswer: 'An implicit regularizer biasing toward smooth solutions',
    explanation: 'Gradient descent naturally finds simple, smooth solutions rather than arbitrary complex ones, providing implicit regularization even without explicit techniques.',
    relatedCardIds: ['rc-3.2-2'],
    order: 2,
  },
  {
    id: 'q-3.2-3',
    lessonId: '3.2',
    question: 'ML models generally _____ well (within training distribution) but _____ poorly (outside it).',
    type: 'fill-blank',
    correctAnswer: 'interpolate; extrapolate',
    explanation: 'Models handle variations within the training data distribution (interpolation) but fail on fundamentally new inputs outside that distribution (extrapolation).',
    relatedCardIds: ['rc-3.2-3'],
    order: 3,
  },
  {
    id: 'q-3.3-1',
    lessonId: '3.3',
    question: 'Information leakage occurs when:',
    type: 'multiple-choice',
    options: ['The model is too large', 'Test data information influences training or hyperparameter tuning', 'Training takes too many epochs', 'The validation set is too large'],
    correctAnswer: 'Test data information influences training or hyperparameter tuning',
    explanation: 'Any use of test data during development corrupts the evaluation. Keep test data completely separate until final evaluation.',
    relatedCardIds: ['rc-3.3-1'],
    order: 1,
  },
  {
    id: 'q-3.3-2',
    lessonId: '3.3',
    question: 'For temporal data (like stock prices), the correct split method is:',
    type: 'multiple-choice',
    options: ['Random shuffling into train/val/test', 'Chronological: train on earlier data, validate/test on later data', 'Only K-fold cross-validation', 'Use the entire dataset for both training and testing'],
    correctAnswer: 'Chronological: train on earlier data, validate/test on later data',
    explanation: 'Random shuffling creates temporal leakage -- the model would see future data during training. Chronological splits maintain the real-world constraint.',
    relatedCardIds: ['rc-3.3-2'],
    order: 2,
  },
  {
    id: 'q-3.3-3',
    lessonId: '3.3',
    question: 'A common-sense baseline for predicting tomorrow\'s temperature is to predict it equals _____ temperature.',
    type: 'fill-blank',
    correctAnswer: 'today\'s',
    explanation: 'This persistence baseline is surprisingly hard to beat due to temperature autocorrelation. Always establish baselines before building complex models.',
    relatedCardIds: ['rc-3.3-3'],
    order: 3,
  },
  {
    id: 'q-3.4-1',
    lessonId: '3.4',
    question: 'If your model cannot fit the training data, the first step is:',
    type: 'multiple-choice',
    options: ['Add dropout', 'Increase model capacity or fix the architecture', 'Get more training data', 'Reduce the learning rate further'],
    correctAnswer: 'Increase model capacity or fix the architecture',
    explanation: 'Underfitting means the model lacks capacity to learn the patterns. Adding regularization (dropout) would make it even worse.',
    relatedCardIds: ['rc-3.4-1'],
    order: 1,
  },
  {
    id: 'q-3.4-2',
    lessonId: '3.4',
    question: '"Architecture priors" refers to:',
    type: 'multiple-choice',
    options: ['Running a model before training', 'Encoding domain knowledge into the model structure', 'Using weights from a previous model', 'Preprocessing data before feeding it to the model'],
    correctAnswer: 'Encoding domain knowledge into the model structure',
    explanation: 'Example: convolutions encode translation invariance for images, reducing the hypothesis space to patterns that make sense for the domain.',
    relatedCardIds: ['rc-3.4-2'],
    order: 2,
  },
  {
    id: 'q-3.4-3',
    lessonId: '3.4',
    question: 'The recommended strategy is to first _____, then regularize to close the generalization gap.',
    type: 'fill-blank',
    correctAnswer: 'overfit',
    explanation: 'First prove the model can learn by achieving near-zero training loss (overfit), then add regularization techniques to improve validation performance.',
    relatedCardIds: ['rc-3.4-3'],
    order: 3,
  },
  {
    id: 'q-3.5-1',
    lessonId: '3.5',
    question: 'Dropout(0.5) during training means:',
    type: 'multiple-choice',
    options: ['50% of training data is dropped', '50% of layer activations are randomly set to zero each step', 'Learning rate is halved', '50% of layers are removed'],
    correctAnswer: '50% of layer activations are randomly set to zero each step',
    explanation: 'Dropout randomly zeroes a fraction of unit outputs during each training step, forcing the network to learn redundant representations.',
    relatedCardIds: ['rc-3.5-1'],
    order: 1,
  },
  {
    id: 'q-3.5-2',
    lessonId: '3.5',
    question: 'Which technique stops training when validation performance stops improving?',
    type: 'multiple-choice',
    options: ['Dropout', 'L2 regularization', 'Early stopping', 'Data augmentation'],
    correctAnswer: 'Early stopping',
    explanation: 'Early stopping monitors validation loss (or another metric) and halts training when it begins increasing, preventing overfitting.',
    relatedCardIds: ['rc-3.5-2'],
    order: 2,
  },
  {
    id: 'q-3.5-3',
    lessonId: '3.5',
    question: 'L2 regularization pushes weight values toward _____, producing smoother decision boundaries.',
    type: 'fill-blank',
    correctAnswer: 'zero',
    explanation: 'L2 penalizes large weights by adding their squared magnitude to the loss. This produces smaller, more distributed weights and smoother, simpler model behavior.',
    relatedCardIds: ['rc-3.5-3'],
    order: 3,
  },
  {
    id: 'q-3.6-1',
    lessonId: '3.6',
    question: 'For 98% class A and 2% class B, the best evaluation metric is:',
    type: 'multiple-choice',
    options: ['Accuracy', 'Precision, recall, or AUC', 'Mean squared error', 'Number of epochs'],
    correctAnswer: 'Precision, recall, or AUC',
    explanation: 'With severe class imbalance, accuracy is misleading. A model always predicting class A gets 98% accuracy but is useless. Precision, recall, and AUC are more informative.',
    relatedCardIds: ['rc-3.6-1'],
    order: 1,
  },
  {
    id: 'q-3.6-2',
    lessonId: '3.6',
    question: 'The first step of the universal ML workflow is:',
    type: 'multiple-choice',
    options: ['Build a model', 'Choose a framework', 'Define the task: frame the problem, understand data, choose a success metric', 'Collect as much data as possible'],
    correctAnswer: 'Define the task: frame the problem, understand data, choose a success metric',
    explanation: 'Before any modeling, you must understand what you are trying to solve, explore the data, and define what success means.',
    relatedCardIds: ['rc-3.6-2'],
    order: 2,
  },
  {
    id: 'q-3.6-3',
    lessonId: '3.6',
    question: 'Before writing model code, you should _____ your data to identify potential issues.',
    type: 'fill-blank',
    correctAnswer: 'explore',
    explanation: 'Data exploration reveals class imbalances, missing values, outliers, and other issues that affect modeling decisions.',
    relatedCardIds: ['rc-3.6-3'],
    order: 3,
  },
  {
    id: 'q-3.7-1',
    lessonId: '3.7',
    question: 'The correct model development order is:',
    type: 'multiple-choice',
    options: ['Regularize first, then build a big model, then beat baseline', 'Beat baseline, then scale up to overfit, then regularize and tune', 'Scale up first, then beat baseline, then regularize', 'Regularize first, then beat baseline, then deploy'],
    correctAnswer: 'Beat baseline, then scale up to overfit, then regularize and tune',
    explanation: 'First prove value (beat baseline), then prove capacity (overfit), then improve generalization (regularize and tune).',
    relatedCardIds: ['rc-3.7-1'],
    order: 1,
  },
  {
    id: 'q-3.7-2',
    lessonId: '3.7',
    question: 'Training loss near zero but high validation loss means:',
    type: 'multiple-choice',
    options: ['Ready to deploy', 'The model has enough capacity -- now regularize to close the gap', 'Underfitting', 'Learning rate is too high'],
    correctAnswer: 'The model has enough capacity -- now regularize to close the gap',
    explanation: 'Near-zero training loss proves the model can learn. The gap with validation loss must be closed with regularization techniques.',
    relatedCardIds: ['rc-3.7-2'],
    order: 2,
  },
  {
    id: 'q-3.7-3',
    lessonId: '3.7',
    question: 'If your model cannot beat a _____ baseline, the problem is likely in data prep or problem formulation.',
    type: 'fill-blank',
    correctAnswer: 'common-sense',
    explanation: 'If a trivial prediction (like always guessing the most common class) beats your model, the issue is not model capacity but something more fundamental.',
    relatedCardIds: ['rc-3.7-3'],
    order: 3,
  },
  {
    id: 'q-3.8-1',
    lessonId: '3.8',
    question: 'Distribution shift in production means:',
    type: 'multiple-choice',
    options: ['The model was trained on too much data', 'Real-world data has diverged from the training data distribution', 'Model weights were accidentally modified', 'The learning rate was wrong during training'],
    correctAnswer: 'Real-world data has diverged from the training data distribution',
    explanation: 'Production data changes over time. A model trained on 2023 data may perform poorly on 2025 data due to shifting patterns.',
    relatedCardIds: ['rc-3.8-1'],
    order: 1,
  },
  {
    id: 'q-3.8-2',
    lessonId: '3.8',
    question: 'After deploying a model, you should:',
    type: 'multiple-choice',
    options: ['Never change it once deployed', 'Monitor continuously and retrain when performance degrades', 'Remove the test set', 'Increase the learning rate'],
    correctAnswer: 'Monitor continuously and retrain when performance degrades',
    explanation: 'Ongoing monitoring catches distribution shift and concept drift. Periodic retraining on fresh data maintains model quality.',
    relatedCardIds: ['rc-3.8-2'],
    order: 2,
  },
  {
    id: 'q-3.8-3',
    lessonId: '3.8',
    question: 'The deployed version of a model, optimized for making predictions, is called the _____ model.',
    type: 'fill-blank',
    correctAnswer: 'inference',
    explanation: 'The inference model is the exported, optimized version used for production predictions, often with optimizations like quantization or graph compilation.',
    relatedCardIds: ['rc-3.8-3'],
    order: 3,
  },
  {
    id: 'q-3.9-1',
    lessonId: '3.9',
    question: 'L1 regularization is preferred over L2 when you want:',
    type: 'multiple-choice',
    options: ['All features to contribute equally', 'Automatic feature selection through weight sparsity', 'Faster training speed', 'Larger model capacity'],
    correctAnswer: 'Automatic feature selection through weight sparsity',
    explanation: 'L1 drives some weights to exactly zero, effectively removing irrelevant features. L2 shrinks weights but rarely makes them exactly zero.',
    relatedCardIds: ['rc-3.9-1'],
    order: 1,
  },
  {
    id: 'q-3.9-2',
    lessonId: '3.9',
    question: 'Data augmentation helps generalization by:',
    type: 'multiple-choice',
    options: ['Collecting more real-world data', 'Increasing training diversity through random transformations of existing data', 'Removing noisy examples from the dataset', 'Reducing model capacity'],
    correctAnswer: 'Increasing training diversity through random transformations of existing data',
    explanation: 'Random transformations (flips, rotations, crops) artificially diversify training data, forcing the model to learn invariant features rather than memorizing specifics.',
    relatedCardIds: ['rc-3.9-2'],
    order: 2,
  },
  {
    id: 'q-3.9-3',
    lessonId: '3.9',
    question: 'Too much regularization causes _____, too little causes _____.',
    type: 'fill-blank',
    correctAnswer: 'underfitting; overfitting',
    explanation: 'Regularization constrains the model. Too much prevents it from learning (underfitting); too little allows it to memorize training data (overfitting). The goal is the right balance.',
    relatedCardIds: ['rc-3.9-3'],
    order: 3,
  },
  {
    id: 'q-3.10-1',
    lessonId: '3.10',
    question: 'The universal ML workflow ends with:',
    type: 'multiple-choice',
    options: ['Training the model', 'Evaluating on the test set', 'Deploying, monitoring, and maintaining the model', 'Publishing results'],
    correctAnswer: 'Deploying, monitoring, and maintaining the model',
    explanation: 'The workflow extends beyond evaluation: deploy the model, monitor performance in production, and maintain it over time with retraining.',
    relatedCardIds: ['rc-3.10-1'],
    order: 1,
  },
  {
    id: 'q-3.10-2',
    lessonId: '3.10',
    question: 'If both training and validation loss are high and not improving:',
    type: 'multiple-choice',
    options: ['Overfitting', 'Underfitting', 'Test set too small', 'Learning rate too high'],
    correctAnswer: 'Underfitting',
    explanation: 'High and flat losses on both sets indicate the model cannot learn the patterns -- it needs more capacity, better architecture, or fixed data.',
    relatedCardIds: ['rc-3.10-2'],
    order: 2,
  },
  {
    id: 'q-3.10-3',
    lessonId: '3.10',
    question: 'The single most diagnostic tool for ML development is plotting _____ vs _____ loss curves over epochs.',
    type: 'fill-blank',
    correctAnswer: 'training; validation',
    explanation: 'These curves reveal learning progress, overfitting onset, optimal training duration, and whether you need more capacity or more regularization.',
    relatedCardIds: ['rc-3.10-3'],
    order: 3,
  },
  {
    id: 'q-4.1-1',
    lessonId: '4.1',
    question: 'The Functional API is necessary when your model has:',
    type: 'multiple-choice',
    options: ['More than 10 layers', 'Multiple inputs, multiple outputs, or non-linear topology', 'More than 1 million parameters', 'GPU requirements'],
    correctAnswer: 'Multiple inputs, multiple outputs, or non-linear topology',
    explanation: 'The Sequential API only handles linear stacks of layers. The Functional API can build any directed acyclic graph (DAG) topology.',
    relatedCardIds: ['rc-4.1-1'],
    order: 1,
  },
  {
    id: 'q-4.1-2',
    lessonId: '4.1',
    question: 'The Sequential API can represent a model with skip connections.',
    type: 'true-false',
    options: ['True', 'False'],
    correctAnswer: 'False',
    explanation: 'Skip connections create a non-linear topology (a layer receives input from a non-adjacent layer). This requires the Functional API or subclassing, not Sequential.',
    relatedCardIds: ['rc-4.1-2'],
    order: 2,
  },
  {
    id: 'q-4.1-3',
    lessonId: '4.1',
    question: 'The Functional API builds models as a directed acyclic _____ of layers.',
    type: 'fill-blank',
    correctAnswer: 'graph',
    explanation: 'Layers are nodes and tensor flows between them are edges, forming a DAG. This representation also enables Keras to inspect and visualize the model topology.',
    relatedCardIds: ['rc-4.1-3'],
    order: 3,
  },
  {
    id: 'q-4.2-1',
    lessonId: '4.2',
    question: 'The main advantage of model subclassing over the Functional API is:',
    type: 'multiple-choice',
    options: ['Better performance at inference time', 'Maximum flexibility -- arbitrary Python control flow in the forward pass', 'Easier debugging with plot_model()', 'Smaller model files'],
    correctAnswer: 'Maximum flexibility -- arbitrary Python control flow in the forward pass',
    explanation: 'Subclassing lets you use if-else statements, loops, and any Python code in the forward pass, at the cost of losing some inspectability.',
    relatedCardIds: ['rc-4.2-1'],
    order: 1,
  },
  {
    id: 'q-4.2-2',
    lessonId: '4.2',
    question: 'Subclassed models can be fully inspected and serialized as easily as Functional API models.',
    type: 'true-false',
    options: ['True', 'False'],
    correctAnswer: 'False',
    explanation: 'Subclassed models are opaque to Keras because the forward pass is arbitrary Python code. They lose features like plot_model(), layer graph inspection, and some serialization capabilities.',
    relatedCardIds: ['rc-4.2-2'],
    order: 2,
  },
  {
    id: 'q-4.2-3',
    lessonId: '4.2',
    question: 'Which approach would you use for a model that conditionally routes inputs through different layers based on the input?',
    type: 'multiple-choice',
    options: ['Sequential API', 'Functional API', 'Model subclassing', 'None of the above'],
    correctAnswer: 'Model subclassing',
    explanation: 'Conditional routing (if-else on input values) requires arbitrary Python control flow, which only subclassing supports.',
    relatedCardIds: ['rc-4.2-3'],
    order: 3,
  },
  {
    id: 'q-4.3-1',
    lessonId: '4.3',
    question: 'To automatically stop training when validation loss stops improving, use:',
    type: 'multiple-choice',
    options: ['ModelCheckpoint', 'EarlyStopping', 'ReduceLROnPlateau', 'TensorBoard'],
    correctAnswer: 'EarlyStopping',
    explanation: 'EarlyStopping monitors a metric (typically val_loss) and halts training after a specified number of epochs without improvement (patience).',
    relatedCardIds: ['rc-4.3-1'],
    order: 1,
  },
  {
    id: 'q-4.3-2',
    lessonId: '4.3',
    question: 'TensorBoard is used to:',
    type: 'multiple-choice',
    options: ['Compile models faster', 'Visualize training metrics, model graphs, and embeddings in a web dashboard', 'Replace the optimizer', 'Increase batch size'],
    correctAnswer: 'Visualize training metrics, model graphs, and embeddings in a web dashboard',
    explanation: 'TensorBoard provides interactive visualization of loss curves, metrics, computation graphs, and more, helping diagnose training issues.',
    relatedCardIds: ['rc-4.3-2'],
    order: 2,
  },
  {
    id: 'q-4.3-3',
    lessonId: '4.3',
    question: 'Callbacks execute custom logic at various points during _____, such as at the end of each epoch.',
    type: 'fill-blank',
    correctAnswer: 'training',
    explanation: 'Callbacks hook into the training loop at epoch boundaries, batch boundaries, and other events, enabling custom behavior without modifying fit().',
    relatedCardIds: ['rc-4.3-3'],
    order: 3,
  },
  {
    id: 'q-4.4-1',
    lessonId: '4.4',
    question: 'In a custom training loop, which step is NOT handled automatically (unlike model.fit)?',
    type: 'multiple-choice',
    options: ['Forward pass', 'Gradient computation', 'Manually zeroing gradients, updating weights, and logging metrics', 'Loss calculation'],
    correctAnswer: 'Manually zeroing gradients, updating weights, and logging metrics',
    explanation: 'model.fit() automates the entire training loop. Custom loops require you to manually handle gradient zeroing, optimizer.apply_gradients(), and metric tracking.',
    relatedCardIds: ['rc-4.4-1'],
    order: 1,
  },
  {
    id: 'q-4.4-2',
    lessonId: '4.4',
    question: 'During inference, Dropout layers are completely disabled and all units are active.',
    type: 'true-false',
    options: ['True', 'False'],
    correctAnswer: 'True',
    explanation: 'Dropout is a training-only regularization technique. At inference time, all units are active (but scaled appropriately to maintain expected output magnitudes).',
    relatedCardIds: ['rc-4.4-2'],
    order: 2,
  },
  {
    id: 'q-4.4-3',
    lessonId: '4.4',
    question: 'tf.GradientTape is used in custom training loops to:',
    type: 'multiple-choice',
    options: ['Load data from disk', 'Record operations for computing gradients via backpropagation', 'Visualize model architecture', 'Save model checkpoints'],
    correctAnswer: 'Record operations for computing gradients via backpropagation',
    explanation: 'GradientTape records the forward pass operations so that tape.gradient() can compute gradients of the loss with respect to trainable variables.',
    relatedCardIds: ['rc-4.4-3'],
    order: 3,
  },
  {
    id: 'q-4.5-1',
    lessonId: '4.5',
    question: 'ConvNets are better than Dense networks for images because:',
    type: 'multiple-choice',
    options: ['They have more parameters', 'They learn translation-invariant local patterns and build spatial hierarchies', 'They use more GPU memory', 'They do not require activation functions'],
    correctAnswer: 'They learn translation-invariant local patterns and build spatial hierarchies',
    explanation: 'Convolutions detect local patterns (edges, textures) that work anywhere in the image, and deeper layers combine these into increasingly complex features (parts, objects).',
    relatedCardIds: ['rc-4.5-1'],
    order: 1,
  },
  {
    id: 'q-4.5-2',
    lessonId: '4.5',
    question: 'A convolution filter learned in one part of an image can detect the same pattern in any other part.',
    type: 'true-false',
    options: ['True', 'False'],
    correctAnswer: 'True',
    explanation: 'This is translation invariance -- one of the key properties of convolutions. A filter trained to detect vertical edges works regardless of position in the image.',
    relatedCardIds: ['rc-4.5-2'],
    order: 2,
  },
  {
    id: 'q-4.5-3',
    lessonId: '4.5',
    question: 'In a ConvNet, early layers learn simple patterns like _____, while deeper layers learn complex patterns like _____.',
    type: 'fill-blank',
    correctAnswer: 'edges; objects',
    explanation: 'The spatial hierarchy: edges and textures in early layers combine into parts and whole objects in deeper layers. This hierarchical representation is what makes ConvNets powerful.',
    relatedCardIds: ['rc-4.5-3'],
    order: 3,
  },
  {
    id: 'q-4.6-1',
    lessonId: '4.6',
    question: 'MaxPooling2D(pool_size=2) on a (16, 16, 32) feature map produces:',
    type: 'multiple-choice',
    options: ['(16, 16, 16)', '(8, 8, 32)', '(8, 8, 16)', '(16, 16, 32)'],
    correctAnswer: '(8, 8, 32)',
    explanation: 'pool_size=2 halves each spatial dimension (16->8). The depth/channels (32) are unchanged. MaxPooling downsamples spatially while preserving feature depth.',
    relatedCardIds: ['rc-4.6-1'],
    order: 1,
  },
  {
    id: 'q-4.6-2',
    lessonId: '4.6',
    question: 'The purpose of max pooling in a ConvNet is to:',
    type: 'multiple-choice',
    options: ['Add more parameters for learning', 'Progressively downsample feature maps to reduce computation and increase receptive field', 'Increase image resolution', 'Normalize feature values'],
    correctAnswer: 'Progressively downsample feature maps to reduce computation and increase receptive field',
    explanation: 'Max pooling reduces spatial dimensions, lowering computational cost while expanding each neuron\'s effective receptive field over the original input.',
    relatedCardIds: ['rc-4.6-2'],
    order: 2,
  },
  {
    id: 'q-4.6-3',
    lessonId: '4.6',
    question: 'A typical ConvNet architecture alternates between Conv2D layers and pooling layers, ending with Dense layers for classification.',
    type: 'true-false',
    options: ['True', 'False'],
    correctAnswer: 'True',
    explanation: 'The standard pattern is: (Conv2D + Pooling) repeated several times to extract features, followed by Flatten and Dense layers to perform the final classification.',
    relatedCardIds: ['rc-4.6-3'],
    order: 3,
  },
  {
    id: 'q-4.7-1',
    lessonId: '4.7',
    question: 'When training a ConvNet on a small dataset (e.g., 2000 images), the biggest risk is:',
    type: 'multiple-choice',
    options: ['Underfitting due to too few parameters', 'Overfitting due to limited data diversity', 'Running out of GPU memory', 'Training taking too long'],
    correctAnswer: 'Overfitting due to limited data diversity',
    explanation: 'Small datasets provide limited diversity. The model can easily memorize the specific training examples rather than learning generalizable patterns.',
    relatedCardIds: ['rc-4.7-1'],
    order: 1,
  },
  {
    id: 'q-4.7-2',
    lessonId: '4.7',
    question: 'Data augmentation helps small-data ConvNets by:',
    type: 'multiple-choice',
    options: ['Downloading more images from the internet', 'Generating transformed versions of existing images to increase training diversity', 'Reducing the model size', 'Removing duplicate images'],
    correctAnswer: 'Generating transformed versions of existing images to increase training diversity',
    explanation: 'Augmentation creates flipped, rotated, cropped, and color-shifted versions of training images, providing more varied examples without collecting new data.',
    relatedCardIds: ['rc-4.7-2'],
    order: 2,
  },
  {
    id: 'q-4.7-3',
    lessonId: '4.7',
    question: 'Data augmentation is a form of _____ that increases training diversity without collecting new data.',
    type: 'fill-blank',
    correctAnswer: 'regularization',
    explanation: 'Augmentation prevents memorization by presenting different views of each training example, acting as an input-level regularizer.',
    relatedCardIds: ['rc-4.7-3'],
    order: 3,
  },
  {
    id: 'q-4.8-1',
    lessonId: '4.8',
    question: 'Transfer learning with feature extraction means:',
    type: 'multiple-choice',
    options: ['Training a model from scratch with more data', 'Freezing a pretrained convolutional base and training only a new classifier head', 'Transferring data between different datasets', 'Using the same architecture with different hyperparameters'],
    correctAnswer: 'Freezing a pretrained convolutional base and training only a new classifier head',
    explanation: 'The pretrained convolutional base (e.g., from ImageNet) extracts general visual features. Only the new classifier on top is trained for your specific task.',
    relatedCardIds: ['rc-4.8-1'],
    order: 1,
  },
  {
    id: 'q-4.8-2',
    lessonId: '4.8',
    question: 'Why does transfer learning work well for image tasks?',
    type: 'multiple-choice',
    options: ['All images are the same size', 'Early ConvNet layers learn universal visual features (edges, textures) useful across tasks', 'Pretrained models are always better', 'It requires no training at all'],
    correctAnswer: 'Early ConvNet layers learn universal visual features (edges, textures) useful across tasks',
    explanation: 'Lower layers learn generic features applicable to many vision tasks. Only the higher-level, task-specific features need retraining.',
    relatedCardIds: ['rc-4.8-2'],
    order: 2,
  },
  {
    id: 'q-4.8-3',
    lessonId: '4.8',
    question: 'Transfer learning is most beneficial when you have a small dataset for your target task.',
    type: 'true-false',
    options: ['True', 'False'],
    correctAnswer: 'True',
    explanation: 'With limited data, training from scratch is prone to overfitting. A pretrained base provides a strong feature extractor, requiring far fewer examples to train the classifier head.',
    relatedCardIds: ['rc-4.8-3'],
    order: 3,
  },
  {
    id: 'q-4.9-1',
    lessonId: '4.9',
    question: 'Fine-tuning differs from feature extraction in that:',
    type: 'multiple-choice',
    options: ['Fine-tuning freezes all layers', 'Fine-tuning unfreezes some top layers of the pretrained base and retrains them with a low learning rate', 'Fine-tuning only changes the learning rate', 'Fine-tuning removes the pretrained base entirely'],
    correctAnswer: 'Fine-tuning unfreezes some top layers of the pretrained base and retrains them with a low learning rate',
    explanation: 'Feature extraction keeps the entire base frozen. Fine-tuning unfreezes the top layers to adapt higher-level features to the new task, using a small learning rate to avoid destroying pretrained knowledge.',
    relatedCardIds: ['rc-4.9-1'],
    order: 1,
  },
  {
    id: 'q-4.9-2',
    lessonId: '4.9',
    question: 'When fine-tuning, you should use the same high learning rate as training from scratch.',
    type: 'true-false',
    options: ['True', 'False'],
    correctAnswer: 'False',
    explanation: 'Fine-tuning requires a much lower learning rate (typically 10-100x smaller) to make small adjustments without destroying the valuable pretrained weights.',
    relatedCardIds: ['rc-4.9-2'],
    order: 2,
  },
  {
    id: 'q-4.9-3',
    lessonId: '4.9',
    question: 'Fine-tuning should only be done after the new _____ head has been trained with the base frozen.',
    type: 'fill-blank',
    correctAnswer: 'classifier',
    explanation: 'If you fine-tune immediately, the randomly initialized classifier head produces large random gradients that would corrupt the pretrained weights. Train the head first, then fine-tune.',
    relatedCardIds: ['rc-4.9-3'],
    order: 3,
  },
  {
    id: 'q-4.10-1',
    lessonId: '4.10',
    question: 'Residual connections solve the problem of:',
    type: 'multiple-choice',
    options: ['Slow data loading', 'Vanishing gradients in very deep networks', 'Large model file sizes', 'Overfitting on small datasets'],
    correctAnswer: 'Vanishing gradients in very deep networks',
    explanation: 'Skip connections add the layer input to its output (y = f(x) + x), ensuring gradient signal flows even through many layers, preventing vanishing gradients.',
    relatedCardIds: ['rc-4.10-1'],
    order: 1,
  },
  {
    id: 'q-4.10-2',
    lessonId: '4.10',
    question: 'A residual block computes y = f(x) + x. If f(x) learns to output all zeros, what does the block output?',
    type: 'multiple-choice',
    options: ['Zero', 'x (the identity)', 'An error', 'Undefined'],
    correctAnswer: 'x (the identity)',
    explanation: 'If f(x) = 0, then y = 0 + x = x. This means the worst case is the identity function, ensuring the block never degrades the signal. The network can always "skip" unhelpful layers.',
    relatedCardIds: ['rc-4.10-2'],
    order: 2,
  },
  {
    id: 'q-4.10-3',
    lessonId: '4.10',
    question: 'Without residual connections, networks deeper than about 20 layers typically perform worse than shallower ones.',
    type: 'true-false',
    options: ['True', 'False'],
    correctAnswer: 'True',
    explanation: 'Vanishing gradients make very deep plain networks nearly untrainable. Residual connections (introduced by ResNet) solved this, enabling networks with 100+ layers.',
    relatedCardIds: ['rc-4.10-3'],
    order: 3,
  },
  {
    id: 'q-4.11-1',
    lessonId: '4.11',
    question: 'Batch normalization helps training by:',
    type: 'multiple-choice',
    options: ['Increasing the batch size', 'Normalizing layer inputs to have zero mean and unit variance, stabilizing training', 'Removing outliers from the data', 'Reducing the number of parameters'],
    correctAnswer: 'Normalizing layer inputs to have zero mean and unit variance, stabilizing training',
    explanation: 'BatchNorm normalizes the output of each layer, reducing internal covariate shift and allowing higher learning rates for faster, more stable training.',
    relatedCardIds: ['rc-4.11-1'],
    order: 1,
  },
  {
    id: 'q-4.11-2',
    lessonId: '4.11',
    question: 'SeparableConv2D = depthwise convolution + _____ convolution (1x1, mixing channels).',
    type: 'fill-blank',
    correctAnswer: 'pointwise',
    explanation: 'Depthwise handles spatial filtering per channel independently; pointwise (1x1 conv) handles cross-channel mixing. Together they achieve similar results to standard convolution with far fewer parameters.',
    relatedCardIds: ['rc-4.11-2'],
    order: 2,
  },
  {
    id: 'q-4.11-3',
    lessonId: '4.11',
    question: 'Depthwise separable convolutions are more efficient because they:',
    type: 'multiple-choice',
    options: ['Use larger kernels', 'Decouple spatial and channel-wise operations, dramatically reducing parameters', 'Skip the activation function', 'Process images at lower resolution'],
    correctAnswer: 'Decouple spatial and channel-wise operations, dramatically reducing parameters',
    explanation: 'Standard convolutions jointly learn spatial and channel patterns. Separable convolutions factor this into two cheaper operations, often using 8-12x fewer parameters.',
    relatedCardIds: ['rc-4.11-3'],
    order: 3,
  },
  {
    id: 'q-4.12-1',
    lessonId: '4.12',
    question: 'The Mini Xception architecture combines which key techniques?',
    type: 'multiple-choice',
    options: ['Only dense layers and dropout', 'Residual connections, depthwise separable convolutions, and batch normalization', 'Only standard convolutions', 'Only transfer learning'],
    correctAnswer: 'Residual connections, depthwise separable convolutions, and batch normalization',
    explanation: 'Mini Xception integrates modern ConvNet best practices: separable convolutions for efficiency, residual connections for depth, and batch normalization for stability.',
    relatedCardIds: ['rc-4.12-1'],
    order: 1,
  },
  {
    id: 'q-4.12-2',
    lessonId: '4.12',
    question: 'An _____ study systematically removes components to determine which contribute to performance.',
    type: 'fill-blank',
    correctAnswer: 'ablation',
    explanation: 'Ablation studies are the gold standard for understanding component contributions: remove each piece and measure the impact on performance.',
    relatedCardIds: ['rc-4.12-2'],
    order: 2,
  },
  {
    id: 'q-4.12-3',
    lessonId: '4.12',
    question: 'When building a complex model, you should add all components at once rather than incrementally.',
    type: 'true-false',
    options: ['True', 'False'],
    correctAnswer: 'False',
    explanation: 'Best practice is to add components incrementally, measuring each one\'s contribution. This approach, validated by ablation studies, ensures every component earns its place.',
    relatedCardIds: ['rc-4.12-3'],
    order: 3,
  },
  {
    id: 'q-5.1-1',
    lessonId: '5.1',
    question: 'What do intermediate activation visualizations reveal about a ConvNet?',
    type: 'multiple-choice',
    options: ['The final classification probabilities', 'How each layer progressively transforms the input into increasingly abstract features', 'The model\'s learning rate', 'The optimal number of epochs'],
    correctAnswer: 'How each layer progressively transforms the input into increasingly abstract features',
    explanation: 'Visualizing activations shows the information distillation process: early layers show recognizable patterns (edges, colors), deeper layers show abstract feature combinations.',
    relatedCardIds: ['rc-5.1-1'],
    order: 1,
  },
  {
    id: 'q-5.1-2',
    lessonId: '5.1',
    question: 'Deeper layers in a ConvNet produce activations that are more visually interpretable than earlier layers.',
    type: 'true-false',
    options: ['True', 'False'],
    correctAnswer: 'False',
    explanation: 'The opposite is true. Early layers produce recognizable features (edges, textures). Deeper layers produce increasingly abstract, sparse activations that encode high-level semantic concepts.',
    relatedCardIds: ['rc-5.1-2'],
    order: 2,
  },
  {
    id: 'q-5.1-3',
    lessonId: '5.1',
    question: 'As you go deeper in a ConvNet, feature maps become more _____ and less visually interpretable.',
    type: 'fill-blank',
    correctAnswer: 'abstract',
    explanation: 'Early layers show recognizable patterns; deep layers show abstract concepts that are meaningful to the classification task but not to human visual inspection.',
    relatedCardIds: ['rc-5.1-3'],
    order: 3,
  },
  {
    id: 'q-5.2-1',
    lessonId: '5.2',
    question: 'Grad-CAM produces:',
    type: 'multiple-choice',
    options: ['A list of feature importances', 'A heatmap showing which spatial regions of the input most influenced the prediction', 'A modified version of the input image', 'The gradient values for each weight'],
    correctAnswer: 'A heatmap showing which spatial regions of the input most influenced the prediction',
    explanation: 'Grad-CAM uses the gradients flowing into the final convolutional layer to produce a heatmap highlighting the discriminative image regions used for the prediction.',
    relatedCardIds: ['rc-5.2-1'],
    order: 1,
  },
  {
    id: 'q-5.2-2',
    lessonId: '5.2',
    question: 'Grad-CAM can detect when a model is relying on:',
    type: 'multiple-choice',
    options: ['Too many parameters', 'Spurious features like background instead of the actual object', 'Too high a learning rate', 'Too much training data'],
    correctAnswer: 'Spurious features like background instead of the actual object',
    explanation: 'If Grad-CAM highlights the background instead of the object (e.g., snow instead of a husky), it reveals the model learned a shortcut rather than the intended features.',
    relatedCardIds: ['rc-5.2-2'],
    order: 2,
  },
  {
    id: 'q-5.2-3',
    lessonId: '5.2',
    question: 'Grad-CAM answers: "Which _____ regions of the input most influenced the model\'s prediction?"',
    type: 'fill-blank',
    correctAnswer: 'spatial',
    explanation: 'Grad-CAM creates a spatial heatmap overlaid on the input image, showing which locations were most important for the classification decision.',
    relatedCardIds: ['rc-5.2-3'],
    order: 3,
  },
  {
    id: 'q-5.3-1',
    lessonId: '5.3',
    question: 'Semantic segmentation assigns:',
    type: 'multiple-choice',
    options: ['One label per image', 'One label per pixel', 'One bounding box per object', 'One label per object instance'],
    correctAnswer: 'One label per pixel',
    explanation: 'In semantic segmentation, every pixel in the image receives a class label. This is denser than bounding boxes and does not distinguish between instances of the same class.',
    relatedCardIds: ['rc-5.3-1'],
    order: 1,
  },
  {
    id: 'q-5.3-2',
    lessonId: '5.3',
    question: 'The encoder-decoder architecture for segmentation uses skip connections to:',
    type: 'multiple-choice',
    options: ['Speed up training', 'Recover fine spatial detail lost during downsampling', 'Reduce the model size', 'Eliminate the need for pooling'],
    correctAnswer: 'Recover fine spatial detail lost during downsampling',
    explanation: 'The encoder downsamples for semantic understanding but loses spatial precision. Skip connections pass high-resolution features from encoder to decoder for precise boundary recovery.',
    relatedCardIds: ['rc-5.3-2'],
    order: 2,
  },
  {
    id: 'q-5.3-3',
    lessonId: '5.3',
    question: 'Instance segmentation distinguishes between different objects of the same class, while semantic segmentation does not.',
    type: 'true-false',
    options: ['True', 'False'],
    correctAnswer: 'True',
    explanation: 'Semantic segmentation labels all "car" pixels the same. Instance segmentation gives each individual car a unique ID, enabling counting and individual tracking.',
    relatedCardIds: ['rc-5.3-3'],
    order: 3,
  },
  {
    id: 'q-5.4-1',
    lessonId: '5.4',
    question: 'SAM (Segment Anything Model) is a foundation model for segmentation that generalizes via:',
    type: 'multiple-choice',
    options: ['Training on each new dataset', 'Prompting with points, boxes, or text', 'Hardcoded rules for each object type', 'Unsupervised clustering'],
    correctAnswer: 'Prompting with points, boxes, or text',
    explanation: 'SAM was trained on over 1 billion masks and can segment any object when prompted, without task-specific training -- similar to how LLMs respond to text prompts.',
    relatedCardIds: ['rc-5.4-1'],
    order: 1,
  },
  {
    id: 'q-5.4-2',
    lessonId: '5.4',
    question: 'SAM requires fine-tuning on your specific dataset before it can segment objects.',
    type: 'true-false',
    options: ['True', 'False'],
    correctAnswer: 'False',
    explanation: 'SAM is designed to segment anything zero-shot via prompting. While fine-tuning can improve domain-specific performance, it is not required for general use.',
    relatedCardIds: ['rc-5.4-2'],
    order: 2,
  },
  {
    id: 'q-5.4-3',
    lessonId: '5.4',
    question: 'SAM is to segmentation what GPT is to text: a _____ model that generalizes via prompting.',
    type: 'fill-blank',
    correctAnswer: 'foundation',
    explanation: 'Both are large pretrained models that respond to prompts for new tasks without requiring task-specific retraining.',
    relatedCardIds: ['rc-5.4-3'],
    order: 3,
  },
  {
    id: 'q-5.5-1',
    lessonId: '5.5',
    question: 'The key difference between single-stage and two-stage object detectors is:',
    type: 'multiple-choice',
    options: ['Single-stage uses GPUs, two-stage does not', 'Single-stage predicts boxes and classes simultaneously; two-stage first proposes regions, then classifies', 'Single-stage is always more accurate', 'Two-stage is always faster'],
    correctAnswer: 'Single-stage predicts boxes and classes simultaneously; two-stage first proposes regions, then classifies',
    explanation: 'Two-stage detectors (like Faster R-CNN) first generate region proposals, then classify each. Single-stage (like YOLO, SSD) predict directly, trading some accuracy for speed.',
    relatedCardIds: ['rc-5.5-1'],
    order: 1,
  },
  {
    id: 'q-5.5-2',
    lessonId: '5.5',
    question: 'Non-maximum suppression removes _____ detections of the same object.',
    type: 'fill-blank',
    correctAnswer: 'duplicate',
    explanation: 'NMS keeps only the highest-confidence bounding box for each object and discards overlapping boxes that likely detect the same instance.',
    relatedCardIds: ['rc-5.5-2'],
    order: 2,
  },
  {
    id: 'q-5.5-3',
    lessonId: '5.5',
    question: 'IoU (Intersection over Union) measures:',
    type: 'multiple-choice',
    options: ['Model accuracy', 'The overlap between predicted and ground truth bounding boxes', 'Training speed', 'The number of detected objects'],
    correctAnswer: 'The overlap between predicted and ground truth bounding boxes',
    explanation: 'IoU = area of overlap / area of union. An IoU of 0.5+ is typically considered a correct detection. Higher thresholds require more precise localization.',
    relatedCardIds: ['rc-5.5-3'],
    order: 3,
  },
  {
    id: 'q-5.6-1',
    lessonId: '5.6',
    question: 'YOLO processes the entire image in:',
    type: 'multiple-choice',
    options: ['Multiple passes, one per object', 'A single forward pass through the network', 'A two-stage pipeline', 'Pixel by pixel sequentially'],
    correctAnswer: 'A single forward pass through the network',
    explanation: 'YOLO (You Only Look Once) divides the image into a grid and predicts all bounding boxes and class probabilities in one pass, making it very fast.',
    relatedCardIds: ['rc-5.6-1'],
    order: 1,
  },
  {
    id: 'q-5.6-2',
    lessonId: '5.6',
    question: 'YOLO models prioritize inference speed, making them suitable for real-time applications.',
    type: 'true-false',
    options: ['True', 'False'],
    correctAnswer: 'True',
    explanation: 'YOLO is designed for real-time object detection, processing images at high frame rates. This makes it ideal for video surveillance, autonomous driving, and other time-critical applications.',
    relatedCardIds: ['rc-5.6-2'],
    order: 2,
  },
  {
    id: 'q-5.6-3',
    lessonId: '5.6',
    question: 'COCO is the standard object detection benchmark with _____ object categories.',
    type: 'fill-blank',
    correctAnswer: '80',
    explanation: 'COCO (Common Objects in Context) contains 80 object categories and is the most widely used benchmark for evaluating detection and segmentation models.',
    relatedCardIds: ['rc-5.6-3'],
    order: 3,
  },
  {
    id: 'q-5.7-1',
    lessonId: '5.7',
    question: 'For timeseries forecasting, the simplest yet surprisingly strong baseline is:',
    type: 'multiple-choice',
    options: ['A linear regression model', 'Predicting the last known value (persistence)', 'A random forest', 'A deep LSTM network'],
    correctAnswer: 'Predicting the last known value (persistence)',
    explanation: 'The persistence baseline (predict tomorrow = today) exploits temporal autocorrelation and is often surprisingly competitive, especially for short horizons.',
    relatedCardIds: ['rc-5.7-1'],
    order: 1,
  },
  {
    id: 'q-5.7-2',
    lessonId: '5.7',
    question: 'A sliding window approach for timeseries extracts:',
    type: 'multiple-choice',
    options: ['Random samples from the series', 'Overlapping (input sequence, target value) pairs in chronological order', 'Only the first and last values', 'Non-overlapping blocks'],
    correctAnswer: 'Overlapping (input sequence, target value) pairs in chronological order',
    explanation: 'The sliding window moves one step at a time through the series, creating many overlapping training examples while maintaining temporal order.',
    relatedCardIds: ['rc-5.7-2'],
    order: 2,
  },
  {
    id: 'q-5.7-3',
    lessonId: '5.7',
    question: 'Timeseries data must be split _____ (not randomly) to avoid temporal leakage.',
    type: 'fill-blank',
    correctAnswer: 'chronologically',
    explanation: 'Random splitting would let the model see future data during training. Training data must come before validation, which comes before test data.',
    relatedCardIds: ['rc-5.7-3'],
    order: 3,
  },
  {
    id: 'q-5.8-1',
    lessonId: '5.8',
    question: 'Conv1D is similar to Conv2D but operates along:',
    type: 'multiple-choice',
    options: ['Two spatial dimensions', 'One temporal dimension', 'Three color channels', 'The batch dimension'],
    correctAnswer: 'One temporal dimension',
    explanation: 'Conv1D slides a kernel along the time axis to detect local temporal patterns, just as Conv2D slides kernels along spatial dimensions.',
    relatedCardIds: ['rc-5.8-1'],
    order: 1,
  },
  {
    id: 'q-5.8-2',
    lessonId: '5.8',
    question: 'Conv1D can capture long-range temporal dependencies as effectively as attention mechanisms.',
    type: 'true-false',
    options: ['True', 'False'],
    correctAnswer: 'False',
    explanation: 'Conv1D only looks at local windows. To capture long-range dependencies, you need either very deep stacks of Conv1D layers (dilated convolutions) or attention mechanisms that directly connect distant positions.',
    relatedCardIds: ['rc-5.8-2'],
    order: 2,
  },
  {
    id: 'q-5.8-3',
    lessonId: '5.8',
    question: 'Conv1D detects _____ temporal patterns by sliding a kernel along the time axis.',
    type: 'fill-blank',
    correctAnswer: 'local',
    explanation: 'Just like Conv2D detects local spatial patterns, Conv1D detects local temporal patterns within the kernel\'s receptive field.',
    relatedCardIds: ['rc-5.8-3'],
    order: 3,
  },
  {
    id: 'q-5.9-1',
    lessonId: '5.9',
    question: 'LSTM handles long sequences better than SimpleRNN because:',
    type: 'multiple-choice',
    options: ['It uses more memory', 'Gating mechanisms (forget, input, output) prevent vanishing gradients', 'It processes tokens in reverse', 'It uses convolutions internally'],
    correctAnswer: 'Gating mechanisms (forget, input, output) prevent vanishing gradients',
    explanation: 'LSTM gates create gradient highways that allow information to persist over long sequences. The forget gate controls what to discard, while input and output gates control what to add and expose.',
    relatedCardIds: ['rc-5.9-1'],
    order: 1,
  },
  {
    id: 'q-5.9-2',
    lessonId: '5.9',
    question: 'The forget gate in an LSTM determines:',
    type: 'multiple-choice',
    options: ['Which inputs to process next', 'Which parts of the cell state to discard', 'The learning rate', 'The output sequence length'],
    correctAnswer: 'Which parts of the cell state to discard',
    explanation: 'The forget gate outputs values between 0 and 1 for each element of the cell state: 0 means completely forget, 1 means completely keep.',
    relatedCardIds: ['rc-5.9-2'],
    order: 2,
  },
  {
    id: 'q-5.9-3',
    lessonId: '5.9',
    question: 'The main limitation of SimpleRNN is the _____ gradient problem on long sequences.',
    type: 'fill-blank',
    correctAnswer: 'vanishing',
    explanation: 'As gradients are multiplied through many time steps, they shrink exponentially toward zero, making it impossible for the network to learn long-range dependencies.',
    relatedCardIds: ['rc-5.9-3'],
    order: 3,
  },
  {
    id: 'q-5.10-1',
    lessonId: '5.10',
    question: 'Bidirectional(LSTM(64)) produces output with how many features?',
    type: 'multiple-choice',
    options: ['64', '128', '32', '256'],
    correctAnswer: '128',
    explanation: 'Two LSTM(64) instances run in opposite directions (forward and backward). Their outputs are concatenated: 64 + 64 = 128 features per time step.',
    relatedCardIds: ['rc-5.10-1'],
    order: 1,
  },
  {
    id: 'q-5.10-2',
    lessonId: '5.10',
    question: 'Bidirectional RNNs can be used for real-time prediction where future context is not available.',
    type: 'true-false',
    options: ['True', 'False'],
    correctAnswer: 'False',
    explanation: 'Bidirectional RNNs require the complete sequence (including future time steps) to produce output. They cannot be used when you need to predict at each time step without seeing future data.',
    relatedCardIds: ['rc-5.10-2'],
    order: 2,
  },
  {
    id: 'q-5.10-3',
    lessonId: '5.10',
    question: 'Stacking multiple LSTM layers requires return_sequences=True on:',
    type: 'multiple-choice',
    options: ['Only the last LSTM layer', 'All LSTM layers except the last', 'Only the first LSTM layer', 'None of the layers'],
    correctAnswer: 'All LSTM layers except the last',
    explanation: 'Intermediate LSTM layers must output the full sequence (not just the final state) so the next LSTM layer receives a sequence input. Only the final layer can return just the last output.',
    relatedCardIds: ['rc-5.10-3'],
    order: 3,
  },
  {
    id: 'q-5.11-1',
    lessonId: '5.11',
    question: 'Self-attention computes relationships between:',
    type: 'multiple-choice',
    options: ['Different models', 'All positions within the same sequence', 'Different datasets', 'Training and validation sets'],
    correctAnswer: 'All positions within the same sequence',
    explanation: 'Self-attention computes weighted connections between every pair of positions in a sequence, allowing each position to attend to all others regardless of distance.',
    relatedCardIds: ['rc-5.11-1'],
    order: 1,
  },
  {
    id: 'q-5.11-2',
    lessonId: '5.11',
    question: 'The attention mechanism solves which fundamental limitation of RNNs?',
    type: 'multiple-choice',
    options: ['Slow training speed', 'The information bottleneck of compressing an entire sequence into a fixed-size state', 'High memory usage', 'Inability to process text'],
    correctAnswer: 'The information bottleneck of compressing an entire sequence into a fixed-size state',
    explanation: 'RNNs must compress all prior information into a fixed-size hidden state. Attention bypasses this by directly accessing any position in the input sequence.',
    relatedCardIds: ['rc-5.11-2'],
    order: 2,
  },
  {
    id: 'q-5.11-3',
    lessonId: '5.11',
    question: 'Attention lets the model directly _____ any position rather than relying on sequential propagation.',
    type: 'fill-blank',
    correctAnswer: 'access',
    explanation: 'Attention provides direct connections between any two positions, eliminating the need to propagate information step by step through intermediate positions.',
    relatedCardIds: ['rc-5.11-3'],
    order: 3,
  },
  {
    id: 'q-5.12-1',
    lessonId: '5.12',
    question: 'Before deploying a vision model, you should verify it focuses on relevant features using:',
    type: 'multiple-choice',
    options: ['Only test set accuracy', 'Grad-CAM or similar explainability techniques', 'The number of parameters', 'Training time'],
    correctAnswer: 'Grad-CAM or similar explainability techniques',
    explanation: 'High accuracy alone does not guarantee the model learned the right features. Grad-CAM can reveal if the model focuses on spurious correlations (e.g., background instead of object).',
    relatedCardIds: ['rc-5.12-1'],
    order: 1,
  },
  {
    id: 'q-5.12-2',
    lessonId: '5.12',
    question: 'A model with high test accuracy is guaranteed to work well in production.',
    type: 'true-false',
    options: ['True', 'False'],
    correctAnswer: 'False',
    explanation: 'High test accuracy can still mask problems: the model might rely on spurious features, fail on edge cases, or degrade when the data distribution shifts in production.',
    relatedCardIds: ['rc-5.12-2'],
    order: 2,
  },
  {
    id: 'q-5.12-3',
    lessonId: '5.12',
    question: 'The progression from classification to segmentation to detection represents increasingly _____ understanding of image content.',
    type: 'fill-blank',
    correctAnswer: 'detailed',
    explanation: 'Classification answers "what is in the image?", segmentation answers "where is each class, pixel by pixel?", and detection answers "where is each object instance and what is it?"',
    relatedCardIds: ['rc-5.12-3'],
    order: 3,
  },
  {
    id: 'q-6.1-1',
    lessonId: '6.1',
    question: 'Modern LLMs use subword tokenization as a compromise between:',
    type: 'multiple-choice',
    options: ['Speed and accuracy', 'Vocabulary size and ability to handle rare/unseen words', 'GPU memory and CPU memory', 'Training time and inference time'],
    correctAnswer: 'Vocabulary size and ability to handle rare/unseen words',
    explanation: 'Subword tokenization (BPE/WordPiece) keeps common words intact while splitting rare words into known subpieces, balancing vocabulary size, sequence length, and out-of-vocabulary handling.',
    relatedCardIds: ['rc-6.1-1'],
    order: 1,
  },
  {
    id: 'q-6.1-2',
    lessonId: '6.1',
    question: 'Which tokenization approach would represent "unhappiness" as ["un", "happiness"] or ["un", "happi", "ness"]?',
    type: 'multiple-choice',
    options: ['Character-level tokenization', 'Word-level tokenization', 'Subword tokenization', 'Sentence-level tokenization'],
    correctAnswer: 'Subword tokenization',
    explanation: 'Subword tokenization breaks words into meaningful subpieces based on frequency in the training corpus, allowing it to handle any word by composing known subwords.',
    relatedCardIds: ['rc-6.1-2'],
    order: 2,
  },
  {
    id: 'q-6.1-3',
    lessonId: '6.1',
    question: 'The three levels of text tokenization from coarsest to finest are: word, _____, and character.',
    type: 'fill-blank',
    correctAnswer: 'subword',
    explanation: 'Word tokenization is coarsest (large vocabulary, cannot handle unknown words), character is finest (tiny vocabulary, very long sequences), and subword is the practical middle ground.',
    relatedCardIds: ['rc-6.1-3'],
    order: 3,
  },
  {
    id: 'q-6.2-1',
    lessonId: '6.2',
    question: 'An Embedding(10000, 128) layer is:',
    type: 'multiple-choice',
    options: ['A dense layer with 128 outputs', 'A learnable lookup table mapping 10000 token IDs to 128-dimensional vectors', 'A convolutional layer with 128 filters', 'A dropout layer with rate 0.128'],
    correctAnswer: 'A learnable lookup table mapping 10000 token IDs to 128-dimensional vectors',
    explanation: 'The Embedding layer stores a 10000 x 128 weight matrix. Each token ID indexes one row, retrieving a learned 128-dimensional vector representation.',
    relatedCardIds: ['rc-6.2-1'],
    order: 1,
  },
  {
    id: 'q-6.2-2',
    lessonId: '6.2',
    question: 'Word embeddings capture semantic relationships: words with similar meanings have similar vectors.',
    type: 'true-false',
    options: ['True', 'False'],
    correctAnswer: 'True',
    explanation: 'Embeddings learn to place semantically related words near each other in vector space. Famous examples include "king - man + woman = queen" in Word2Vec embeddings.',
    relatedCardIds: ['rc-6.2-2'],
    order: 2,
  },
  {
    id: 'q-6.2-3',
    lessonId: '6.2',
    question: 'Pretrained word embeddings like Word2Vec or GloVe allow models to leverage _____ learned from large corpora.',
    type: 'fill-blank',
    correctAnswer: 'semantic relationships',
    explanation: 'Pretrained embeddings encode word similarities and analogies learned from billions of words, giving models a head start even with limited task-specific data.',
    relatedCardIds: ['rc-6.2-3'],
    order: 3,
  },
  {
    id: 'q-6.3-1',
    lessonId: '6.3',
    question: 'A language model is trained to predict:',
    type: 'multiple-choice',
    options: ['Image labels from text descriptions', 'The next token given the preceding tokens', 'The sentiment of a sentence', 'The language of a text'],
    correctAnswer: 'The next token given the preceding tokens',
    explanation: 'Language modeling is the task of predicting the next token in a sequence. This self-supervised objective requires no manual labels and enables training on massive text corpora.',
    relatedCardIds: ['rc-6.3-1'],
    order: 1,
  },
  {
    id: 'q-6.3-2',
    lessonId: '6.3',
    question: 'Language models require manually labeled training data.',
    type: 'true-false',
    options: ['True', 'False'],
    correctAnswer: 'False',
    explanation: 'Language models use self-supervised learning: the labels come from the text itself (the next token). This is why they can be trained on virtually unlimited text from the internet.',
    relatedCardIds: ['rc-6.3-2'],
    order: 2,
  },
  {
    id: 'q-6.3-3',
    lessonId: '6.3',
    question: 'A language model\'s training labels are the input sequence _____ by one token position.',
    type: 'fill-blank',
    correctAnswer: 'shifted',
    explanation: 'At each position, the target is the next token. So the labels are simply the input shifted by one position to the right.',
    relatedCardIds: ['rc-6.3-3'],
    order: 3,
  },
  {
    id: 'q-6.4-1',
    lessonId: '6.4',
    question: 'The key advantage of Transformers over RNNs is:',
    type: 'multiple-choice',
    options: ['They use less memory', 'They process all tokens in parallel via attention, enabling faster training on long sequences', 'They have fewer parameters', 'They do not require GPU training'],
    correctAnswer: 'They process all tokens in parallel via attention, enabling faster training on long sequences',
    explanation: 'RNNs must process tokens one at a time sequentially. Transformers compute attention over all positions simultaneously, enabling massive parallelism on GPUs.',
    relatedCardIds: ['rc-6.4-1'],
    order: 1,
  },
  {
    id: 'q-6.4-2',
    lessonId: '6.4',
    question: 'In self-attention, Query, Key, and Value matrices are derived from:',
    type: 'multiple-choice',
    options: ['Three separate input sequences', 'The same input sequence, projected through different learned weight matrices', 'Random initialization only', 'The loss function'],
    correctAnswer: 'The same input sequence, projected through different learned weight matrices',
    explanation: 'In self-attention, the same input is linearly projected three ways to create Q, K, and V. This lets each position query and attend to all other positions.',
    relatedCardIds: ['rc-6.4-2'],
    order: 2,
  },
  {
    id: 'q-6.4-3',
    lessonId: '6.4',
    question: 'Transformers process all tokens in _____ via attention, unlike RNNs which process tokens sequentially.',
    type: 'fill-blank',
    correctAnswer: 'parallel',
    explanation: 'Parallel processing of all positions simultaneously is what makes Transformers vastly faster to train than RNNs on modern GPU hardware.',
    relatedCardIds: ['rc-6.4-3'],
    order: 3,
  },
  {
    id: 'q-6.5-1',
    lessonId: '6.5',
    question: 'In the Transformer decoder, causal masking ensures that:',
    type: 'multiple-choice',
    options: ['All tokens can see all other tokens', 'Each position can only attend to earlier positions, not future ones', 'The model processes tokens in reverse order', 'Attention weights are always positive'],
    correctAnswer: 'Each position can only attend to earlier positions, not future ones',
    explanation: 'Causal masking prevents the model from "cheating" by looking at future tokens during training. Position i can only attend to positions 0 through i.',
    relatedCardIds: ['rc-6.5-1'],
    order: 1,
  },
  {
    id: 'q-6.5-2',
    lessonId: '6.5',
    question: 'Positional encoding is necessary because:',
    type: 'multiple-choice',
    options: ['Transformers are too fast without it', 'Attention is permutation-invariant and has no inherent notion of token order', 'Embeddings are too small', 'Gradients would vanish without it'],
    correctAnswer: 'Attention is permutation-invariant and has no inherent notion of token order',
    explanation: 'Unlike RNNs which process tokens sequentially, attention treats input as a set. Positional encodings inject order information so the model can distinguish "dog bites man" from "man bites dog."',
    relatedCardIds: ['rc-6.5-2'],
    order: 2,
  },
  {
    id: 'q-6.5-3',
    lessonId: '6.5',
    question: 'In cross-attention, Queries come from the _____ and Keys/Values come from the _____.',
    type: 'fill-blank',
    correctAnswer: 'decoder; encoder',
    explanation: 'Cross-attention lets the decoder query the encoder output. Each decoder position decides which parts of the source sequence are relevant for generating the current token.',
    relatedCardIds: ['rc-6.5-3'],
    order: 3,
  },
  {
    id: 'q-6.6-1',
    lessonId: '6.6',
    question: 'Fine-tuning a pretrained Transformer (like BERT) for a specific task typically involves:',
    type: 'multiple-choice',
    options: ['Training the entire model from scratch', 'Adding a task-specific head and training on labeled data for the target task', 'Only changing the tokenizer', 'Removing all attention layers'],
    correctAnswer: 'Adding a task-specific head and training on labeled data for the target task',
    explanation: 'Pretrained Transformers provide rich language representations. Fine-tuning adds a small task-specific layer and trains on relatively little labeled data for remarkable performance.',
    relatedCardIds: ['rc-6.6-1'],
    order: 1,
  },
  {
    id: 'q-6.6-2',
    lessonId: '6.6',
    question: 'BERT uses which type of attention?',
    type: 'multiple-choice',
    options: ['Causal (unidirectional) attention', 'Bidirectional (full) attention', 'No attention', 'Cross-attention only'],
    correctAnswer: 'Bidirectional (full) attention',
    explanation: 'BERT uses masked language modeling with full bidirectional attention -- each token can attend to all other tokens. GPT uses causal (left-to-right) attention.',
    relatedCardIds: ['rc-6.6-2'],
    order: 2,
  },
  {
    id: 'q-6.6-3',
    lessonId: '6.6',
    question: 'Self-attention captures dependencies at any _____ in one step, unlike RNNs which propagate sequentially.',
    type: 'fill-blank',
    correctAnswer: 'distance',
    explanation: 'Attention directly connects any two positions regardless of how far apart they are, computing their relationship in a single operation.',
    relatedCardIds: ['rc-6.6-3'],
    order: 3,
  },
  {
    id: 'q-6.7-1',
    lessonId: '6.7',
    question: 'Temperature in text generation controls:',
    type: 'multiple-choice',
    options: ['The speed of generation', 'The randomness/diversity of token selection from the probability distribution', 'The maximum sequence length', 'The model size'],
    correctAnswer: 'The randomness/diversity of token selection from the probability distribution',
    explanation: 'Low temperature sharpens the distribution (more deterministic, picking high-probability tokens). High temperature flattens it (more random, exploring unlikely tokens).',
    relatedCardIds: ['rc-6.7-1'],
    order: 1,
  },
  {
    id: 'q-6.7-2',
    lessonId: '6.7',
    question: 'Top-k sampling restricts token selection to:',
    type: 'multiple-choice',
    options: ['The k longest tokens', 'The k most probable tokens at each step', 'Every k-th token', 'The first k tokens in the vocabulary'],
    correctAnswer: 'The k most probable tokens at each step',
    explanation: 'Top-k sampling only considers the k highest-probability tokens, setting all others to zero probability. This prevents selecting very unlikely tokens while maintaining diversity.',
    relatedCardIds: ['rc-6.7-2'],
    order: 2,
  },
  {
    id: 'q-6.7-3',
    lessonId: '6.7',
    question: 'Low temperature produces _____ but potentially repetitive text; high temperature produces diverse but potentially _____ text.',
    type: 'fill-blank',
    correctAnswer: 'coherent; incoherent',
    explanation: 'Temperature is a tradeoff: low values give safe, predictable output while high values give creative but potentially nonsensical output.',
    relatedCardIds: ['rc-6.7-3'],
    order: 3,
  },
  {
    id: 'q-6.8-1',
    lessonId: '6.8',
    question: 'LoRA (Low-Rank Adaptation) makes fine-tuning more efficient by:',
    type: 'multiple-choice',
    options: ['Using a smaller dataset', 'Training only small low-rank matrices added to frozen pretrained weights', 'Reducing the learning rate', 'Removing layers from the model'],
    correctAnswer: 'Training only small low-rank matrices added to frozen pretrained weights',
    explanation: 'LoRA freezes the original model and injects small trainable low-rank decomposition matrices. This reduces trainable parameters by 10,000x while achieving comparable performance.',
    relatedCardIds: ['rc-6.8-1'],
    order: 1,
  },
  {
    id: 'q-6.8-2',
    lessonId: '6.8',
    question: 'RLHF (Reinforcement Learning from Human Feedback) is used to:',
    type: 'multiple-choice',
    options: ['Train the initial language model', 'Align LLM outputs with human preferences for helpfulness and safety', 'Speed up training', 'Reduce model size'],
    correctAnswer: 'Align LLM outputs with human preferences for helpfulness and safety',
    explanation: 'RLHF uses human preference ratings to train a reward model, then optimizes the LLM to produce outputs that score highly, making it more helpful and less harmful.',
    relatedCardIds: ['rc-6.8-2'],
    order: 2,
  },
  {
    id: 'q-6.8-3',
    lessonId: '6.8',
    question: 'RLHF uses _____ preferences to train a reward model that guides LLM behavior.',
    type: 'fill-blank',
    correctAnswer: 'human',
    explanation: 'Human raters compare model outputs and indicate preferences. These preferences train a reward model that provides the signal for reinforcement learning.',
    relatedCardIds: ['rc-6.8-3'],
    order: 3,
  },
  {
    id: 'q-6.9-1',
    lessonId: '6.9',
    question: 'RAG (Retrieval-Augmented Generation) improves LLMs by:',
    type: 'multiple-choice',
    options: ['Making the model larger', 'Retrieving relevant documents and including them in the context before generation', 'Training on more data', 'Removing attention layers'],
    correctAnswer: 'Retrieving relevant documents and including them in the context before generation',
    explanation: 'RAG grounds LLM responses in retrieved facts, reducing hallucination and enabling access to information not in the model\'s training data.',
    relatedCardIds: ['rc-6.9-1'],
    order: 1,
  },
  {
    id: 'q-6.9-2',
    lessonId: '6.9',
    question: 'Multimodal LLMs can process only text input.',
    type: 'true-false',
    options: ['True', 'False'],
    correctAnswer: 'False',
    explanation: 'Multimodal LLMs can process multiple input types (modalities) such as text, images, audio, and video, enabling tasks like image captioning and visual question answering.',
    relatedCardIds: ['rc-6.9-2'],
    order: 2,
  },
  {
    id: 'q-6.9-3',
    lessonId: '6.9',
    question: 'Chain-of-thought prompting improves LLM reasoning by encouraging the model to show its _____ steps.',
    type: 'fill-blank',
    correctAnswer: 'reasoning',
    explanation: 'By generating intermediate reasoning steps before the final answer, chain-of-thought prompting enables more complex problem-solving and makes errors easier to identify.',
    relatedCardIds: ['rc-6.9-3'],
    order: 3,
  },
  {
    id: 'q-6.10-1',
    lessonId: '6.10',
    question: 'A VAE encoder outputs parameters of a distribution rather than a single point because:',
    type: 'multiple-choice',
    options: ['It reduces computation', 'This enables smooth interpolation in latent space and generation of new samples', 'It is required by the loss function', 'It prevents overfitting'],
    correctAnswer: 'This enables smooth interpolation in latent space and generation of new samples',
    explanation: 'Encoding to a distribution (mean + variance) ensures the latent space is continuous and smooth, so nearby points decode to similar outputs and random samples produce valid results.',
    relatedCardIds: ['rc-6.10-1'],
    order: 1,
  },
  {
    id: 'q-6.10-2',
    lessonId: '6.10',
    question: 'The VAE loss function combines:',
    type: 'multiple-choice',
    options: ['Only reconstruction loss', 'Only KL divergence', 'Reconstruction loss and KL divergence that regularizes the latent space', 'Cross-entropy and accuracy'],
    correctAnswer: 'Reconstruction loss and KL divergence that regularizes the latent space',
    explanation: 'Reconstruction loss ensures faithfulness to input. KL divergence pushes the learned distribution toward a standard normal, creating a smooth, complete latent space.',
    relatedCardIds: ['rc-6.10-2'],
    order: 2,
  },
  {
    id: 'q-6.10-3',
    lessonId: '6.10',
    question: 'In a VAE, the encoder outputs _____ and variance (parameters of a distribution), not a single point.',
    type: 'fill-blank',
    correctAnswer: 'mean',
    explanation: 'Encoding to mean and variance defines a Gaussian distribution in latent space. During training, samples are drawn from this distribution using the reparameterization trick.',
    relatedCardIds: ['rc-6.10-3'],
    order: 3,
  },
  {
    id: 'q-6.11-1',
    lessonId: '6.11',
    question: 'Diffusion models generate images by:',
    type: 'multiple-choice',
    options: ['Directly predicting pixel values in one step', 'Gradually removing noise from a random starting point over many steps', 'Searching a database of existing images', 'Using GANs internally'],
    correctAnswer: 'Gradually removing noise from a random starting point over many steps',
    explanation: 'Diffusion models learn to reverse a gradual noising process. Generation starts from pure noise and iteratively denoises, producing a clean image over many small steps.',
    relatedCardIds: ['rc-6.11-1'],
    order: 1,
  },
  {
    id: 'q-6.11-2',
    lessonId: '6.11',
    question: 'The U-Net in a diffusion model receives which inputs?',
    type: 'multiple-choice',
    options: ['Only the noisy image', 'The noisy image and a timestep encoding indicating the noise level', 'Only the original clean image', 'The loss value and learning rate'],
    correctAnswer: 'The noisy image and a timestep encoding indicating the noise level',
    explanation: 'The timestep encoding tells the U-Net how much noise is present so it can calibrate its denoising behavior appropriately for each step.',
    relatedCardIds: ['rc-6.11-2'],
    order: 2,
  },
  {
    id: 'q-6.11-3',
    lessonId: '6.11',
    question: 'Stable Diffusion operates in a compressed _____ space rather than directly in pixel space.',
    type: 'fill-blank',
    correctAnswer: 'latent',
    explanation: 'Working in latent space (encoded by a VAE) dramatically reduces computational cost while maintaining image quality, making high-resolution generation practical.',
    relatedCardIds: ['rc-6.11-3'],
    order: 3,
  },
  {
    id: 'q-6.12-1',
    lessonId: '6.12',
    question: 'The NLP pipeline from raw text to model output proceeds through:',
    type: 'multiple-choice',
    options: ['Embedding, tokenization, classification', 'Tokenization, embedding, Transformer processing', 'Classification, embedding, tokenization', 'Transformer processing, tokenization, embedding'],
    correctAnswer: 'Tokenization, embedding, Transformer processing',
    explanation: 'Raw text is first split into tokens, tokens are mapped to vectors via embeddings, then processed through Transformer layers to produce task-specific outputs.',
    relatedCardIds: ['rc-6.12-1'],
    order: 1,
  },
  {
    id: 'q-6.12-2',
    lessonId: '6.12',
    question: 'Diffusion models have largely replaced GANs for image generation due to more stable training and better diversity.',
    type: 'true-false',
    options: ['True', 'False'],
    correctAnswer: 'True',
    explanation: 'Diffusion models avoid the training instability and mode collapse issues of GANs, producing more diverse outputs with more reliable training procedures.',
    relatedCardIds: ['rc-6.12-2'],
    order: 2,
  },
  {
    id: 'q-6.12-3',
    lessonId: '6.12',
    question: 'The key insight across Module 6 is that _____ learning on large datasets, followed by task-specific adaptation, is the dominant paradigm.',
    type: 'fill-blank',
    correctAnswer: 'self-supervised',
    explanation: 'Self-supervised pretraining (predicting next tokens, masked tokens, or denoising) on massive datasets, followed by fine-tuning or prompting, is the foundation of modern NLP and generation.',
    relatedCardIds: ['rc-6.12-3'],
    order: 3,
  },
  {
    id: 'q-7.1-1',
    lessonId: '7.1',
    question: 'Bayesian optimization for hyperparameter tuning is preferred over grid search because:',
    type: 'multiple-choice',
    options: ['It always finds the global optimum', 'It uses past results to intelligently choose which configurations to try next', 'It requires no computation', 'It only works with neural networks'],
    correctAnswer: 'It uses past results to intelligently choose which configurations to try next',
    explanation: 'Bayesian optimization builds a probabilistic model of the objective function and uses it to select promising hyperparameters, exploring efficiently rather than exhaustively.',
    relatedCardIds: ['rc-7.1-1'],
    order: 1,
  },
  {
    id: 'q-7.1-2',
    lessonId: '7.1',
    question: 'Model ensembling improves performance by:',
    type: 'multiple-choice',
    options: ['Using a single larger model', 'Combining predictions from multiple diverse models to reduce variance', 'Training faster', 'Using less data'],
    correctAnswer: 'Combining predictions from multiple diverse models to reduce variance',
    explanation: 'Ensembles average out individual model errors. The key requirement is model diversity -- ensembling identical models provides no benefit.',
    relatedCardIds: ['rc-7.1-2'],
    order: 2,
  },
  {
    id: 'q-7.1-3',
    lessonId: '7.1',
    question: 'Bayesian optimization uses a _____ model of past results to balance exploration and exploitation.',
    type: 'fill-blank',
    correctAnswer: 'probabilistic',
    explanation: 'The surrogate model (often a Gaussian Process) predicts which hyperparameter configurations are likely to perform well, focusing the search on promising regions.',
    relatedCardIds: ['rc-7.1-3'],
    order: 3,
  },
  {
    id: 'q-7.2-1',
    lessonId: '7.2',
    question: 'Data parallelism distributes training by:',
    type: 'multiple-choice',
    options: ['Splitting the model across GPUs', 'Giving each GPU a different mini-batch of data and synchronizing gradients', 'Using only one GPU more efficiently', 'Reducing the dataset size'],
    correctAnswer: 'Giving each GPU a different mini-batch of data and synchronizing gradients',
    explanation: 'Each GPU processes a different data batch with a copy of the full model. Gradients are averaged across GPUs before updating weights, effectively increasing batch size.',
    relatedCardIds: ['rc-7.2-1'],
    order: 1,
  },
  {
    id: 'q-7.2-2',
    lessonId: '7.2',
    question: 'Model parallelism splits:',
    type: 'multiple-choice',
    options: ['The dataset across GPUs', 'The model itself across GPUs, with each GPU handling different layers or parts', 'The learning rate across GPUs', 'The loss function across GPUs'],
    correctAnswer: 'The model itself across GPUs, with each GPU handling different layers or parts',
    explanation: 'When a model is too large for one GPU\'s memory, model parallelism places different parts on different GPUs. Data parallelism is more common and simpler.',
    relatedCardIds: ['rc-7.2-2'],
    order: 2,
  },
  {
    id: 'q-7.2-3',
    lessonId: '7.2',
    question: 'The main overhead of multi-GPU data-parallel training is _____ synchronization between devices.',
    type: 'fill-blank',
    correctAnswer: 'gradient',
    explanation: 'After each batch, gradients from all GPUs must be communicated and averaged (all-reduce), which becomes the bottleneck as the number of GPUs increases.',
    relatedCardIds: ['rc-7.2-3'],
    order: 3,
  },
  {
    id: 'q-7.3-1',
    lessonId: '7.3',
    question: 'Mixed precision training uses float16 for most computations because:',
    type: 'multiple-choice',
    options: ['It is more accurate than float32', 'It halves memory usage and doubles throughput on modern GPUs', 'It is required by all frameworks', 'It eliminates the need for normalization'],
    correctAnswer: 'It halves memory usage and doubles throughput on modern GPUs',
    explanation: 'Float16 uses half the memory of float32 and modern GPUs have dedicated hardware (Tensor Cores) that processes float16 at 2-8x the speed of float32.',
    relatedCardIds: ['rc-7.3-1'],
    order: 1,
  },
  {
    id: 'q-7.3-2',
    lessonId: '7.3',
    question: 'Quantization to INT8 is primarily used for:',
    type: 'multiple-choice',
    options: ['Training larger models faster', 'Reducing inference latency and model size for deployment', 'Improving training accuracy', 'Increasing the learning rate'],
    correctAnswer: 'Reducing inference latency and model size for deployment',
    explanation: 'INT8 quantization is a deployment optimization that shrinks model size by 4x and speeds up inference, with minimal accuracy loss for most models.',
    relatedCardIds: ['rc-7.3-2'],
    order: 2,
  },
  {
    id: 'q-7.3-3',
    lessonId: '7.3',
    question: 'Loss scaling prevents gradient _____ in float16 by temporarily amplifying gradient values.',
    type: 'fill-blank',
    correctAnswer: 'underflow',
    explanation: 'Float16 has limited range. Small gradient values can round to zero (underflow). Loss scaling multiplies the loss by a large factor, keeping gradients in the representable range.',
    relatedCardIds: ['rc-7.3-3'],
    order: 3,
  },
  {
    id: 'q-7.4-1',
    lessonId: '7.4',
    question: 'According to Chollet, current deep learning\'s fundamental limitation is:',
    type: 'multiple-choice',
    options: ['Not enough training data', 'It can only perform local generalization (interpolation), not extreme generalization (reasoning)', 'GPUs are too slow', 'Neural networks have too few layers'],
    correctAnswer: 'It can only perform local generalization (interpolation), not extreme generalization (reasoning)',
    explanation: 'Deep learning excels at pattern matching within the training distribution but cannot reason abstractly about novel situations requiring genuine understanding.',
    relatedCardIds: ['rc-7.4-1'],
    order: 1,
  },
  {
    id: 'q-7.4-2',
    lessonId: '7.4',
    question: 'Deep learning models truly understand the data they process.',
    type: 'true-false',
    options: ['True', 'False'],
    correctAnswer: 'False',
    explanation: 'Current deep learning systems are sophisticated curve-fitting machines. They recognize patterns but do not form genuine understanding or mental models of the world.',
    relatedCardIds: ['rc-7.4-2'],
    order: 2,
  },
  {
    id: 'q-7.4-3',
    lessonId: '7.4',
    question: 'Which is a real limitation of current deep learning?',
    type: 'multiple-choice',
    options: ['It cannot process images', 'It requires explicit programming of rules', 'It struggles with tasks requiring reasoning about novel situations not seen during training', 'It cannot scale to large datasets'],
    correctAnswer: 'It struggles with tasks requiring reasoning about novel situations not seen during training',
    explanation: 'DL systems fail on problems requiring genuine reasoning, abstraction, and handling of situations fundamentally different from training data.',
    relatedCardIds: ['rc-7.4-3'],
    order: 3,
  },
  {
    id: 'q-7.5-1',
    lessonId: '7.5',
    question: 'Program synthesis as a future AI direction proposes:',
    type: 'multiple-choice',
    options: ['Writing more Python code', 'Models that generate and search over programs rather than just learning continuous weights', 'Using symbolic AI exclusively', 'Abandoning neural networks'],
    correctAnswer: 'Models that generate and search over programs rather than just learning continuous weights',
    explanation: 'Combining neural networks with discrete program search could enable genuine reasoning and abstraction, going beyond the limitations of pure gradient-based learning.',
    relatedCardIds: ['rc-7.5-1'],
    order: 1,
  },
  {
    id: 'q-7.5-2',
    lessonId: '7.5',
    question: 'The future of AI likely involves combining deep learning with other approaches rather than relying on deep learning alone.',
    type: 'true-false',
    options: ['True', 'False'],
    correctAnswer: 'True',
    explanation: 'Chollet argues that hybrid approaches combining deep learning\'s pattern recognition with program synthesis, symbolic reasoning, and other techniques will be needed for more general AI.',
    relatedCardIds: ['rc-7.5-2'],
    order: 2,
  },
  {
    id: 'q-7.5-3',
    lessonId: '7.5',
    question: 'Test-time _____ would allow models to learn from new situations during inference.',
    type: 'fill-blank',
    correctAnswer: 'adaptation',
    explanation: 'Currently models are frozen after training. Test-time adaptation (or test-time training) would enable on-the-fly learning from new data during deployment.',
    relatedCardIds: ['rc-7.5-3'],
    order: 3,
  },
  {
    id: 'q-7.6-1',
    lessonId: '7.6',
    question: 'The single most important practice for staying current in deep learning is:',
    type: 'multiple-choice',
    options: ['Memorizing architectures', 'Reading papers and implementing ideas regularly', 'Buying the latest GPU', 'Attending every conference'],
    correctAnswer: 'Reading papers and implementing ideas regularly',
    explanation: 'The field moves fast. Regular reading (arxiv, blogs) combined with hands-on implementation ensures both theoretical understanding and practical skill.',
    relatedCardIds: ['rc-7.6-1'],
    order: 1,
  },
  {
    id: 'q-7.6-2',
    lessonId: '7.6',
    question: 'The ML workflow covered in this course applies only to deep learning, not to traditional ML.',
    type: 'true-false',
    options: ['True', 'False'],
    correctAnswer: 'False',
    explanation: 'The universal ML workflow (define task, develop model, deploy and monitor) applies to all machine learning, not just deep learning. The principles of evaluation, regularization, and iteration are universal.',
    relatedCardIds: ['rc-7.6-2'],
    order: 2,
  },
  {
    id: 'q-7.6-3',
    lessonId: '7.6',
    question: 'The universal ML workflow progresses from _____ the task, to developing a model, to deploying and monitoring.',
    type: 'fill-blank',
    correctAnswer: 'defining',
    explanation: 'Define (understand problem, data, and metrics) -> Develop (build, train, evaluate, iterate) -> Deploy (serve, monitor, maintain). This workflow applies to every ML project.',
    relatedCardIds: ['rc-7.6-3'],
    order: 3,
  }
];

/** Index quiz questions by lesson ID for O(1) lookup. */
export const quizQuestionsByLesson: Record<string, QuizQuestion[]> = {};
for (const q of quizQuestions) {
  if (!quizQuestionsByLesson[q.lessonId]) {
    quizQuestionsByLesson[q.lessonId] = [];
  }
  quizQuestionsByLesson[q.lessonId].push(q);
}

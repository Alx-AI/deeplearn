/**
 * Lesson definitions for the Multi-Agent Reinforcement Learning book.
 *
 * 65 lessons across 9 modules, each targeting ~15 minutes of focused learning.
 * Content sourced from "Multi-Agent Reinforcement Learning: Foundations and
 * Modern Approaches" (Albrecht, Christianos & Schäfer).
 */

import type { Lesson } from '@/lib/db/schema';

export const lessons: Lesson[] = [
  // ---------------------------------------------------------------------------
  // Module 1: Introduction to Multi-Agent Systems (Chapter 1)
  // ---------------------------------------------------------------------------
  {
    id: 'marl-1.1',
    moduleId: 'marl-mod-1',
    title: 'What Are Multi-Agent Systems?',
    description: 'Define multi-agent systems and understand why studying multiple interacting agents requires new tools beyond single-agent RL.',
    order: 1,
    sourceSections: ['1.1'],
    prerequisites: [],
    learningObjectives: [
      'Define a multi-agent system and identify its key components',
      'Explain why single-agent methods are insufficient when multiple agents interact',
      'Recognise examples of multi-agent systems in everyday life and technology',
    ],
    keyConcepts: [
      'Multi-agent system: multiple autonomous agents interacting in a shared environment',
      'Agent: an entity that perceives its environment and takes actions to achieve goals',
      'Interaction effects: one agent\'s action can change the outcome for all others',
      'Emergent behaviour: complex group outcomes arising from simple individual rules',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-1.1.mdx',
  },
  {
    id: 'marl-1.2',
    moduleId: 'marl-mod-1',
    title: 'Multi-Agent Reinforcement Learning Overview',
    description: 'Understand how MARL combines reinforcement learning with multi-agent interaction and why it is a growing research area.',
    order: 2,
    sourceSections: ['1.2'],
    prerequisites: ['marl-1.1'],
    learningObjectives: [
      'Explain how MARL extends single-agent RL to multi-agent settings',
      'Describe the key axes of variation in MARL: cooperation, competition, mixed',
      'Understand the role of learning in multi-agent systems versus hand-designed strategies',
    ],
    keyConcepts: [
      'MARL: reinforcement learning with multiple simultaneously learning agents',
      'Cooperative vs. competitive vs. mixed-motive settings',
      'Joint action: the combined actions of all agents at a given time step',
      'Scalability challenge: the joint action space grows exponentially with agent count',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-1.2.mdx',
  },
  {
    id: 'marl-1.3',
    moduleId: 'marl-mod-1',
    title: 'MARL Application Examples',
    description: 'Survey real-world applications of MARL in robotics, autonomous driving, game playing, and resource management.',
    order: 3,
    sourceSections: ['1.3'],
    prerequisites: ['marl-1.2'],
    learningObjectives: [
      'Identify at least four application domains where MARL is used or studied',
      'Explain why each application naturally requires multi-agent reasoning',
      'Connect application characteristics (cooperation, competition, partial observability) to MARL concepts',
    ],
    keyConcepts: [
      'Autonomous vehicles: multiple vehicles coordinating in traffic',
      'Multi-robot systems: warehouse robots, search-and-rescue teams',
      'Game AI: StarCraft, Go, poker as multi-agent testbeds',
      'Resource management: network routing, energy grids, supply chains',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-1.3.mdx',
  },
  {
    id: 'marl-1.4',
    moduleId: 'marl-mod-1',
    title: 'Challenges, Agendas, and Book Roadmap',
    description: 'Identify the core challenges of MARL research and preview the structure of the book\'s journey from game theory to deep MARL.',
    order: 4,
    sourceSections: ['1.4', '1.5', '1.6'],
    prerequisites: ['marl-1.3'],
    learningObjectives: [
      'List the major open challenges in MARL research',
      'Distinguish between the cooperative, competitive, and general-sum research agendas',
      'Understand the logical progression from single-agent RL through game theory to deep MARL',
    ],
    keyConcepts: [
      'Non-stationarity: the environment appears to change because other agents are learning',
      'Credit assignment: determining each agent\'s contribution to a team reward',
      'Scalability: methods must handle growing numbers of agents',
      'Book roadmap: RL → game theory → solution concepts → MARL algorithms → deep MARL → practice',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-1.4.mdx',
  },

  // ---------------------------------------------------------------------------
  // Module 2: Single-Agent Reinforcement Learning (Chapter 2)
  // ---------------------------------------------------------------------------
  {
    id: 'marl-2.1',
    moduleId: 'marl-mod-2',
    title: 'The RL Problem: Agents and Environments',
    description: 'Formalise the agent-environment interaction loop and understand observations, actions, and rewards.',
    order: 1,
    sourceSections: ['2.1'],
    prerequisites: ['marl-1.4'],
    learningObjectives: [
      'Describe the agent-environment interaction loop: observe, act, receive reward',
      'Distinguish between the agent and the environment in an RL problem',
      'Understand how the reward signal encodes the agent\'s goal',
    ],
    keyConcepts: [
      'Agent-environment loop: agent observes state, takes action, receives reward, transitions to new state',
      'Policy: a mapping from states to actions (the agent\'s strategy)',
      'Reward signal: scalar feedback indicating how good the last action was',
      'Episode: a sequence from initial state to terminal state',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-2.1.mdx',
  },
  {
    id: 'marl-2.2',
    moduleId: 'marl-mod-2',
    title: 'Markov Decision Processes',
    description: 'Define MDPs formally — states, actions, transitions, and rewards — and understand the Markov property.',
    order: 2,
    sourceSections: ['2.2'],
    prerequisites: ['marl-2.1'],
    learningObjectives: [
      'Define the five components of an MDP: S, A, T, R, γ',
      'State the Markov property and explain why it simplifies decision-making',
      'Distinguish between deterministic and stochastic transition functions',
    ],
    keyConcepts: [
      'MDP tuple: (S, A, T, R, γ) — states, actions, transition function, reward function, discount factor',
      'Markov property: the future depends only on the current state, not the history',
      'Transition function T(s\'|s,a): probability of reaching state s\' from s via action a',
      'Reward function R(s,a,s\'): immediate numerical feedback for a transition',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-2.2.mdx',
  },
  {
    id: 'marl-2.3',
    moduleId: 'marl-mod-2',
    title: 'Discounted Returns and Optimal Policies',
    description: 'Understand discounted cumulative reward, why we discount, and what it means for a policy to be optimal.',
    order: 3,
    sourceSections: ['2.3'],
    prerequisites: ['marl-2.2'],
    learningObjectives: [
      'Define the discounted return and explain the role of the discount factor γ',
      'Explain why discounting is necessary for infinite-horizon problems',
      'Define an optimal policy and understand that at least one always exists in finite MDPs',
    ],
    keyConcepts: [
      'Return G_t: the discounted sum of future rewards from time t onward',
      'Discount factor γ ∈ [0,1): trades off immediate vs. future rewards',
      'Optimal policy π*: a policy that maximises expected return from every state',
      'Deterministic vs. stochastic policies: always choosing one action vs. sampling from a distribution',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-2.3.mdx',
  },
  {
    id: 'marl-2.4',
    moduleId: 'marl-mod-2',
    title: 'Value Functions and the Bellman Equation',
    description: 'Define state-value and action-value functions and derive the Bellman equation that relates them recursively.',
    order: 4,
    sourceSections: ['2.4'],
    prerequisites: ['marl-2.3'],
    learningObjectives: [
      'Define V^π(s) and Q^π(s,a) and explain what each measures',
      'Derive the Bellman expectation equation for V^π',
      'Understand the Bellman optimality equation and its significance for finding optimal policies',
    ],
    keyConcepts: [
      'State-value function V^π(s): expected return starting from state s under policy π',
      'Action-value function Q^π(s,a): expected return from state s, taking action a, then following π',
      'Bellman equation: V^π(s) = Σ_a π(a|s) Σ_{s\'} T(s\'|s,a)[R(s,a,s\') + γ V^π(s\')]',
      'Optimal value function V*(s) = max_a Q*(s,a)',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-2.4.mdx',
  },
  {
    id: 'marl-2.5',
    moduleId: 'marl-mod-2',
    title: 'Dynamic Programming: Policy and Value Iteration',
    description: 'Learn exact solution methods for MDPs when the model is known: policy iteration and value iteration.',
    order: 5,
    sourceSections: ['2.5'],
    prerequisites: ['marl-2.4'],
    learningObjectives: [
      'Explain policy evaluation, policy improvement, and how they combine into policy iteration',
      'Describe value iteration as a simplified alternative',
      'Understand why DP methods require a known model and finite state-action spaces',
    ],
    keyConcepts: [
      'Policy evaluation: iteratively computing V^π by applying the Bellman equation',
      'Policy improvement: greedily updating the policy based on current value estimates',
      'Policy iteration: alternating evaluation and improvement until convergence',
      'Value iteration: combining evaluation and improvement into a single update step',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-2.5.mdx',
  },
  {
    id: 'marl-2.6',
    moduleId: 'marl-mod-2',
    title: 'Temporal-Difference Learning: Sarsa and Q-Learning',
    description: 'Learn model-free RL methods that learn from experience: TD learning, Sarsa (on-policy), and Q-learning (off-policy).',
    order: 6,
    sourceSections: ['2.6'],
    prerequisites: ['marl-2.5'],
    learningObjectives: [
      'Explain the TD update rule and how it bootstraps from estimated values',
      'Distinguish Sarsa (on-policy) from Q-learning (off-policy)',
      'Understand exploration vs. exploitation and the role of ε-greedy policies',
    ],
    keyConcepts: [
      'TD learning: updating value estimates using the immediate reward plus a bootstrapped estimate',
      'Sarsa: on-policy TD that updates Q(s,a) using the action actually taken next',
      'Q-learning: off-policy TD that updates Q(s,a) using the maximum over next actions',
      'ε-greedy: with probability ε take a random action, otherwise take the greedy action',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-2.6.mdx',
  },
  {
    id: 'marl-2.7',
    moduleId: 'marl-mod-2',
    title: 'Evaluation with Learning Curves',
    description: 'Understand how to evaluate RL agents using learning curves, return metrics, and proper experimental methodology.',
    order: 7,
    sourceSections: ['2.7', '2.8', '2.9'],
    prerequisites: ['marl-2.6'],
    learningObjectives: [
      'Interpret learning curves and identify signs of convergence or instability',
      'Understand the importance of averaging over multiple random seeds',
      'Know common evaluation pitfalls and best practices in RL',
    ],
    keyConcepts: [
      'Learning curve: plotting episode return (or average return) against training episodes',
      'Smoothing: using moving averages to visualise trends through noisy returns',
      'Multiple seeds: running experiments with different random seeds to measure variability',
      'Convergence: when the learning curve plateaus at a stable performance level',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-2.7.mdx',
  },

  // ---------------------------------------------------------------------------
  // Module 3: Game Theory Foundations (Chapter 3)
  // ---------------------------------------------------------------------------
  {
    id: 'marl-3.1',
    moduleId: 'marl-mod-3',
    title: 'Normal-Form Games',
    description: 'Define normal-form (matrix) games, payoff matrices, and the key types: common-reward, zero-sum, and general-sum.',
    order: 1,
    sourceSections: ['3.1'],
    prerequisites: ['marl-2.7'],
    learningObjectives: [
      'Define a normal-form game and its components: players, actions, utility functions',
      'Read and construct payoff matrices for two-player games',
      'Classify games as common-reward, zero-sum, or general-sum',
    ],
    keyConcepts: [
      'Normal-form game: a one-shot simultaneous-move game defined by players, action sets, and payoffs',
      'Payoff matrix: a table showing the utility for each combination of player actions',
      'Zero-sum game: one agent\'s gain is the other\'s loss (utilities sum to zero)',
      'General-sum game: payoffs can have any structure — cooperation and competition can coexist',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-3.1.mdx',
  },
  {
    id: 'marl-3.2',
    moduleId: 'marl-mod-3',
    title: 'Repeated Normal-Form Games',
    description: 'Extend one-shot games to repeated interactions where agents can condition behaviour on history.',
    order: 2,
    sourceSections: ['3.2'],
    prerequisites: ['marl-3.1'],
    learningObjectives: [
      'Explain how repeating a game changes the strategic landscape',
      'Understand the role of history-dependent strategies (e.g., tit-for-tat)',
      'State the folk theorem informally: repeated play enables cooperative outcomes',
    ],
    keyConcepts: [
      'Repeated game: a stage game played multiple (possibly infinite) times',
      'History: the sequence of joint actions in all previous rounds',
      'Strategy in repeated games: a mapping from histories to actions',
      'Folk theorem: in infinitely repeated games, many outcomes can be sustained as equilibria',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-3.2.mdx',
  },
  {
    id: 'marl-3.3',
    moduleId: 'marl-mod-3',
    title: 'Stochastic Games',
    description: 'Generalise MDPs and repeated games into stochastic (Markov) games where state transitions depend on joint actions.',
    order: 3,
    sourceSections: ['3.3'],
    prerequisites: ['marl-3.2'],
    learningObjectives: [
      'Define a stochastic game and its components',
      'Show how both MDPs and repeated games are special cases of stochastic games',
      'Understand how joint actions affect state transitions in multi-agent settings',
    ],
    keyConcepts: [
      'Stochastic game: tuple (I, S, {A_i}, T, {R_i}, γ) with state-dependent interactions',
      'MDP as a special case: a stochastic game with a single agent',
      'Repeated game as a special case: a stochastic game with a single state',
      'Joint action space: the Cartesian product of all individual action spaces',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-3.3.mdx',
  },
  {
    id: 'marl-3.4',
    moduleId: 'marl-mod-3',
    title: 'Partially Observable Stochastic Games',
    description: 'Extend stochastic games with partial observability — agents receive private observations instead of the full state.',
    order: 4,
    sourceSections: ['3.4'],
    prerequisites: ['marl-3.3'],
    learningObjectives: [
      'Define a POSG and explain the observation function',
      'Understand why partial observability makes multi-agent problems much harder',
      'Connect POSGs to the single-agent POMDP as a special case',
    ],
    keyConcepts: [
      'POSG: a stochastic game where each agent receives a private observation instead of the full state',
      'Observation function O_i(o|s,a): probability agent i sees observation o after joint action a in state s',
      'Belief state: an agent\'s probability distribution over possible states given its history',
      'Dec-POMDP: a cooperative POSG where all agents share a common reward function',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-3.4.mdx',
  },
  {
    id: 'marl-3.5',
    moduleId: 'marl-mod-3',
    title: 'Communication and Knowledge in Games',
    description: 'Understand how communication, common knowledge, and information asymmetry shape multi-agent interactions.',
    order: 5,
    sourceSections: ['3.5', '3.6'],
    prerequisites: ['marl-3.4'],
    learningObjectives: [
      'Explain the difference between implicit and explicit communication',
      'Define common knowledge and understand its role in coordination',
      'Understand how information asymmetry affects strategic behaviour',
    ],
    keyConcepts: [
      'Explicit communication: agents send messages through a dedicated channel',
      'Implicit communication: agents infer information from observed actions',
      'Common knowledge: a fact is common knowledge if everyone knows it, and knows that everyone knows it, etc.',
      'Information asymmetry: agents have different information about the environment or each other',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-3.5.mdx',
  },
  {
    id: 'marl-3.6',
    moduleId: 'marl-mod-3',
    title: 'From RL to Game Theory: A Dictionary',
    description: 'Map concepts between RL and game theory — see how MDPs, POMDPs, policies, and value functions translate to game-theoretic counterparts.',
    order: 6,
    sourceSections: ['3.7', '3.8'],
    prerequisites: ['marl-3.5'],
    learningObjectives: [
      'Map RL concepts (state, action, policy, value) to game-theoretic counterparts',
      'Understand the hierarchy: normal-form ⊂ repeated ⊂ stochastic ⊂ POSG',
      'Know the key modelling choices when formalising a multi-agent problem',
    ],
    keyConcepts: [
      'MDP ↔ single-agent stochastic game; POMDP ↔ single-agent POSG',
      'Policy in RL ↔ strategy in game theory',
      'Value function in RL ↔ expected utility in game theory',
      'Model hierarchy: each game model generalises the previous one',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-3.6.mdx',
  },

  // ---------------------------------------------------------------------------
  // Module 4: Solution Concepts for Games (Chapter 4)
  // ---------------------------------------------------------------------------
  {
    id: 'marl-4.1',
    moduleId: 'marl-mod-4',
    title: 'Joint Policies and Expected Returns',
    description: 'Define joint policies, joint action profiles, and expected returns in multi-agent settings.',
    order: 1,
    sourceSections: ['4.1'],
    prerequisites: ['marl-3.6'],
    learningObjectives: [
      'Define a joint policy and a joint action profile',
      'Compute expected returns under a joint policy in a normal-form game',
      'Understand why evaluating joint policies differs from the single-agent case',
    ],
    keyConcepts: [
      'Joint policy: the combination of all agents\' individual policies',
      'Joint action profile: a tuple of one action per agent',
      'Expected return: the utility an agent expects under a given joint policy',
      'Mixed strategy: a probability distribution over actions (as opposed to a pure strategy)',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-4.1.mdx',
  },
  {
    id: 'marl-4.2',
    moduleId: 'marl-mod-4',
    title: 'Best Response and Minimax Strategies',
    description: 'Define best-response strategies and minimax strategies for zero-sum games.',
    order: 2,
    sourceSections: ['4.2', '4.3'],
    prerequisites: ['marl-4.1'],
    learningObjectives: [
      'Define a best response to a given opponent strategy',
      'Explain the minimax strategy and the minimax theorem for zero-sum games',
      'Compute best responses and minimax values in simple matrix games',
    ],
    keyConcepts: [
      'Best response: a policy that maximises an agent\'s return given the other agents\' policies',
      'Dominant strategy: a best response regardless of what others do',
      'Minimax strategy: maximise your minimum payoff (optimal in zero-sum games)',
      'Minimax theorem (von Neumann): in two-player zero-sum games, max-min = min-max',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-4.2.mdx',
  },
  {
    id: 'marl-4.3',
    moduleId: 'marl-mod-4',
    title: 'Nash Equilibrium',
    description: 'Define Nash equilibrium, prove existence via Nash\'s theorem, and explore its properties and limitations.',
    order: 3,
    sourceSections: ['4.4', '4.5'],
    prerequisites: ['marl-4.2'],
    learningObjectives: [
      'Define a Nash equilibrium and verify whether a joint policy is a Nash equilibrium',
      'State Nash\'s theorem: every finite game has at least one Nash equilibrium in mixed strategies',
      'Identify Nash equilibria in simple 2x2 games',
    ],
    keyConcepts: [
      'Nash equilibrium: a joint policy where no agent can improve by unilaterally changing its strategy',
      'Nash\'s theorem: every finite game has at least one NE (possibly in mixed strategies)',
      'Multiple equilibria: a game can have many Nash equilibria with different payoffs',
      'NE as a fixed point: every agent is playing a best response to the others',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-4.3.mdx',
  },
  {
    id: 'marl-4.4',
    moduleId: 'marl-mod-4',
    title: 'Correlated Equilibrium',
    description: 'Understand correlated equilibrium as a relaxation of Nash equilibrium that allows coordination via shared signals.',
    order: 4,
    sourceSections: ['4.6'],
    prerequisites: ['marl-4.3'],
    learningObjectives: [
      'Define correlated equilibrium and explain how it differs from Nash equilibrium',
      'Understand the role of a correlation device (traffic light analogy)',
      'Know that every Nash equilibrium is a correlated equilibrium but not vice versa',
    ],
    keyConcepts: [
      'Correlated equilibrium: agents follow recommendations from a shared signal if no one benefits from deviating',
      'Correlation device: a mechanism that sends private signals (recommendations) to each agent',
      'CE generalises NE: the set of CEs is a convex superset of the set of NEs',
      'Coarse correlated equilibrium: agents commit to follow the device before seeing the recommendation',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-4.4.mdx',
  },
  {
    id: 'marl-4.5',
    moduleId: 'marl-mod-4',
    title: 'Limitations of Equilibrium Solutions',
    description: 'Critically examine why equilibria can be unsatisfying: multiplicity, inefficiency, and questionable rationality assumptions.',
    order: 5,
    sourceSections: ['4.7'],
    prerequisites: ['marl-4.4'],
    learningObjectives: [
      'Explain the equilibrium selection problem — what to do when there are multiple equilibria',
      'Understand why Nash equilibria can be Pareto-dominated (inefficient)',
      'Question the common knowledge of rationality assumption underlying equilibrium analysis',
    ],
    keyConcepts: [
      'Equilibrium selection: choosing among multiple equilibria is itself a hard problem',
      'Pareto-dominated equilibria: an NE where all players could be better off at another outcome',
      'Rationality assumptions: equilibria assume fully rational agents with common knowledge',
      'Bounded rationality: real agents have limited computation and information',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-4.5.mdx',
  },
  {
    id: 'marl-4.6',
    moduleId: 'marl-mod-4',
    title: 'Pareto Optimality and Social Welfare',
    description: 'Define Pareto optimality, social welfare, and understand the tension between individual and collective rationality.',
    order: 6,
    sourceSections: ['4.8', '4.9'],
    prerequisites: ['marl-4.5'],
    learningObjectives: [
      'Define Pareto optimality and Pareto dominance',
      'Define social welfare (utilitarian and egalitarian) as a measure of collective good',
      'Understand the tension between Nash equilibrium and Pareto optimality (e.g., Prisoner\'s Dilemma)',
    ],
    keyConcepts: [
      'Pareto optimal: no agent can be made better off without making another worse off',
      'Pareto dominance: outcome A dominates B if all agents prefer A and at least one strictly',
      'Social welfare: aggregate utility — utilitarian (sum) or egalitarian (minimum)',
      'Price of anarchy: the ratio of worst Nash equilibrium welfare to optimal welfare',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-4.6.mdx',
  },
  {
    id: 'marl-4.7',
    moduleId: 'marl-mod-4',
    title: 'No-Regret Learning',
    description: 'Understand external regret and how no-regret learning connects to equilibrium convergence.',
    order: 7,
    sourceSections: ['4.10'],
    prerequisites: ['marl-4.6'],
    learningObjectives: [
      'Define external regret and the no-regret property',
      'Understand why no-regret learning is desirable in repeated games',
      'Know that if all agents use no-regret algorithms, play converges to a coarse correlated equilibrium',
    ],
    keyConcepts: [
      'External regret: the difference between cumulative payoff and the best fixed action in hindsight',
      'No-regret learning: average regret goes to zero as the number of rounds grows',
      'Connection to CCE: joint play of no-regret learners converges to the set of coarse correlated equilibria',
      'Regret minimisation as a learning objective: perform nearly as well as the best fixed strategy',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-4.7.mdx',
  },
  {
    id: 'marl-4.8',
    moduleId: 'marl-mod-4',
    title: 'Computational Complexity of Equilibria',
    description: 'Understand the computational hardness of finding Nash equilibria and why this matters for MARL algorithms.',
    order: 8,
    sourceSections: ['4.11', '4.12'],
    prerequisites: ['marl-4.7'],
    learningObjectives: [
      'Understand that finding a Nash equilibrium is computationally hard (PPAD-complete)',
      'Know that computing a correlated equilibrium is tractable (solvable via linear programming)',
      'Appreciate the implications of computational complexity for scalable MARL algorithms',
    ],
    keyConcepts: [
      'PPAD-completeness: finding an NE is unlikely to have a polynomial-time algorithm',
      'Linear programming: correlated equilibria can be found efficiently via LP',
      'Support enumeration: a brute-force method for small games',
      'Lemke-Howson algorithm: a classic method for two-player games that may take exponential time',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-4.8.mdx',
  },

  // ---------------------------------------------------------------------------
  // Module 5: MARL First Steps and Challenges (Chapter 5)
  // ---------------------------------------------------------------------------
  {
    id: 'marl-5.1',
    moduleId: 'marl-mod-5',
    title: 'The General Multi-Agent Learning Process',
    description: 'Understand the general MARL learning loop and how it extends the single-agent RL process.',
    order: 1,
    sourceSections: ['5.1'],
    prerequisites: ['marl-4.8'],
    learningObjectives: [
      'Describe the general MARL learning loop with multiple simultaneously learning agents',
      'Explain how the transition and reward functions are now influenced by all agents\' actions',
      'Understand the fundamental complication: each agent\'s environment includes the other agents',
    ],
    keyConcepts: [
      'MARL loop: all agents simultaneously observe, act, receive rewards, and update policies',
      'Non-stationary environment: from each agent\'s perspective, the environment changes as others learn',
      'Moving target problem: the optimal policy shifts because other agents\' policies shift',
      'Multi-agent credit assignment: decomposing team outcomes into individual contributions',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-5.1.mdx',
  },
  {
    id: 'marl-5.2',
    moduleId: 'marl-mod-5',
    title: 'Convergence in Multi-Agent Settings',
    description: 'Examine what convergence means in MARL and why single-agent convergence guarantees break down.',
    order: 2,
    sourceSections: ['5.2'],
    prerequisites: ['marl-5.1'],
    learningObjectives: [
      'Explain why Q-learning convergence guarantees do not transfer directly to multi-agent settings',
      'Define convergence criteria for MARL: convergence to a Nash equilibrium, to rationality, etc.',
      'Understand the difference between convergence in self-play vs. convergence against arbitrary opponents',
    ],
    keyConcepts: [
      'Single-agent convergence: Q-learning converges to optimal Q* in stationary environments',
      'MARL non-stationarity: other learning agents violate the stationary environment assumption',
      'Convergence to Nash: agents learn policies forming a Nash equilibrium',
      'Convergence in self-play vs. robustness: converging against yourself ≠ converging against others',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-5.2.mdx',
  },
  {
    id: 'marl-5.3',
    moduleId: 'marl-mod-5',
    title: 'Central Learning and Independent Learning',
    description: 'Compare centralised learning (one controller for all agents) with independent learning (each agent learns alone).',
    order: 3,
    sourceSections: ['5.3'],
    prerequisites: ['marl-5.2'],
    learningObjectives: [
      'Define centralised learning and independent learning and their assumptions',
      'Explain the exponential blowup of joint action spaces in centralised learning',
      'Understand why independent learning is practical but lacks theoretical guarantees',
    ],
    keyConcepts: [
      'Centralised learning: a single learner controls all agents using the joint action space',
      'Joint action space: |A|^n grows exponentially with the number of agents n',
      'Independent learning: each agent learns its own policy ignoring others (treating them as environment)',
      'IQL (Independent Q-Learning): each agent runs Q-learning independently',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-5.3.mdx',
  },
  {
    id: 'marl-5.4',
    moduleId: 'marl-mod-5',
    title: 'Non-Stationarity and Equilibrium Selection',
    description: 'Deep dive into non-stationarity and the equilibrium selection problem in multi-agent learning.',
    order: 4,
    sourceSections: ['5.4.1', '5.4.2'],
    prerequisites: ['marl-5.3'],
    learningObjectives: [
      'Explain why non-stationarity is the fundamental challenge in MARL',
      'Describe how non-stationarity manifests in practice (oscillating policies, failure to converge)',
      'Understand the equilibrium selection problem: learning may converge to a suboptimal equilibrium',
    ],
    keyConcepts: [
      'Non-stationarity: the transition and reward functions change from each agent\'s perspective',
      'Policy oscillation: agents may cycle through strategies without converging',
      'Equilibrium selection: with multiple equilibria, which one will learning reach?',
      'Miscoordination: agents may converge to incompatible strategies',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-5.4.mdx',
  },
  {
    id: 'marl-5.5',
    moduleId: 'marl-mod-5',
    title: 'Credit Assignment and Scaling',
    description: 'Understand the multi-agent credit assignment problem and the challenge of scaling MARL to many agents.',
    order: 5,
    sourceSections: ['5.4.3', '5.4.4'],
    prerequisites: ['marl-5.4'],
    learningObjectives: [
      'Explain the multi-agent credit assignment problem',
      'Understand why shared rewards make it hard to determine each agent\'s contribution',
      'Appreciate the computational challenges of scaling MARL beyond a few agents',
    ],
    keyConcepts: [
      'Credit assignment: determining which agent\'s action was responsible for a team outcome',
      'Lazy agent problem: an agent that contributes nothing but still receives team reward',
      'Relative overgeneralisation: agents converge to a suboptimal joint policy because deviating alone fails',
      'Scalability: computation, communication, and coordination costs grow with agent count',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-5.5.mdx',
  },
  {
    id: 'marl-5.6',
    moduleId: 'marl-mod-5',
    title: 'Self-Play and Mixed-Play',
    description: 'Understand self-play as a training paradigm and how it differs from mixed-play with diverse opponents.',
    order: 6,
    sourceSections: ['5.5'],
    prerequisites: ['marl-5.5'],
    learningObjectives: [
      'Define self-play and explain its role in competitive and cooperative MARL',
      'Understand the limitations of self-play: potential for cyclic strategies and overspecialisation',
      'Explain mixed-play and population-based approaches as alternatives',
    ],
    keyConcepts: [
      'Self-play: an agent trains against copies of itself',
      'Benefits: provides a curriculum of increasingly strong opponents',
      'Cyclic strategies: self-play can cycle through rock-paper-scissors-style policies',
      'Mixed-play: training against a diverse population of strategies',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-5.6.mdx',
  },
  {
    id: 'marl-5.7',
    moduleId: 'marl-mod-5',
    title: 'Chapter Integration: The MARL Landscape',
    description: 'Synthesise the challenges and approaches into a complete picture of the MARL research landscape.',
    order: 7,
    sourceSections: ['5.6'],
    prerequisites: ['marl-5.6'],
    learningObjectives: [
      'Organise MARL approaches along key dimensions: centralised/independent, cooperative/competitive',
      'Map challenges (non-stationarity, credit assignment, scalability) to solution strategies',
      'Preview how foundational algorithms in the next module address these challenges',
    ],
    keyConcepts: [
      'MARL taxonomy: organising methods by information structure and training paradigm',
      'Challenge-solution mapping: each challenge motivates specific algorithmic innovations',
      'Cooperative MARL: shared reward, team coordination, credit assignment',
      'Competitive MARL: zero-sum games, minimax, opponent modelling',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-5.7.mdx',
  },

  // ---------------------------------------------------------------------------
  // Module 6: Foundational MARL Algorithms (Chapter 6)
  // ---------------------------------------------------------------------------
  {
    id: 'marl-6.1',
    moduleId: 'marl-mod-6',
    title: 'Value Iteration for Games',
    description: 'Extend dynamic programming value iteration to two-player zero-sum stochastic games.',
    order: 1,
    sourceSections: ['6.1'],
    prerequisites: ['marl-5.7'],
    learningObjectives: [
      'Extend single-agent value iteration to the two-player zero-sum game setting',
      'Understand how the minimax operator replaces the max operator in the Bellman update',
      'Know the conditions under which value iteration for games converges',
    ],
    keyConcepts: [
      'Minimax value iteration: V(s) = max_{a1} min_{a2} [R(s,a1,a2) + γ Σ T(s\'|s,a1,a2) V(s\')]',
      'Shapley\'s theorem: minimax value iteration converges in zero-sum stochastic games',
      'Stage game: the normal-form game at each state of the stochastic game',
      'Linear programming: solving the minimax at each state via LP',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-6.1.mdx',
  },
  {
    id: 'marl-6.2',
    moduleId: 'marl-mod-6',
    title: 'Joint-Action TD Learning',
    description: 'Introduce joint-action learning where agents learn Q-values over the joint action space.',
    order: 2,
    sourceSections: ['6.2'],
    prerequisites: ['marl-6.1'],
    learningObjectives: [
      'Define joint-action Q-values Q(s, a1, a2, ..., an)',
      'Explain the TD update for joint-action learners',
      'Understand the key assumption: observability of all agents\' actions',
    ],
    keyConcepts: [
      'Joint-action Q-value: Q(s, a) where a is the joint action of all agents',
      'TD update: Q(s,a) ← Q(s,a) + α[r + γ V(s\') - Q(s,a)]',
      'Action observability: agents can see what actions others took after each step',
      'Equilibrium computation: the value V(s\') depends on the solution concept used',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-6.2.mdx',
  },
  {
    id: 'marl-6.3',
    moduleId: 'marl-mod-6',
    title: 'Minimax, Nash, and Correlated Q-Learning',
    description: 'Study three classic MARL algorithms that combine Q-learning with game-theoretic solution concepts.',
    order: 3,
    sourceSections: ['6.2.1', '6.2.2', '6.2.3', '6.2.4'],
    prerequisites: ['marl-6.2'],
    learningObjectives: [
      'Explain Minimax-Q learning and its guarantees in zero-sum games',
      'Describe Nash-Q learning and why it extends to general-sum games',
      'Understand Correlated-Q learning and its use of linear programming',
    ],
    keyConcepts: [
      'Minimax-Q: Q-learning with minimax operator; converges in zero-sum games',
      'Nash-Q: Q-learning with Nash equilibrium computation at each state; general-sum',
      'Correlated-Q: Q-learning with correlated equilibrium via linear programming',
      'Convergence limitations: Nash-Q and Correlated-Q lack general convergence guarantees',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-6.3.mdx',
  },
  {
    id: 'marl-6.4',
    moduleId: 'marl-mod-6',
    title: 'Agent Modeling and Fictitious Play',
    description: 'Introduce agent modelling — maintaining beliefs about opponents — and fictitious play as a foundational example.',
    order: 4,
    sourceSections: ['6.3', '6.3.1'],
    prerequisites: ['marl-6.3'],
    learningObjectives: [
      'Explain the concept of agent modelling: maintaining beliefs about other agents\' strategies',
      'Describe fictitious play and how agents compute best responses to empirical action frequencies',
      'Know that fictitious play converges in certain game classes but not in general',
    ],
    keyConcepts: [
      'Agent modelling: building a model of other agents\' behaviour to improve decision-making',
      'Fictitious play: assume opponents play their historical empirical distribution; best respond to it',
      'Empirical frequency: the fraction of times each action has been played historically',
      'Convergence: fictitious play converges in zero-sum, potential, and 2×n games',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-6.4.mdx',
  },
  {
    id: 'marl-6.5',
    moduleId: 'marl-mod-6',
    title: 'Joint-Action Learning with Agent Models',
    description: 'Combine joint-action learning with explicit models of other agents\' strategies.',
    order: 5,
    sourceSections: ['6.3.2'],
    prerequisites: ['marl-6.4'],
    learningObjectives: [
      'Explain how agent models replace the need to observe or assume opponents\' strategies',
      'Describe joint-action learning variants that use opponent models for value computation',
      'Understand the tradeoff between model accuracy and computational cost',
    ],
    keyConcepts: [
      'Model-based value computation: using predicted opponent actions to compute expected values',
      'Frequency-based models: estimating opponent strategy from observed action frequencies',
      'Maximum likelihood models: the simplest opponent model — assume the most likely action',
      'Tradeoff: more sophisticated models are more accurate but more expensive',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-6.5.mdx',
  },
  {
    id: 'marl-6.6',
    moduleId: 'marl-mod-6',
    title: 'Bayesian Learning and Value of Information',
    description: 'Understand Bayesian approaches to agent modelling and the value of gathering information about opponents.',
    order: 6,
    sourceSections: ['6.3.3'],
    prerequisites: ['marl-6.5'],
    learningObjectives: [
      'Explain Bayesian agent modelling: maintaining a posterior over opponent strategy types',
      'Understand the exploration-exploitation tradeoff in multi-agent settings',
      'Define the value of information and why it matters when other agents are partially observable',
    ],
    keyConcepts: [
      'Bayesian agent modelling: maintaining a prior and updating beliefs via Bayes\' rule',
      'Type-based reasoning: assuming the opponent belongs to one of several known types',
      'Value of information: the expected gain from learning more about the opponent',
      'Exploration vs. exploitation: gathering info about opponents vs. exploiting current beliefs',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-6.6.mdx',
  },
  {
    id: 'marl-6.7',
    moduleId: 'marl-mod-6',
    title: 'Policy Gradient Methods for MARL',
    description: 'Apply policy gradient methods to multi-agent settings where agents directly optimise parameterised policies.',
    order: 7,
    sourceSections: ['6.4.1', '6.4.2'],
    prerequisites: ['marl-6.6'],
    learningObjectives: [
      'Explain why policy gradient methods are natural for multi-agent settings',
      'Describe how infinitesimal gradient ascent (IGA) works in multi-agent games',
      'Understand the connection between gradient dynamics and game dynamics',
    ],
    keyConcepts: [
      'Policy gradient in MARL: each agent updates its policy parameters via gradient ascent on expected return',
      'Infinitesimal gradient ascent (IGA): simultaneous gradient ascent with infinitesimally small steps',
      'Gradient dynamics: the trajectory of joint policy parameters as all agents take gradient steps',
      'Convergence issues: gradient dynamics can cycle or diverge in general games',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-6.7.mdx',
  },
  {
    id: 'marl-6.8',
    moduleId: 'marl-mod-6',
    title: 'Win or Learn Fast (WoLF)',
    description: 'Study the WoLF principle — use variable learning rates to improve convergence — and WoLF-IGA and WoLF-PHC algorithms.',
    order: 8,
    sourceSections: ['6.4.3', '6.4.4', '6.4.5'],
    prerequisites: ['marl-6.7'],
    learningObjectives: [
      'Explain the WoLF principle: learn fast when losing, slowly when winning',
      'Describe WoLF-IGA and WoLF-PHC as concrete instantiations',
      'Understand why variable learning rates help avoid oscillation in multi-agent gradient dynamics',
    ],
    keyConcepts: [
      'WoLF principle: use a higher learning rate when performing below the Nash equilibrium value',
      'WoLF-IGA: IGA with variable step sizes based on the WoLF principle',
      'WoLF-PHC: policy hill climbing with WoLF-style variable learning rates',
      'Convergence: WoLF-IGA converges in certain two-player two-action games',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-6.8.mdx',
  },
  {
    id: 'marl-6.9',
    moduleId: 'marl-mod-6',
    title: 'Regret Matching and No-Regret Learning',
    description: 'Learn regret matching — a practical no-regret algorithm — and its application to finding equilibria.',
    order: 9,
    sourceSections: ['6.5'],
    prerequisites: ['marl-6.8'],
    learningObjectives: [
      'Define regret matching and how it uses cumulative regret to update action probabilities',
      'Explain counterfactual regret minimisation (CFR) and its role in solving imperfect-information games',
      'Know that regret matching converges to a coarse correlated equilibrium',
    ],
    keyConcepts: [
      'Regret matching: play actions proportionally to their positive cumulative regret',
      'Cumulative regret: total regret for not having played a given action in hindsight',
      'CFR (Counterfactual Regret Minimisation): extends regret matching to extensive-form games',
      'Poker solving: CFR is the foundation of superhuman poker AI (Libratus, Pluribus)',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-6.9.mdx',
  },
  {
    id: 'marl-6.10',
    moduleId: 'marl-mod-6',
    title: 'Chapter Integration: Algorithm Landscape',
    description: 'Map all foundational MARL algorithms to the challenges they address and the game settings where they apply.',
    order: 10,
    sourceSections: ['6.6'],
    prerequisites: ['marl-6.9'],
    learningObjectives: [
      'Organise foundational MARL algorithms by approach: value-based, policy-based, agent-modelling, no-regret',
      'Match algorithms to game settings: zero-sum, general-sum, cooperative',
      'Identify the limitations that motivate the move to deep MARL methods',
    ],
    keyConcepts: [
      'Algorithm taxonomy: value-based (Minimax-Q, Nash-Q) vs. policy-based (IGA, WoLF) vs. model-based (FP)',
      'Convergence landscape: what converges where and with what guarantees',
      'Scalability gap: tabular methods cannot handle large state-action spaces',
      'Motivation for deep MARL: function approximation needed for real-world problems',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-6.10.mdx',
  },

  // ---------------------------------------------------------------------------
  // Module 7: Deep Learning and Deep RL (Chapters 7 + 8)
  // ---------------------------------------------------------------------------
  {
    id: 'marl-7.1',
    moduleId: 'marl-mod-7',
    title: 'Function Approximation for RL',
    description: 'Understand why tabular methods fail at scale and how function approximation addresses this.',
    order: 1,
    sourceSections: ['7.1', '7.2'],
    prerequisites: ['marl-6.10'],
    learningObjectives: [
      'Explain why tabular methods are impractical for large or continuous state spaces',
      'Describe how function approximation replaces lookup tables with parameterised functions',
      'Understand the deadly triad: function approximation + bootstrapping + off-policy can diverge',
    ],
    keyConcepts: [
      'Curse of dimensionality: the number of states grows exponentially with state dimensions',
      'Function approximation: using a parameterised function to estimate value functions or policies',
      'Generalisation: function approximation allows learning about unvisited states',
      'Deadly triad: the combination of function approximation, bootstrapping, and off-policy learning can diverge',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-7.1.mdx',
  },
  {
    id: 'marl-7.2',
    moduleId: 'marl-mod-7',
    title: 'Feedforward Neural Networks',
    description: 'Review the architecture of feedforward neural networks: layers, activations, and the universal approximation theorem.',
    order: 2,
    sourceSections: ['7.3'],
    prerequisites: ['marl-7.1'],
    learningObjectives: [
      'Describe the architecture of a feedforward neural network: input, hidden, and output layers',
      'Explain common activation functions: ReLU, sigmoid, tanh, softmax',
      'State the universal approximation theorem and its practical significance',
    ],
    keyConcepts: [
      'Feedforward network: layers connected sequentially with no cycles',
      'Activation functions: non-linear functions that enable the network to learn complex mappings',
      'Universal approximation theorem: a sufficiently wide single-hidden-layer network can approximate any continuous function',
      'Depth vs. width: deeper networks are often more efficient than wider ones',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-7.2.mdx',
  },
  {
    id: 'marl-7.3',
    moduleId: 'marl-mod-7',
    title: 'Gradient-Based Optimisation and Backpropagation',
    description: 'Understand how neural networks are trained: loss functions, gradient descent, and backpropagation.',
    order: 3,
    sourceSections: ['7.4'],
    prerequisites: ['marl-7.2'],
    learningObjectives: [
      'Explain how loss functions measure the quality of network predictions',
      'Describe stochastic gradient descent and common optimisers (SGD, Adam)',
      'Understand backpropagation as efficient computation of gradients via the chain rule',
    ],
    keyConcepts: [
      'Loss function: maps predictions and targets to a scalar measuring prediction error',
      'Gradient descent: iteratively updating parameters in the direction that reduces loss',
      'Backpropagation: efficient gradient computation using the chain rule of calculus',
      'Adam optimiser: adaptive learning rates per parameter, widely used in deep RL',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-7.3.mdx',
  },
  {
    id: 'marl-7.4',
    moduleId: 'marl-mod-7',
    title: 'Convolutional and Recurrent Networks',
    description: 'Review CNNs for spatial data and RNNs/LSTMs for sequential data — both essential for deep RL.',
    order: 4,
    sourceSections: ['7.5', '7.6'],
    prerequisites: ['marl-7.3'],
    learningObjectives: [
      'Explain how convolutional neural networks exploit spatial structure',
      'Describe recurrent neural networks and LSTMs for processing sequences',
      'Understand when to use CNNs vs. RNNs in RL applications',
    ],
    keyConcepts: [
      'CNN: uses convolutional filters to detect local spatial patterns; great for pixel observations',
      'RNN: maintains hidden state across timesteps; processes variable-length sequences',
      'LSTM: a gated RNN variant that handles long-term dependencies better than vanilla RNNs',
      'CNN for RL: processing image observations (Atari, StarCraft); RNN for RL: handling partial observability',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-7.4.mdx',
  },
  {
    id: 'marl-7.5',
    moduleId: 'marl-mod-7',
    title: 'Deep Q-Networks',
    description: 'Study DQN — the first successful combination of deep learning with RL — and its key innovations.',
    order: 5,
    sourceSections: ['8.1'],
    prerequisites: ['marl-7.4'],
    learningObjectives: [
      'Explain how DQN approximates Q-values with a neural network',
      'Describe the two key innovations: experience replay and target networks',
      'Understand why these innovations stabilise training',
    ],
    keyConcepts: [
      'DQN: a neural network that maps states to Q-values for each action',
      'Experience replay: storing transitions in a buffer and sampling random minibatches',
      'Target network: a slowly-updated copy of the Q-network used to compute stable targets',
      'Atari benchmark: DQN achieved superhuman performance on multiple Atari games',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-7.5.mdx',
  },
  {
    id: 'marl-7.6',
    moduleId: 'marl-mod-7',
    title: 'Policy Gradient Theorem and REINFORCE',
    description: 'Derive the policy gradient theorem and implement REINFORCE as the simplest policy gradient algorithm.',
    order: 6,
    sourceSections: ['8.2.1', '8.2.2', '8.2.3'],
    prerequisites: ['marl-7.5'],
    learningObjectives: [
      'State and intuitively explain the policy gradient theorem',
      'Describe the REINFORCE algorithm and its use of Monte Carlo returns',
      'Understand the high variance problem of REINFORCE and the role of baselines',
    ],
    keyConcepts: [
      'Policy gradient theorem: ∇J(θ) = E[∇log π(a|s;θ) · Q^π(s,a)]',
      'REINFORCE: sample trajectories, compute returns, update policy via gradient ascent',
      'High variance: Monte Carlo returns are noisy, making learning slow',
      'Baseline: subtracting a state-dependent baseline reduces variance without introducing bias',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-7.6.mdx',
  },
  {
    id: 'marl-7.7',
    moduleId: 'marl-mod-7',
    title: 'Actor-Critic Methods: A2C and PPO',
    description: 'Combine policy gradients with value function baselines: advantage actor-critic (A2C) and proximal policy optimisation (PPO).',
    order: 7,
    sourceSections: ['8.2.4', '8.2.5', '8.2.6'],
    prerequisites: ['marl-7.6'],
    learningObjectives: [
      'Explain the actor-critic architecture: actor (policy) + critic (value function)',
      'Describe A2C and how the advantage function reduces variance',
      'Understand PPO\'s clipped objective and why it improves training stability',
    ],
    keyConcepts: [
      'Actor-critic: the actor selects actions, the critic evaluates them',
      'Advantage function A(s,a) = Q(s,a) - V(s): how much better an action is than average',
      'A2C (Advantage Actor-Critic): uses the advantage to reduce policy gradient variance',
      'PPO (Proximal Policy Optimisation): clips the policy ratio to prevent large updates',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-7.7.mdx',
  },
  {
    id: 'marl-7.8',
    moduleId: 'marl-mod-7',
    title: 'Policy Gradients in Practice',
    description: 'Practical considerations for implementing policy gradient methods: hyperparameters, normalisation, and common pitfalls.',
    order: 8,
    sourceSections: ['8.2.7', '8.2.8'],
    prerequisites: ['marl-7.7'],
    learningObjectives: [
      'Identify key hyperparameters and their typical ranges for policy gradient methods',
      'Understand reward normalisation, observation normalisation, and gradient clipping',
      'Know common failure modes and debugging strategies',
    ],
    keyConcepts: [
      'Reward normalisation: scaling rewards to have zero mean and unit variance',
      'Gradient clipping: bounding gradient norms to prevent destructive large updates',
      'Entropy bonus: encouraging exploration by adding policy entropy to the objective',
      'Debugging tips: check reward scale, learning rate, network architecture, and entropy',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-7.8.mdx',
  },
  {
    id: 'marl-7.9',
    moduleId: 'marl-mod-7',
    title: 'Observations, States, and Histories',
    description: 'Clarify the distinction between states, observations, and histories and how to handle partial observability in deep RL.',
    order: 9,
    sourceSections: ['8.3', '8.4'],
    prerequisites: ['marl-7.8'],
    learningObjectives: [
      'Distinguish between state (full information), observation (partial information), and history',
      'Explain how RNNs and frame stacking handle partial observability',
      'Understand centralised state vs. decentralised observations in multi-agent settings',
    ],
    keyConcepts: [
      'State: the complete description of the environment at a given time',
      'Observation: what an agent actually sees (may be a partial view of the state)',
      'History: the sequence of observations and actions an agent has experienced',
      'Frame stacking: concatenating recent observations to approximate state from observations',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-7.9.mdx',
  },

  // ---------------------------------------------------------------------------
  // Module 8: Multi-Agent Deep Reinforcement Learning (Chapter 9)
  // ---------------------------------------------------------------------------
  {
    id: 'marl-8.1',
    moduleId: 'marl-mod-8',
    title: 'Training and Execution Modes (CTDE)',
    description: 'Understand centralised training with decentralised execution (CTDE) — the dominant paradigm in deep MARL.',
    order: 1,
    sourceSections: ['9.1', '9.2'],
    prerequisites: ['marl-7.9'],
    learningObjectives: [
      'Define CTDE and explain why it is the most practical deep MARL paradigm',
      'Distinguish between fully centralised, fully decentralised, and CTDE approaches',
      'Understand what information is available during training vs. execution',
    ],
    keyConcepts: [
      'CTDE: use global information during training, but agents act on local observations at test time',
      'Centralised training: access to global state, all observations, and all actions',
      'Decentralised execution: each agent acts based only on its own observation',
      'Why CTDE: training is a simulation (global info available), execution is the real world (only local info)',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-8.1.mdx',
  },
  {
    id: 'marl-8.2',
    moduleId: 'marl-mod-8',
    title: 'Independent Deep Learning',
    description: 'Apply deep RL algorithms (DQN, PPO) independently to each agent and understand the resulting challenges.',
    order: 2,
    sourceSections: ['9.3'],
    prerequisites: ['marl-8.1'],
    learningObjectives: [
      'Describe independent DQN and independent PPO as simple MARL baselines',
      'Explain why experience replay is problematic in multi-agent settings',
      'Understand the non-stationarity issue from the deep learning perspective',
    ],
    keyConcepts: [
      'IDQN: each agent runs its own DQN ignoring other agents',
      'IPPO: each agent runs independent PPO',
      'Replay buffer staleness: stored transitions become invalid as other agents\' policies change',
      'Surprising effectiveness: despite theoretical issues, independent methods often work well in practice',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-8.2.mdx',
  },
  {
    id: 'marl-8.3',
    moduleId: 'marl-mod-8',
    title: 'Multi-Agent Policy Gradients and Centralised Critics',
    description: 'Extend policy gradient methods with centralised critics that condition on global information.',
    order: 3,
    sourceSections: ['9.4.1', '9.4.2', '9.4.3'],
    prerequisites: ['marl-8.2'],
    learningObjectives: [
      'Explain how centralised critics work within the CTDE framework',
      'Describe MADDPG (Multi-Agent DDPG) and its use of centralised Q-functions',
      'Understand MAPPO and its practical benefits over independent PPO',
    ],
    keyConcepts: [
      'Centralised critic: a value function conditioned on global state or all agents\' observations',
      'Decentralised actors: each agent\'s policy conditions only on its own observation',
      'MADDPG: extends DDPG with centralised critics for each agent',
      'MAPPO: multi-agent PPO with a centralised value function; strong and simple baseline',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-8.3.mdx',
  },
  {
    id: 'marl-8.4',
    moduleId: 'marl-mod-8',
    title: 'Counterfactual Baselines and Equilibrium Selection',
    description: 'Use counterfactual baselines to solve credit assignment and understand equilibrium selection in deep MARL.',
    order: 4,
    sourceSections: ['9.4.4', '9.4.5'],
    prerequisites: ['marl-8.3'],
    learningObjectives: [
      'Explain COMA (Counterfactual Multi-Agent Policy Gradients) and its counterfactual baseline',
      'Understand how counterfactual baselines address the credit assignment problem',
      'Know how equilibrium selection manifests in deep MARL training',
    ],
    keyConcepts: [
      'COMA: uses a counterfactual baseline that marginalises over one agent\'s actions',
      'Counterfactual baseline: "what would the value be if agent i had taken a different action?"',
      'Advantage = Q(s, a) - Σ_{a\'_i} π_i(a\'_i|o_i) Q(s, (a\'_i, a_{-i}))',
      'Equilibrium selection in practice: training dynamics often pick the equilibrium, not the algorithm',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-8.4.mdx',
  },
  {
    id: 'marl-8.5',
    moduleId: 'marl-mod-8',
    title: 'Value Decomposition: VDN and QMIX',
    description: 'Learn value decomposition methods that factorise a team Q-function into individual agent utilities.',
    order: 5,
    sourceSections: ['9.5.1', '9.5.2', '9.5.3'],
    prerequisites: ['marl-8.4'],
    learningObjectives: [
      'Explain the idea of value decomposition for cooperative MARL',
      'Describe VDN (Value Decomposition Network) and its additivity assumption',
      'Describe QMIX and how the mixing network enforces monotonicity',
    ],
    keyConcepts: [
      'Value decomposition: factoring Q_tot into individual agent Q-values for decentralised execution',
      'VDN: Q_tot = Σ Q_i (simple additive decomposition)',
      'QMIX: Q_tot = f(Q_1, ..., Q_n) where f is monotone in each Q_i',
      'Mixing network: a neural network with non-negative weights that combines individual Q-values',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-8.5.mdx',
  },
  {
    id: 'marl-8.6',
    moduleId: 'marl-mod-8',
    title: 'Value Decomposition in Practice and Beyond',
    description: 'Explore practical considerations for QMIX and extensions like QTRAN, QPLEX, and weighted QMIX.',
    order: 6,
    sourceSections: ['9.5.4', '9.5.5'],
    prerequisites: ['marl-8.5'],
    learningObjectives: [
      'Understand the representational limitations of VDN and QMIX monotonicity',
      'Know about extensions: QTRAN, QPLEX, weighted QMIX',
      'Identify when value decomposition is appropriate vs. centralised critics',
    ],
    keyConcepts: [
      'Monotonicity limitation: QMIX cannot represent joint Q-functions where argmax disagrees with individual Q maxima',
      'QTRAN: exact decomposition via transformation but harder to train in practice',
      'QPLEX: duplex duelling architecture for better expressiveness',
      'When to use: value decomposition shines in cooperative settings with shared rewards',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-8.6.mdx',
  },
  {
    id: 'marl-8.7',
    moduleId: 'marl-mod-8',
    title: 'Agent Modeling with Neural Networks',
    description: 'Use neural networks to model other agents\' behaviour for improved decision-making.',
    order: 7,
    sourceSections: ['9.6'],
    prerequisites: ['marl-8.6'],
    learningObjectives: [
      'Explain how to train a neural network model of another agent\'s policy',
      'Describe approaches: supervised learning on observed actions, theory of mind networks',
      'Understand the recursive modelling problem: agents modelling agents modelling agents',
    ],
    keyConcepts: [
      'Neural agent model: a network predicting another agent\'s action given its observation',
      'Supervised opponent modelling: train on (observation, action) pairs from the opponent',
      'Theory of mind: modelling what the other agent believes about you',
      'Recursive reasoning: higher-order beliefs lead to infinite regress',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-8.7.mdx',
  },
  {
    id: 'marl-8.8',
    moduleId: 'marl-mod-8',
    title: 'Homogeneous Agents: Parameter and Experience Sharing',
    description: 'Exploit agent symmetry by sharing parameters and experience across identical agents.',
    order: 8,
    sourceSections: ['9.7'],
    prerequisites: ['marl-8.7'],
    learningObjectives: [
      'Define homogeneous agents and when parameter sharing is appropriate',
      'Explain parameter sharing: all agents share a single policy network',
      'Describe experience sharing: pooling transitions from all agents into a shared buffer',
    ],
    keyConcepts: [
      'Homogeneous agents: agents with identical observation and action spaces',
      'Parameter sharing: using a single network for all agents (with agent ID as additional input)',
      'Experience sharing: pooling transitions from all agents to improve sample efficiency',
      'Symmetry breaking: agent IDs or positional encodings prevent identical behaviour',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-8.8.mdx',
  },
  {
    id: 'marl-8.9',
    moduleId: 'marl-mod-8',
    title: 'Self-Play and AlphaZero',
    description: 'Study self-play in deep MARL: AlphaGo, AlphaGo Zero, and AlphaZero as milestones.',
    order: 9,
    sourceSections: ['9.8'],
    prerequisites: ['marl-8.8'],
    learningObjectives: [
      'Trace the evolution from AlphaGo to AlphaGo Zero to AlphaZero',
      'Explain how MCTS (Monte Carlo Tree Search) combines with deep RL in self-play',
      'Understand the tabula rasa learning paradigm: learning from scratch without human data',
    ],
    keyConcepts: [
      'AlphaGo: CNN + MCTS + supervised learning from human games + RL self-play',
      'AlphaGo Zero: no human data; pure self-play with MCTS and neural network',
      'AlphaZero: generalised to chess, shogi, and Go with a single algorithm',
      'MCTS: tree search using neural network for evaluation and prior probability of moves',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-8.9.mdx',
  },
  {
    id: 'marl-8.10',
    moduleId: 'marl-mod-8',
    title: 'Population-Based Training: PSRO and AlphaStar',
    description: 'Move beyond simple self-play to population-based training with PSRO and AlphaStar as case studies.',
    order: 10,
    sourceSections: ['9.9', '9.10'],
    prerequisites: ['marl-8.9'],
    learningObjectives: [
      'Explain the Double Oracle algorithm and Policy-Space Response Oracles (PSRO)',
      'Describe how AlphaStar used population-based training to master StarCraft II',
      'Understand the league training architecture and its role in avoiding cyclic strategies',
    ],
    keyConcepts: [
      'Double Oracle: iteratively compute best responses and add them to a growing strategy set',
      'PSRO: the RL version of Double Oracle; train policies, compute meta-game, add best responses',
      'AlphaStar: deep MARL system that defeated professional StarCraft II players',
      'League training: maintaining a population of agents with different training objectives',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-8.10.mdx',
  },

  // ---------------------------------------------------------------------------
  // Module 9: MARL in Practice and Environments (Chapters 10 + 11)
  // ---------------------------------------------------------------------------
  {
    id: 'marl-9.1',
    moduleId: 'marl-mod-9',
    title: 'The Agent-Environment Interface and Implementation',
    description: 'Implement the MARL agent-environment interface using standard APIs like PettingZoo and Gymnasium.',
    order: 1,
    sourceSections: ['10.1', '10.2'],
    prerequisites: ['marl-8.10'],
    learningObjectives: [
      'Describe the standard multi-agent environment interface (parallel and sequential APIs)',
      'Implement a simple MARL training loop using PettingZoo',
      'Understand observation and action space specifications for multiple agents',
    ],
    keyConcepts: [
      'PettingZoo: the standard Python library for multi-agent environments',
      'Parallel API: all agents act simultaneously each step',
      'AEC (Agent-Environment Cycle) API: agents act one at a time in sequence',
      'Observation/action spaces: defining per-agent observation and action spaces',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-9.1.mdx',
  },
  {
    id: 'marl-9.2',
    moduleId: 'marl-mod-9',
    title: 'Centralised Values, Decomposition, and Tips',
    description: 'Practical tips for implementing centralised critics, value decomposition, and debugging MARL training.',
    order: 2,
    sourceSections: ['10.3', '10.4', '10.5'],
    prerequisites: ['marl-9.1'],
    learningObjectives: [
      'Implement centralised value functions that condition on global state',
      'Set up QMIX-style value decomposition in practice',
      'Apply debugging and hyperparameter tuning tips specific to MARL',
    ],
    keyConcepts: [
      'Global state construction: concatenating observations, adding global features',
      'QMIX implementation: individual Q-networks, mixing network, hypernetwork for weights',
      'Hyperparameter tips: target update frequency, replay buffer size, batch size',
      'Debugging MARL: check single-agent performance first, then add agents incrementally',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-9.2.mdx',
  },
  {
    id: 'marl-9.3',
    moduleId: 'marl-mod-9',
    title: 'Presenting Results and Matrix Games',
    description: 'Learn how to present MARL experimental results clearly and use matrix games as diagnostic tools.',
    order: 3,
    sourceSections: ['10.6', '11.1', '11.2'],
    prerequisites: ['marl-9.2'],
    learningObjectives: [
      'Follow best practices for reporting MARL experimental results',
      'Use matrix games (Prisoner\'s Dilemma, Matching Pennies, etc.) to diagnose algorithm behaviour',
      'Understand the role of standard benchmarks in MARL research',
    ],
    keyConcepts: [
      'Reporting: learning curves with confidence intervals over multiple seeds',
      'Matrix games as diagnostics: simple games that test specific algorithm properties',
      'Prisoner\'s Dilemma: tests cooperation vs. defection dynamics',
      'Matching Pennies: a zero-sum game that tests convergence to mixed strategies',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-9.3.mdx',
  },
  {
    id: 'marl-9.4',
    moduleId: 'marl-mod-9',
    title: 'Multi-Agent Environments Survey',
    description: 'Survey the landscape of MARL environments: from grid worlds to StarCraft to real-world robotics.',
    order: 4,
    sourceSections: ['11.3', '11.4'],
    prerequisites: ['marl-9.3'],
    learningObjectives: [
      'Categorise MARL environments by properties: cooperative, competitive, mixed, partially observable',
      'Know the major benchmark environments: SMAC, MPE, Level-Based Foraging, Hanabi, etc.',
      'Understand how environment choice affects algorithm evaluation',
    ],
    keyConcepts: [
      'SMAC (StarCraft Multi-Agent Challenge): cooperative micromanagement benchmark',
      'MPE (Multi-Agent Particle Environment): simple 2D physics with communication',
      'Level-Based Foraging (LBF): cooperative foraging requiring agent coordination',
      'Hanabi: a cooperative card game with partial observability; tests theory of mind',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/marl-9.4.mdx',
  },
];

/**
 * Lesson 7.9: Observations, States, and Histories
 *
 * Covers: State vs observation vs history, handling partial observability, centralised vs decentralised
 * Source sections: 8.3, 8.4
 */

import type { LessonContentData } from '../../deep-learning-python/lessons/lesson-1-1';

const lesson: LessonContentData = {
  lessonId: 'marl-7.9',
  title: 'Observations, States, and Histories',
  sections: [
    {
      id: 'marl-7.9.1',
      title: 'State vs Observation vs History',
      content: `
All the deep RL algorithms we have seen -- DQN, REINFORCE, A2C, PPO -- were presented as if the agent observes the full **state** of the environment. In many real-world settings, however, the agent only receives a partial, noisy view called an **observation**. Understanding the distinction between states, observations, and histories is critical for correctly applying deep RL.

The **state** s_t is the complete description of the environment at time step t. It contains all information needed to determine future dynamics: given the state and an action, the transition function fully specifies the distribution over next states. In a fully observable environment, the agent directly sees s_t and can condition its policy on it.

An **observation** o_t is what the agent actually perceives. In a partially observable environment, o_t may omit important information. A robot with a forward-facing camera, for example, cannot see what is behind it. A sensor might measure position but not velocity. The observation is generated by an observation function O(o | s), which maps states to (possibly stochastic) observations.

The key problem with partial observability is that the current observation alone may be insufficient for optimal decision-making. Two different states might produce the same observation, but require different actions. To recover the necessary information, the agent needs to consider its **history** of observations: h_t = (o_0, o_1, ..., o_t). The full history is a sufficient statistic for the underlying state -- in principle, the agent can extract all available information about the true state from the sequence of everything it has observed.

When we write pi(a | s; phi) in a fully observable setting, the input to the policy network is the state s. In a partially observable setting, the input should instead be the history h_t: pi(a | h_t; phi). The same applies to value functions: V(h_t; theta) rather than V(s; theta). This seemingly small change has major architectural implications, because the history is a variable-length sequence that grows with every time step.
`,
      reviewCardIds: ['rc-marl-7.9-1', 'rc-marl-7.9-2'],
      illustrations: [],
    },
    {
      id: 'marl-7.9.2',
      title: 'Handling Partial Observability with Recurrent Networks',
      content: `
How do we feed a growing history of observations into a neural network that expects fixed-size input? One approach is to concatenate all observations into a single long vector, but this is impractical: the vector grows with episode length, most entries are zero-padded, and feedforward networks struggle with such sparse, high-dimensional inputs.

The standard solution is **recurrent neural networks**, particularly **LSTMs** and **GRUs**, which we introduced in the deep learning chapter. The key idea is elegant: instead of passing the entire history to the network at once, we process observations one at a time, and the recurrent hidden state h_t serves as a compact, learned summary of the history.

At each time step t, the agent receives observation o_t, encodes it (perhaps through a CNN if it is an image, or an MLP if it is a vector), and feeds the encoded observation along with the previous hidden state h_{t-1} into the LSTM or GRU:

h_t = RNN(encode(o_t), h_{t-1}; theta_rnn)

The updated hidden state h_t is then passed to the policy and value heads:

pi(a | h_t; phi) and V(h_t; theta_v)

At the start of each episode, the hidden state is initialised to zeros. As the episode progresses, the hidden state accumulates information, effectively learning what to remember and what to forget about past observations.

This architecture slots directly into the algorithms we already know. DQN, A2C, and PPO all work with recurrent networks -- the only difference is that the policy and value networks now include an LSTM layer, and the hidden state must be carried forward between time steps and reset at episode boundaries. In practice, GRUs are slightly simpler (fewer parameters) while LSTMs are more expressive; both are widely used in deep RL for partially observable environments.

A practical consideration: when training with mini-batches (e.g. from a replay buffer in DQN), you must store and replay sequences of transitions rather than individual transitions, so that the hidden state can be properly reconstructed.
`,
      reviewCardIds: ['rc-marl-7.9-3', 'rc-marl-7.9-4'],
      illustrations: [],
      codeExamples: [
        {
          title: 'Recurrent policy network for partial observability',
          language: 'python',
          code: `import torch
import torch.nn as nn

class RecurrentPolicy(nn.Module):
    def __init__(self, obs_dim, action_dim, hidden_dim=64):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(obs_dim, hidden_dim), nn.ReLU(),
        )
        self.lstm = nn.LSTMCell(hidden_dim, hidden_dim)
        self.policy_head = nn.Linear(hidden_dim, action_dim)
        self.value_head = nn.Linear(hidden_dim, 1)

    def forward(self, obs, hidden_state):
        """Process one time step.
        Args:
            obs: (batch, obs_dim)
            hidden_state: tuple of (h, c), each (batch, hidden_dim)
        Returns:
            action_dist, value, new_hidden_state
        """
        x = self.encoder(obs)
        h, c = self.lstm(x, hidden_state)
        logits = self.policy_head(h)
        value = self.value_head(h)
        return torch.distributions.Categorical(logits=logits), value, (h, c)

    def init_hidden(self, batch_size=1):
        return (torch.zeros(batch_size, 64), torch.zeros(batch_size, 64))`,
        },
      ],
    },
    {
      id: 'marl-7.9.3',
      title: 'Centralised vs Decentralised: A Preview for MARL',
      content: `
The distinction between states and observations takes on even greater importance in **multi-agent** settings. In MARL, each agent i typically receives its own local observation o_t^i, which may differ substantially from what other agents see. Agent 1's camera might cover the left side of a warehouse; Agent 2's might cover the right. Neither agent sees the full state, and neither sees what the other observes.

This creates a fundamental design question: should agents' policies and value functions be conditioned on local information only, or should they have access to global information during some phase of training?

In the **fully decentralised** setting, each agent's policy depends only on its own observation history: pi_i(a | h_t^i; phi_i). This is the only option at execution time in many real-world deployments, where communication is limited or impossible. The policy must be self-contained.

However, during **training** we often have access to the full state or the joint observations of all agents -- for example, in a simulator. This leads to the influential **centralised training with decentralised execution (CTDE)** paradigm that we will study in depth in the MARL chapters. Under CTDE, the value function (critic) can be conditioned on the full state or all agents' observations during training, providing a richer learning signal. The policy (actor), however, is conditioned only on local observations, so it can execute in a fully decentralised manner at deployment.

For value-based methods like DQN variants, CTDE might mean training a centralised Q-function Q(s, a_1, ..., a_n; theta) that takes the global state and all agents' actions, while each agent still selects its action based on its local observation. For actor-critic methods, the critic V(s; theta) or Q(s, a_1, ..., a_n; theta) uses global information, while each actor pi_i(a | o_t^i; phi_i) uses only local observations.

This chapter has established all the single-agent deep RL building blocks -- DQN, policy gradients, actor-critic methods, and handling partial observability. Chapter 9 of the textbook extends these ideas to the multi-agent setting, where the interplay between agents introduces new challenges and opportunities.
`,
      reviewCardIds: ['rc-marl-7.9-5'],
      illustrations: [],
    },
  ],
  summary: `**Key takeaways:**
- The state is the complete environment description; an observation is the agent's partial view. In partially observable environments, the agent must condition on its history of observations h_t = (o_0, ..., o_t).
- Recurrent networks (LSTMs, GRUs) handle partial observability by maintaining a hidden state that summarises the observation history, processing one observation per time step.
- The hidden state is initialised to zeros at each episode start and carried forward between steps. When using replay buffers, sequences of transitions must be stored and replayed.
- In MARL, each agent has its own local observations. The centralised training with decentralised execution (CTDE) paradigm allows critics to use global state during training while actors use only local observations.
- The deep RL building blocks from this module (DQN, policy gradients, actor-critic, recurrent architectures) form the foundation for the deep MARL algorithms in the next chapter.`,
};

export default lesson;

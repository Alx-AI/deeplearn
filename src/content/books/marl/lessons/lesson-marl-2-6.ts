import type { LessonContentData } from '../../deep-learning-python/lessons/lesson-1-1';

const lesson: LessonContentData = {
  lessonId: 'marl-2.6',
  title: 'Temporal-Difference Learning: Sarsa and Q-Learning',
  sections: [
    {
      id: 'marl-2.6.1',
      title: 'The Temporal-Difference Idea',
      content: `
Dynamic programming is powerful but demands full knowledge of the MDP's transition and reward functions. In most real-world problems, these functions are unknown. **Temporal-difference (TD) learning** solves this problem by learning value functions directly from experience -- from the stream of $(s_t, a_t, r_t, s_{t+1})$ tuples generated by the agent's interaction with the environment.

TD methods share the **bootstrapping** idea with DP: they update the value of a state or action using the estimated value of successor states. But instead of summing over all possible successors weighted by their transition probabilities (which requires knowing $T$), TD methods use a single observed successor state from actual experience.

The general TD update rule for action-value functions is:

$$Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\cdot (X - Q(s_t, a_t))$$

Here, $\\alpha \\in (0, 1]$ is the **learning rate** (or step size), and $X$ is the **update target** constructed from an experience sample $(s_t, a_t, r_t, s_{t+1})$. The quantity $(X - Q(s_t, a_t))$ is called the **TD error** -- it measures the discrepancy between the current estimate and the target. The update nudges $Q(s_t, a_t)$ toward the target by a fraction $\\alpha$ of the TD error.

The key insight is that different choices of update target $X$ lead to different algorithms with different properties. Two foundational choices give us **Sarsa** and **Q-learning**, which we examine in the following sections.

TD methods have convergence guarantees under two conditions. First, every state-action pair must be visited infinitely often during learning. Second, the learning rate must satisfy the **stochastic approximation conditions**: $\\sum_k \\alpha_k$ must diverge (ensuring large enough updates to overcome initial conditions) while $\\sum_k \\alpha_k^2$ must converge (ensuring the updates eventually shrink enough to converge). The schedule $\\alpha_k(s, a) = 1/k$ satisfies these conditions, though constant learning rates are common in practice because theoretically sound schedules can lead to slow learning in complex MDPs.
`,
      reviewCardIds: ['rc-marl-2.6-1', 'rc-marl-2.6-2'],
      illustrations: [],
    },
    {
      id: 'marl-2.6.2',
      title: 'Sarsa: On-Policy TD Learning',
      content: `
**Sarsa** is named after the five elements of each update: $(S_t, A_t, R_t, S_{t+1}, A_{t+1})$. It constructs its update target from the Bellman expectation equation for $Q^{\\pi}$ by replacing the model-based expectations with a single experience sample:

$$X = r_t + \\gamma \\cdot Q(s_{t+1}, a_{t+1})$$

where $a_{t+1}$ is the action actually selected by the policy $\\pi$ in the next state $s_{t+1}$. The complete Sarsa update rule is:

$$Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\cdot (r_t + \\gamma \\cdot Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t))$$

Sarsa is an **on-policy** algorithm: the update target uses the action $a_{t+1}$ that the agent actually takes in $s_{t+1}$ according to the current policy. This means the Q-function being learned reflects the behavior of the policy currently being followed, including its exploration behavior.

To learn the optimal policy, Sarsa must gradually improve the policy during training. The standard approach is to use an **epsilon-greedy** policy, which chooses the greedy action ($\\arg\\max_a Q(s, a)$) with probability $1 - \\epsilon$ and a random action with probability $\\epsilon$. By slowly decaying $\\epsilon$ toward 0 during training, the policy gradually shifts from exploration to exploitation, converging toward the optimal deterministic policy.

The epsilon-greedy policy is defined as:

$$\\pi(a \\mid s) = \\begin{cases} 1 - \\epsilon + \\frac{\\epsilon}{|\\mathcal{A}|} & \\text{if } a \\text{ is the greedy action} \\\\ \\frac{\\epsilon}{|\\mathcal{A}|} & \\text{otherwise} \\end{cases}$$

Thus, even the greedy action gets a small probability boost from the exploration term. As long as $\\epsilon > 0$, every action has a nonzero probability of being selected in every state, ensuring the first convergence condition (infinite visits to all state-action pairs) is met.

Because Sarsa is on-policy, the Q-values it learns reflect the epsilon-greedy behavior policy. This means Sarsa tends to be more conservative: if exploration occasionally leads to catastrophic states, Sarsa will learn lower values for actions near those states, effectively learning to avoid them even when the greedy policy would not visit them. This can be a feature or a limitation, depending on the problem.
`,
      reviewCardIds: ['rc-marl-2.6-3', 'rc-marl-2.6-4'],
      illustrations: [],
    },
    {
      id: 'marl-2.6.3',
      title: 'Q-Learning: Off-Policy TD and Epsilon-Greedy Exploration',
      content: `
**Q-learning** (Watkins and Dayan, 1992) takes a different approach by basing its update target on the Bellman *optimality* equation rather than the expectation equation. The update target uses the maximum Q-value in the next state, regardless of which action the agent actually takes:

$$X = r_t + \\gamma \\cdot \\max_{a' \\in \\mathcal{A}} Q(s_{t+1}, a')$$

The complete Q-learning update rule is:

$$Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\cdot (r_t + \\gamma \\cdot \\max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t))$$

The crucial difference from Sarsa is the max operator. Q-learning's target always reflects the best possible action in the next state, not the action the agent actually took. This makes Q-learning an **off-policy** algorithm: the Q-function being learned approximates $Q^*$ -- the optimal action-value function -- regardless of what exploration policy is used to collect data.

This off-policy property has important practical consequences. Q-learning can use any exploration policy to gather experience (as long as it visits all state-action pairs sufficiently often) and will still converge to the optimal value function. The agent might explore with an epsilon-greedy policy, a completely random policy, or even data collected by a different agent -- Q-learning does not care. The update target always bootstraps from the best possible action, so the learned Q-values converge to $Q^*$.

In the Mars Rover example, both Sarsa and Q-learning converge to the same optimal policy (choosing "left" in Start and "right" in Site A and Site B), and their learning curves are nearly identical. However, the choice of the learning rate $\\alpha$ and exploration rate $\\epsilon$ can significantly affect learning speed. For Q-learning on the Mars Rover problem, a decaying learning rate $\\alpha_k = 1/k$ performs best, but for larger, more complex MDPs, a well-chosen constant learning rate often leads to faster learning.

The on-policy vs. off-policy distinction becomes increasingly important in the multi-agent setting. Most MARL algorithms introduced in Chapter 6 extend either Sarsa or Q-learning to multi-agent games, and the choice between on-policy and off-policy learning carries over, affecting how agents can learn from each other's experience and how stable the joint learning process is.
`,
      reviewCardIds: ['rc-marl-2.6-5'],
      illustrations: [],
    },
  ],
  summary: `**Key takeaways:**
- TD methods learn value functions from experience samples, using bootstrapping without requiring a model of the environment.
- Sarsa is on-policy: its update target uses the action actually taken, so the learned $Q$ reflects the current (exploration) policy.
- Q-learning is off-policy: its update target uses max over next-state actions, converging to $Q^*$ regardless of the exploration policy.
- Epsilon-greedy policies balance exploration and exploitation by choosing random actions with probability $\\epsilon$ and greedy actions otherwise.
- Convergence requires visiting all state-action pairs infinitely often and satisfying stochastic approximation conditions on the learning rate.`,
};

export default lesson;

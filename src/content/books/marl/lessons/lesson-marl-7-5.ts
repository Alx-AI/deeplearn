/**
 * Lesson 7.5: Deep Q-Networks
 *
 * Covers: DQN architecture, experience replay, target networks
 * Source sections: 8.1
 */

import type { LessonContentData } from '../../deep-learning-python/lessons/lesson-1-1';

const lesson: LessonContentData = {
  lessonId: 'marl-7.5',
  title: 'Deep Q-Networks',
  sections: [
    {
      id: 'marl-7.5.1',
      title: 'DQN Architecture and the Deep Q-Learning Loss',
      content: `
**Deep Q-Networks (DQN)** (Mnih et al., 2015) is one of the most influential deep RL algorithms, and it serves as a foundation for many single-agent and multi-agent methods. The core idea is simple: replace the tabular Q-table with a neural network Q(s, a; theta) that takes a state as input and outputs an action-value estimate for every discrete action simultaneously.

This architecture is computationally efficient -- a single forward pass through the network produces Q-values for all actions, and the agent simply selects the action with the highest value (or explores via epsilon-greedy). The loss function mirrors the tabular Q-learning update:

L(theta) = (y_t - Q(s_t, a_t; theta))^2

where the target y_t is a bootstrapped estimate:

y_t = r_t (if s_{t+1} is terminal)
y_t = r_t + gamma * max_{a'} Q(s_{t+1}, a'; theta) (otherwise)

An important implementation detail: the target y_t contains the value network Q itself (through the max over next-state values). When computing gradients via backpropagation, we must **stop the gradient** through the target to avoid updating the network toward a constantly shifting goal. In PyTorch, this is done with \`torch.no_grad()\` or \`.detach()\`.

Naive deep Q-learning -- just plugging a neural network into tabular Q-learning -- suffers from two critical problems that we previewed in the deadly triad discussion: the **moving target problem** (bootstrapped targets shift with every parameter update) and **correlated samples** (consecutive transitions are highly dependent). DQN addresses both with two stabilisation mechanisms: target networks and experience replay.
`,
      reviewCardIds: ['rc-marl-7.5-1', 'rc-marl-7.5-2'],
      illustrations: [],
      codeExamples: [
        {
          title: 'DQN network architecture in PyTorch',
          language: 'python',
          code: `import torch
import torch.nn as nn

class DQNetwork(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=128):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim),  # one output per action
        )

    def forward(self, state):
        return self.net(state)  # returns Q-values for all actions`,
        },
      ],
    },
    {
      id: 'marl-7.5.2',
      title: 'Experience Replay: Breaking Temporal Correlations',
      content: `
Standard machine learning assumes training data is **i.i.d.** -- independent and identically distributed. RL experience violates this assumption badly. Consecutive transitions (s_t, a_t, r_t, s_{t+1}) and (s_{t+1}, a_{t+1}, r_{t+1}, s_{t+2}) are obviously correlated: the next state of one is the current state of the other. Training on such correlated sequences causes the network to overfit to its most recent experience and forget earlier lessons -- a phenomenon called **catastrophic forgetting**.

Consider an agent controlling a spaceship. If it approaches the landing zone from the right for several episodes, the value network specialises to right-side approaches. When it later needs to approach from the left, the network gives poor estimates and may fail entirely. Worse, re-training on left-side experience may erase what it learned about the right side.

**Experience replay** solves this with a simple but powerful idea. Instead of training on each transition immediately, the agent stores transitions in a **replay buffer** D = {(s, a, r, s')}. At each training step, a mini-batch B of transitions is sampled **uniformly at random** from the buffer. This randomisation breaks temporal correlations and provides a diverse, approximately i.i.d. training set. The loss is then averaged over the batch:

L(theta) = (1/B) * sum_{k=1}^{B} (y_k - Q(s_k, a_k; theta))^2

The replay buffer is typically implemented as a fixed-capacity FIFO queue: once full, the oldest transitions are discarded as new ones arrive. Replay also improves **sample efficiency** because each transition can be reused in multiple training updates rather than being discarded after one use.

An important constraint: because the buffer contains transitions generated by past policies, replay data is inherently **off-policy**. This means experience replay can only be used with off-policy algorithms like Q-learning; it is incompatible with on-policy methods like REINFORCE or A2C, which require data generated by the current policy.
`,
      reviewCardIds: ['rc-marl-7.5-3', 'rc-marl-7.5-4'],
      illustrations: [],
      codeExamples: [
        {
          title: 'Simple replay buffer implementation',
          language: 'python',
          code: `import random
from collections import deque

class ReplayBuffer:
    def __init__(self, capacity=10_000):
        self.buffer = deque(maxlen=capacity)

    def push(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))

    def sample(self, batch_size):
        batch = random.sample(self.buffer, batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)
        return states, actions, rewards, next_states, dones

    def __len__(self):
        return len(self.buffer)`,
        },
      ],
    },
    {
      id: 'marl-7.5.3',
      title: 'Target Networks: Stabilising the Moving Target',
      content: `
The second stabilisation mechanism in DQN is the **target network**. Recall that the Q-learning target is y_t = r_t + gamma * max_{a'} Q(s_{t+1}, a'; theta). Every time we update theta, the target values change -- even for transitions we have not trained on -- because the network generalises across states. This creates a moving-target problem far worse than in tabular RL, where updating one state leaves all others untouched.

The fix is to maintain a separate copy of the Q-network called the **target network** with parameters theta-bar. This network has the same architecture but its parameters are updated much less frequently. The target becomes:

y_t = r_t + gamma * max_{a'} Q(s_{t+1}, a'; theta-bar)

The main network theta is updated at every training step via gradient descent. The target network theta-bar is updated only periodically -- typically every C steps, the main network's parameters are copied wholesale: theta-bar <- theta. Between updates, the target network is frozen, providing a stable target for the loss computation.

This decoupling means the targets no longer shift with every gradient step, dramatically reducing oscillation and divergence risks. Experiments in the textbook's level-based foraging environment show that naive deep Q-learning barely learns, adding only a target network or only a replay buffer gives modest improvement, but the **combination** of both -- the full DQN algorithm -- converges stably to near-optimal returns.

An important extension is **Double DQN (DDQN)** (van Hasselt et al., 2016), which addresses the overestimation bias inherent in the max operator. DDQN uses the main network to select the greedy action and the target network to evaluate it: y_t = r_t + gamma * Q(s_{t+1}, argmax_{a'} Q(s_{t+1}, a'; theta); theta-bar). This decoupling of action selection and evaluation significantly reduces overestimation and is standard in modern DQN-based methods.
`,
      reviewCardIds: ['rc-marl-7.5-5'],
      illustrations: [],
      codeExamples: [
        {
          title: 'DQN training step with target network',
          language: 'python',
          code: `def dqn_update(q_net, target_net, replay_buffer, optimizer,
               batch_size=512, gamma=0.99):
    states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)

    states = torch.tensor(states, dtype=torch.float32)
    actions = torch.tensor(actions, dtype=torch.long)
    rewards = torch.tensor(rewards, dtype=torch.float32)
    next_states = torch.tensor(next_states, dtype=torch.float32)
    dones = torch.tensor(dones, dtype=torch.float32)

    # Current Q-values for chosen actions
    q_values = q_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)

    # Target values using the frozen target network
    with torch.no_grad():
        max_next_q = target_net(next_states).max(dim=1).values
        targets = rewards + gamma * max_next_q * (1 - dones)

    loss = nn.functional.mse_loss(q_values, targets)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()`,
        },
      ],
    },
  ],
  summary: `**Key takeaways:**
- DQN uses a neural network to approximate the action-value function, outputting Q-values for all discrete actions in a single forward pass.
- Experience replay stores transitions in a buffer and samples random mini-batches, breaking temporal correlations and enabling sample reuse. It requires off-policy learning.
- Target networks provide stable bootstrapped targets by using a frozen copy of the Q-network, updated only periodically.
- Neither replay nor target networks alone suffice; the combination of both is what makes DQN training stable and convergent.
- Double DQN decouples action selection from evaluation to reduce overestimation bias and is standard in practice.`,
};

export default lesson;

/**
 * Lesson 8.2: Independent Deep Learning
 *
 * Covers: IDQN and IPPO, replay buffer staleness, surprising effectiveness
 * Source sections: 9.3
 */

import type { LessonContentData } from '../../deep-learning-python/lessons/lesson-1-1';

const lesson: LessonContentData = {
  lessonId: 'marl-8.2',
  title: 'Independent Deep Learning',
  sections: [
    {
      id: 'marl-8.2.1',
      title: 'IDQN and Independent Policy Gradient Methods',
      content: `
The simplest way to do deep MARL is to pretend other agents do not exist. Each agent applies a standard single-agent deep RL algorithm -- treating every other agent as part of the environment dynamics. This is **independent learning**, and it is surprisingly effective in practice.

**Independent Deep Q-Networks (IDQN)** gives each agent $i$ its own action-value function $Q(h_i^t, a_i \\;; \\theta_i)$, its own replay buffer $D_i$, and trains using standard DQN. The loss for agent $i$ is:

$$L(\\theta_i) = \\frac{1}{B} \\sum \\left( r_i^t + \\gamma \\max_{a_i} Q(h_i^{t+1}, a_i \\;; \\bar{\\theta}_i) - Q(h_i^t, a_i^t \\;; \\theta_i) \\right)^2$$

where $\\bar{\\theta}_i$ are the target network parameters. All agents' value functions are optimized simultaneously by minimizing the aggregate loss across agents. Crucially, each agent's loss depends only on its own observations, actions, and rewards -- it has no knowledge of what other agents are doing.

Similarly, **independent REINFORCE** and **independent A2C (IA2C)** apply policy gradient methods independently. Each agent maintains its own policy $\\pi(\\cdot \\mid h_i^t \\;; \\phi_i)$, computes its own policy gradient from its own experiences, and updates its own parameters. In independent REINFORCE, the gradient is:

$$\\nabla_{\\phi_i} J(\\phi_i) = \\mathbb{E}_{\\pi}\\left[ u_i^t \\cdot \\nabla_{\\phi_i} \\log \\pi(a_i^t \\mid h_i^t \\;; \\phi_i) \\right]$$

where $u_i^t$ is agent $i$'s return. IA2C extends this with a learned baseline (critic) $V(h_i^t \\;; \\theta_i)$ to reduce variance. **Independent PPO (IPPO)** works analogously -- each agent applies PPO using only its own trajectory data.

For IA2C with **parallel environments** ($K$ environments running simultaneously), the policy loss for agent $i$ averages over environments: $L(\\phi) = \\frac{1}{K} \\sum_i \\sum_k L(\\phi_i \\mid k)$, where each term uses the advantage $\\text{Adv}(h_i^{t,k}, a_i^{t,k})$ computed from agent $i$'s critic.
`,
      reviewCardIds: ['rc-marl-8.2-1', 'rc-marl-8.2-2'],
      illustrations: [],
    },
    {
      id: 'marl-8.2.2',
      title: 'Replay Buffer Staleness in Multi-Agent Settings',
      content: `
Off-policy algorithms like IDQN store experience tuples in a replay buffer and learn from sampled mini-batches. In single-agent RL, the replay buffer is a powerful tool for sample efficiency and decorrelating updates. But in multi-agent settings, it introduces a subtle and important problem: **experience staleness**.

In a multi-agent environment, an agent's returns depend not just on its own actions but also on the policies of all other agents. Consider two agents learning chess. Agent 1 uses an opening that works well initially because agent 2 has not yet learned to counter it. These successful experiences get stored in agent 1's replay buffer. As agent 2 improves and learns to counter the opening, the old experiences remain in the buffer, telling agent 1 that the opening is effective -- even though it no longer is. Agent 1 keeps reinforcing a strategy that has already been countered.

This happens because the replay buffer implicitly assumes that stored experiences remain relevant over time. In single-agent RL, the environment dynamics are stationary. In MARL, they are not: other agents are constantly changing their policies, making the effective transition and reward functions non-stationary from each agent's perspective.

Several approaches address this issue:

- **Smaller replay buffers** reach capacity faster, causing older (and more stale) experiences to be overwritten sooner. This is the simplest fix but sacrifices some sample efficiency.

- **Importance sampling corrections** (Foerster et al. 2017) store the action probabilities of all agents alongside each experience. When replaying, agents re-weight experiences based on how much the other agents' policies have changed, correcting for distribution shift.

- **Hysteretic Q-learning** uses a smaller learning rate for updates that would decrease action-value estimates, based on the reasoning that decreases might be caused by other agents' exploration rather than genuine environment dynamics.

- **Lenient learning** probabilistically ignores negative updates early in training, when other agents' policies are most stochastic.

On-policy algorithms like REINFORCE and A2C avoid this problem entirely -- they always learn from the most current data generated by all agents' latest policies. This is an important practical advantage of on-policy methods in multi-agent settings.
`,
      reviewCardIds: ['rc-marl-8.2-3', 'rc-marl-8.2-4'],
      illustrations: [],
    },
    {
      id: 'marl-8.2.3',
      title: 'The Surprising Effectiveness of Independent Learning',
      content: `
Given its simplicity and theoretical limitations -- no explicit modeling of other agents, non-stationary environment dynamics, stale replay buffers -- you might expect independent learning to perform poorly. Yet empirically, it often works remarkably well.

The book demonstrates this with **level-based foraging** experiments. In a $15 \\times 15$ grid with random initial positions and levels, the state space explodes to approximately 5 billion combinations for two agents and two items. Tabular methods cannot handle this scale. But IA2C, using neural networks with just two hidden layers of 64 units each, learned policies that collect all available items (evaluation returns near 1.0) within 40 million environment time steps -- about three hours on a standard CPU.

Scaling up to three agents and three items yields roughly $3.6 \\times 10^{14}$ possible states. IA2C still learned to collect about half the items on average, demonstrating that deep function approximation allows independent learning to generalize across enormous state spaces that would be completely intractable for tabular methods.

Several factors explain independent learning's surprising effectiveness:

1. **Neural network generalization.** Unlike tabular methods that require visiting each state many times, neural networks transfer knowledge between similar states. An agent that learns to approach food from the left can generalize to approaching from the right.

2. **On-policy methods help.** IA2C and IPPO always use the most recent data, partially mitigating non-stationarity. As other agents' policies change, the collected trajectories immediately reflect those changes.

3. **Implicit coordination through rewards.** Even without explicitly modeling other agents, the reward signal indirectly encodes the consequences of multi-agent interaction. If two agents stumble upon a cooperative strategy that yields high returns, gradient updates will reinforce it.

4. **Competitive baselines.** Studies (Papoudakis et al. 2021) have shown that independent learning often matches or closely trails more sophisticated CTDE algorithms, especially in environments where coordination requirements are moderate.

Independent learning serves as both a practical starting point and a strong baseline against which more complex algorithms must justify their added complexity.
`,
      reviewCardIds: ['rc-marl-8.2-5'],
      illustrations: [],
    },
  ],
  summary: `**Key takeaways:**
- Independent learning applies single-agent deep RL to each agent separately: IDQN uses per-agent DQN, IA2C uses per-agent advantage actor-critic, and IPPO uses per-agent PPO.
- Replay buffers in multi-agent settings suffer from experience staleness: stored transitions become outdated as other agents change their policies.
- Mitigations for staleness include smaller buffers, importance sampling corrections, hysteretic Q-learning, and lenient learning. On-policy methods avoid the problem entirely.
- Despite its simplicity, independent learning performs surprisingly well in practice due to neural network generalization, on-policy data freshness, and implicit coordination through rewards.
- IA2C can handle environments with billions of states where tabular methods completely fail.`,
};

export default lesson;

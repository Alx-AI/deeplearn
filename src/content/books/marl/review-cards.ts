import type { ReviewCard } from '@/lib/db/schema';

export const reviewCards: ReviewCard[] = [
  // ============================================================
  // Module 1: Introduction to Multi-Agent Reinforcement Learning
  // ============================================================

  // --- Lesson marl-1.1: Multi-Agent Systems ---
  { id: 'rc-marl-1.1-1', lessonId: 'marl-1.1', prompt: 'What are the three basic components of a multi-agent system?', answer: 'A multi-agent system consists of an **environment**, multiple decision-making **agents**, and their **goals**. The agents interact within the environment to achieve their specified goals.', type: 'recall', bloomLevel: 'remember', tags: ['multi-agent-systems', 'components', 'environment'], order: 1 },
  { id: 'rc-marl-1.1-2', lessonId: 'marl-1.1', prompt: 'In MARL, what does the term "policy" refer to?', answer: 'A **policy** is a function used by the agent to select actions (or assign probabilities to selecting each action) given the current state of the environment. If the environment is only partially observed, the policy may be conditioned on current and past **observations**.', type: 'recall', bloomLevel: 'remember', tags: ['policy', 'agent', 'decision-making'], order: 2 },
  { id: 'rc-marl-1.1-3', lessonId: 'marl-1.1', prompt: 'Why do multi-agent environments often involve agents with limited and imperfect views, and how does this affect their behavior?', answer: 'Multi-agent environments are often characterized by **partial observability**, meaning individual agents may only observe some partial information about the state and different agents may receive different observations. This forces agents to make decisions under **uncertainty** about the full environment state.', type: 'concept', bloomLevel: 'understand', tags: ['partial-observability', 'observations', 'uncertainty'], order: 3 },
  { id: 'rc-marl-1.1-4', lessonId: 'marl-1.1', prompt: 'In the level-based foraging environment, three robots have skill levels and items have collection thresholds. If Robot A (level 2) and Robot B (level 1) are adjacent to an item with level 3, can they collect it? What if only Robot A is adjacent?', answer: 'Yes, Robots A and B can collect the item because their combined levels (2+1=3) meet the item threshold of 3. Robot A alone cannot collect it because its level (2) is less than the item level (3). This demonstrates the need for **multi-agent coordination** in cooperative tasks.', type: 'application', bloomLevel: 'apply', tags: ['level-based-foraging', 'coordination', 'cooperation'], order: 4 },
  { id: 'rc-marl-1.1-5', lessonId: 'marl-1.1', prompt: 'The _____ characteristic of a multi-agent system is that agents must coordinate their actions with (or against) each other to achieve their goals.', answer: '**defining**', type: 'cloze', bloomLevel: 'remember', tags: ['multi-agent-systems', 'coordination', 'definition'], order: 5 },

  // --- Lesson marl-1.2: Multi-Agent Reinforcement Learning ---
  { id: 'rc-marl-1.2-1', lessonId: 'marl-1.2', prompt: 'What is a "joint action" in the MARL training loop?', answer: 'A **joint action** is the combination of individual actions chosen by all n agents at a given time step. If each agent i chooses action a_i, the joint action is the tuple **(a_1, a_2, ..., a_n)** which together changes the state of the environment.', type: 'recall', bloomLevel: 'remember', tags: ['joint-action', 'MARL-loop', 'agents'], order: 1 },
  { id: 'rc-marl-1.2-2', lessonId: 'marl-1.2', prompt: 'What is an "episode" in the context of MARL?', answer: 'An **episode** is a complete run of the MARL training loop from the **initial state** to a **terminal state** (or until a maximum number of time steps). The data from multiple independent episodes is used to continually improve the agents\' policies.', type: 'recall', bloomLevel: 'remember', tags: ['episode', 'training-loop', 'terminal-state'], order: 2 },
  { id: 'rc-marl-1.2-3', lessonId: 'marl-1.2', prompt: 'Explain how MARL can decompose a large decision problem into smaller ones, using the level-based foraging example.', answer: 'Instead of one central agent selecting from all possible action combinations (e.g., 6^3 = 216 actions for 3 robots), MARL introduces **independent agents** each facing only 6 actions. This **decomposition** makes the problem tractable, though agents must then learn to **coordinate** their individual policies.', type: 'concept', bloomLevel: 'understand', tags: ['decomposition', 'scalability', 'coordination'], order: 3 },
  { id: 'rc-marl-1.2-4', lessonId: 'marl-1.2', prompt: 'A fleet of search-and-rescue robots cannot communicate with a central coordinator during missions. Which MARL training/execution paradigm is most appropriate, and why?', answer: 'The **centralized training with decentralized execution** (CTDE) paradigm is most appropriate. During training (in simulation), agents can share information centrally to improve coordination. After training, each robot executes its own **decentralized policy** locally based on its own observations, without needing a central coordinator.', type: 'application', bloomLevel: 'apply', tags: ['CTDE', 'decentralized-execution', 'real-world-deployment'], order: 4 },
  { id: 'rc-marl-1.2-5', lessonId: 'marl-1.2', prompt: 'The _____ is the sum of rewards received by an agent over time, which agents learn to maximize through trial-and-error.', answer: '**return** (or cumulative reward)', type: 'cloze', bloomLevel: 'remember', tags: ['return', 'cumulative-reward', 'learning-objective'], order: 5 },

  // --- Lesson marl-1.3: Application Examples ---
  { id: 'rc-marl-1.3-1', lessonId: 'marl-1.3', prompt: 'In the multi-robot warehouse management application, what is "common reward" (or "shared reward")?', answer: '**Common reward** (or shared reward) is when all agents receive identical rewards. For example, all warehouse robots may receive a collective positive reward whenever **any** order has been completed by any robot, rather than only the robot that completed it.', type: 'recall', bloomLevel: 'remember', tags: ['common-reward', 'warehouse', 'cooperation'], order: 1 },
  { id: 'rc-marl-1.3-2', lessonId: 'marl-1.3', prompt: 'What is "zero-sum reward" in competitive MARL games?', answer: 'In a **zero-sum** reward setting, one agent\'s reward is the negative of the other agent\'s reward. For example, in chess, the winning player gets **+1** and the losing player gets **-1**, so the rewards always sum to zero.', type: 'recall', bloomLevel: 'remember', tags: ['zero-sum', 'competitive', 'reward-structure'], order: 2 },
  { id: 'rc-marl-1.3-3', lessonId: 'marl-1.3', prompt: 'How does autonomous driving represent a "mixed-motive" scenario that differs from purely cooperative or purely competitive settings?', answer: 'In autonomous driving, agents must **collaborate** to avoid collisions (shared safety goal) but are also **self-interested** in minimizing their own driving times and driving smoothly. This mix of cooperative and competitive incentives is called a **general-sum reward** scenario, which is among the most challenging in MARL.', type: 'concept', bloomLevel: 'understand', tags: ['mixed-motive', 'general-sum', 'autonomous-driving'], order: 3 },
  { id: 'rc-marl-1.3-4', lessonId: 'marl-1.3', prompt: 'You are designing a MARL system for an electronic stock trading platform. Each agent trades to maximize its own profit. What reward structure should you use, and what type of observations might agents receive?', answer: 'This is a **general-sum** (mixed-motive) reward structure, where each agent\'s reward is a function of its individual gains and losses over a trading period. Agents might observe **price developments**, key performance indicators, order book state, and possibly external information like news, while their own buy/sell actions remain **private**.', type: 'application', bloomLevel: 'apply', tags: ['electronic-markets', 'trading', 'general-sum'], order: 4 },
  { id: 'rc-marl-1.3-5', lessonId: 'marl-1.3', prompt: 'In competitive two-player games, during MARL training the agents learn to exploit each other\'s _____ and improve their play to eliminate their own weaknesses.', answer: '**weaknesses**', type: 'cloze', bloomLevel: 'remember', tags: ['competitive-play', 'self-play', 'game-playing'], order: 5 },

  // --- Lesson marl-1.4: Challenges of MARL ---
  { id: 'rc-marl-1.4-1', lessonId: 'marl-1.4', prompt: 'What is the "non-stationarity" challenge in MARL?', answer: '**Non-stationarity** is caused by the continually changing policies of agents during learning. Each agent adapts to other agents\' policies, which are themselves changing, potentially causing **cyclic and unstable** learning dynamics known as the **moving target problem**.', type: 'recall', bloomLevel: 'remember', tags: ['non-stationarity', 'moving-target', 'learning-dynamics'], order: 1 },
  { id: 'rc-marl-1.4-2', lessonId: 'marl-1.4', prompt: 'What is the "multi-agent credit assignment" problem?', answer: '**Multi-agent credit assignment** is the problem of determining which agent\'s action contributed to a received reward. This compounds the temporal credit assignment problem in single-agent RL with the additional challenge of **disentangling** each agent\'s contribution to a collective outcome.', type: 'recall', bloomLevel: 'remember', tags: ['credit-assignment', 'reward-attribution', 'cooperation'], order: 2 },
  { id: 'rc-marl-1.4-3', lessonId: 'marl-1.4', prompt: 'Why does the equilibrium selection problem make MARL more challenging than single-agent RL?', answer: 'In single-agent RL, all optimal policies yield the same expected return. In MARL, there may be **multiple equilibrium solutions**, each entailing different returns for different agents. Agents must essentially **negotiate during learning** which equilibrium to converge to, adding a layer of complexity absent in single-agent settings.', type: 'concept', bloomLevel: 'understand', tags: ['equilibrium-selection', 'optimality', 'convergence'], order: 3 },
  { id: 'rc-marl-1.4-4', lessonId: 'marl-1.4', prompt: 'In a cooperative foraging task, three agents simultaneously choose the "collect" action and the team receives a reward of +1. However, one agent was not adjacent to any item. How does this illustrate the multi-agent credit assignment problem?', answer: 'The agent not adjacent to any item did **not contribute** to the reward since its action had no effect (its level was insufficient or it was out of range). However, given only the joint state/action/reward information, it is **non-trivial to disentangle** each agent\'s actual contribution, illustrating why multi-agent credit assignment remains an open challenge.', type: 'application', bloomLevel: 'apply', tags: ['credit-assignment', 'counterfactual-reasoning', 'foraging'], order: 4 },
  { id: 'rc-marl-1.4-5', lessonId: 'marl-1.4', prompt: 'The total number of possible action combinations between agents may grow _____ with the number of agents.', answer: '**exponentially**', type: 'cloze', bloomLevel: 'remember', tags: ['scalability', 'action-space', 'combinatorial-explosion'], order: 5 },

  // ============================================================
  // Module 2: Reinforcement Learning Foundations
  // ============================================================

  // --- Lesson marl-2.1: General Definition of RL ---
  { id: 'rc-marl-2.1-1', lessonId: 'marl-2.1', prompt: 'What is the general definition of reinforcement learning (RL)?', answer: '**Reinforcement learning** algorithms learn solutions for sequential decision processes via **repeated interaction** with an environment. The agent receives observations, chooses actions, and receives scalar rewards to learn an optimal policy.', type: 'recall', bloomLevel: 'remember', tags: ['reinforcement-learning', 'definition', 'sequential-decision'], order: 1 },
  { id: 'rc-marl-2.1-2', lessonId: 'marl-2.1', prompt: 'What is the exploration-exploitation dilemma in RL?', answer: 'The **exploration-exploitation dilemma** is the challenge of balancing **exploring** the outcomes of different actions (which may discover better actions but can accrue low rewards) versus **exploiting** actions currently believed to be best (which achieves a certain level of returns but may miss the optimal actions).', type: 'recall', bloomLevel: 'remember', tags: ['exploration-exploitation', 'dilemma', 'learning'], order: 2 },
  { id: 'rc-marl-2.1-3', lessonId: 'marl-2.1', prompt: 'How does reinforcement learning differ from supervised learning and unsupervised learning?', answer: 'RL is not **supervised learning** because reward signals do not tell the agent which action to take in each state (some actions may give lower immediate reward but lead to better future states). RL differs from **unsupervised learning** because rewards, while not a supervision signal, act as a **proxy** from which to learn an optimal policy.', type: 'concept', bloomLevel: 'understand', tags: ['machine-learning', 'supervised-learning', 'comparison'], order: 3 },
  { id: 'rc-marl-2.1-4', lessonId: 'marl-2.1', prompt: 'An RL agent in a video game can either take a risky shortcut (unknown outcome) or follow a known safe path that gives moderate reward. How does this relate to the exploration-exploitation dilemma?', answer: 'Taking the risky shortcut is **exploration** -- the agent may discover a higher-reward path, but could also receive a low or negative reward. Following the safe path is **exploitation** -- using existing knowledge for a guaranteed moderate return. A good RL algorithm must **balance** both strategies to find the optimal policy.', type: 'application', bloomLevel: 'apply', tags: ['exploration-exploitation', 'decision-making', 'video-games'], order: 4 },
  { id: 'rc-marl-2.1-5', lessonId: 'marl-2.1', prompt: 'An RL problem is defined as the combination of a _____ model and a learning objective.', answer: '**decision process** (e.g., MDP, POMDP, or multi-armed bandit)', type: 'cloze', bloomLevel: 'remember', tags: ['RL-problem', 'decision-process', 'learning-objective'], order: 5 },

  // --- Lesson marl-2.2: Markov Decision Processes ---
  { id: 'rc-marl-2.2-1', lessonId: 'marl-2.2', prompt: 'What are the five components of a finite Markov decision process (MDP)?', answer: 'A finite MDP consists of: (1) a finite set of **states** S with terminal states, (2) a finite set of **actions** A, (3) a **reward function** R: S x A x S -> R, (4) a **state transition probability** function T: S x A x S -> [0,1], and (5) an **initial state distribution** mu.', type: 'recall', bloomLevel: 'remember', tags: ['MDP', 'components', 'formal-definition'], order: 1 },
  { id: 'rc-marl-2.2-2', lessonId: 'marl-2.2', prompt: 'What does the Markov property state?', answer: 'The **Markov property** states that the future state and reward are **conditionally independent** of past states and actions, given the current state and action: Pr(s_{t+1}, r_t | s_t, a_t, s_{t-1}, ..., s_0, a_0) = Pr(s_{t+1}, r_t | s_t, a_t). The current state provides **sufficient information** to choose optimal actions.', type: 'recall', bloomLevel: 'remember', tags: ['Markov-property', 'conditional-independence', 'MDP'], order: 2 },
  { id: 'rc-marl-2.2-3', lessonId: 'marl-2.2', prompt: 'How does a Partially Observable Markov Decision Process (POMDP) differ from a standard MDP?', answer: 'In a **POMDP**, the agent receives **observations** o_t rather than directly observing the full state s_t. These observations depend probabilistically or deterministically on the state. The agent must consider the **history** of past observations to infer the possible current state, whereas in an MDP the agent can fully observe the state.', type: 'concept', bloomLevel: 'understand', tags: ['POMDP', 'partial-observability', 'MDP-comparison'], order: 3 },
  { id: 'rc-marl-2.2-4', lessonId: 'marl-2.2', prompt: 'In the Mars Rover MDP, the rover can go right (50% chance of reaching base with +10, 50% chance of destruction with -10) or left (longer path through two sites, -1 per step, 0.3 chance of getting stuck with -3). Which direction illustrates the trade-off modeled by MDPs?', answer: 'Going right offers a high expected value but with high **variance** (risk of destruction). Going left is safer but slower with cumulative step costs. This illustrates how MDPs model **sequential decision-making under uncertainty** through probabilistic state transitions and different reward outcomes.', type: 'application', bloomLevel: 'apply', tags: ['Mars-Rover', 'risk', 'state-transitions'], order: 4 },
  { id: 'rc-marl-2.2-5', lessonId: 'marl-2.2', prompt: 'The _____ problem is an important special case of the MDP in which each episode terminates after one time step and there is only a single state.', answer: '**multi-armed bandit** (or bandit)', type: 'cloze', bloomLevel: 'remember', tags: ['multi-armed-bandit', 'MDP-special-case', 'exploration'], order: 5 },

  // --- Lesson marl-2.3: Expected Discounted Returns and Optimal Policies ---
  { id: 'rc-marl-2.3-1', lessonId: 'marl-2.3', prompt: 'What is the discounted return and how is it calculated?', answer: 'The **discounted return** is the expected sum of rewards weighted by a discount factor gamma: E[r_0 + gamma*r_1 + gamma^2*r_2 + ...]. For gamma < 1 and bounded rewards, the discounted return is guaranteed to be finite, bounded by **r_max / (1 - gamma)**.', type: 'recall', bloomLevel: 'remember', tags: ['discounted-return', 'discount-factor', 'rewards'], order: 1 },
  { id: 'rc-marl-2.3-2', lessonId: 'marl-2.3', prompt: 'What are the two equivalent interpretations of the discount factor gamma?', answer: 'First, **(1 - gamma)** can be interpreted as the probability with which the MDP terminates after each time step. Second, the agent gives **weight gamma^t** to reward r_t received at time t, so gamma close to 0 creates a **myopic** agent while gamma close to 1 creates a **farsighted** agent.', type: 'recall', bloomLevel: 'remember', tags: ['discount-factor', 'interpretation', 'myopic-farsighted'], order: 2 },
  { id: 'rc-marl-2.3-3', lessonId: 'marl-2.3', prompt: 'Why is a discount factor needed for non-terminating MDPs, and what convention allows discounted returns to work for terminating MDPs as well?', answer: 'In non-terminating MDPs, total undiscounted return may be **infinite**, making it uninformative for comparing policies. The discount factor gamma < 1 ensures finite returns. For terminating MDPs, the convention of **absorbing states** is used: once a terminal state is reached, the MDP stays in that state forever with **zero reward**, so no more rewards accrue.', type: 'concept', bloomLevel: 'understand', tags: ['discount-factor', 'absorbing-states', 'convergence'], order: 3 },
  { id: 'rc-marl-2.3-4', lessonId: 'marl-2.3', prompt: 'In the Mars Rover MDP, the optimal policy with gamma = 0.95 chooses action "left" from Start (V* = 4.1), while gamma = 0.5 leads to choosing "right" (V* = 0). Why does the discount factor change the optimal policy?', answer: 'With gamma = 0.95, the agent is **farsighted** and values the safer, longer path that avoids risk of destruction. With gamma = 0.5, the agent is **myopic** and heavily discounts future rewards, preferring the immediate gamble of the short path. This shows that **gamma is part of the learning problem specification**, not merely a tunable parameter.', type: 'application', bloomLevel: 'apply', tags: ['discount-factor', 'optimal-policy', 'Mars-Rover'], order: 4 },
  { id: 'rc-marl-2.3-5', lessonId: 'marl-2.3', prompt: 'The optimal policy pi* is defined as the policy that maximizes the _____ in each state of the MDP.', answer: '**expected discounted return**', type: 'cloze', bloomLevel: 'remember', tags: ['optimal-policy', 'expected-return', 'MDP'], order: 5 },

  // --- Lesson marl-2.4: Value Functions and Bellman Equation ---
  { id: 'rc-marl-2.4-1', lessonId: 'marl-2.4', prompt: 'What is the state-value function V^pi(s)?', answer: 'The **state-value function** V^pi(s) gives the expected return when starting in state s and following policy pi to select actions. It is defined as V^pi(s) = E_pi[u_t | s_t = s], where u_t is the discounted reward sequence from time t onward.', type: 'recall', bloomLevel: 'remember', tags: ['value-function', 'state-value', 'expected-return'], order: 1 },
  { id: 'rc-marl-2.4-2', lessonId: 'marl-2.4', prompt: 'What is the Bellman equation for the state-value function?', answer: 'The **Bellman equation** expresses V^pi(s) recursively: V^pi(s) = sum_a pi(a|s) sum_s\' T(s\'|s,a) [R(s,a,s\') + gamma * V^pi(s\')]. It relates the value of a state to the **immediate reward** plus the **discounted value of successor states**.', type: 'recall', bloomLevel: 'remember', tags: ['Bellman-equation', 'recursion', 'value-function'], order: 2 },
  { id: 'rc-marl-2.4-3', lessonId: 'marl-2.4', prompt: 'What is the relationship between the state-value function V^pi and the action-value function Q^pi, and why are both useful?', answer: 'The **action-value function** Q^pi(s,a) gives the expected return when taking action a in state s and then following pi. V^pi(s) equals the sum over actions of pi(a|s) * Q^pi(s,a). Q^pi is especially useful because the **optimal policy** can be derived directly by choosing the action with maximum Q* value: pi*(s) = argmax_a Q*(s,a).', type: 'concept', bloomLevel: 'understand', tags: ['action-value', 'state-value', 'optimal-policy'], order: 3 },
  { id: 'rc-marl-2.4-4', lessonId: 'marl-2.4', prompt: 'Given optimal action-values Q*(Start, left) = 4.1 and Q*(Start, right) = 0 in the Mars Rover MDP, what is the optimal policy at the Start state and what is V*(Start)?', answer: 'The optimal policy at Start is pi*(Start) = **argmax_a Q*(Start, a) = left**, since Q*(Start, left) = 4.1 > Q*(Start, right) = 0. Therefore V*(Start) = **4.1**. This illustrates how the optimal policy is derived by choosing the action with maximum **Q* value** in each state.', type: 'application', bloomLevel: 'apply', tags: ['optimal-policy', 'action-value', 'Mars-Rover'], order: 4 },
  { id: 'rc-marl-2.4-5', lessonId: 'marl-2.4', prompt: 'The _____ optimality equations define a system of non-linear equations whose unique solution is the optimal value function V*/Q*.', answer: '**Bellman**', type: 'cloze', bloomLevel: 'remember', tags: ['Bellman-optimality', 'non-linear-equations', 'optimal-value'], order: 5 },

  // --- Lesson marl-2.5: Dynamic Programming ---
  { id: 'rc-marl-2.5-1', lessonId: 'marl-2.5', prompt: 'What are the two tasks that policy iteration alternates between?', answer: 'Policy iteration alternates between **policy evaluation** (computing the value function V^pi for the current policy pi) and **policy improvement** (improving the current policy by making it greedy with respect to V^pi).', type: 'recall', bloomLevel: 'remember', tags: ['policy-iteration', 'policy-evaluation', 'policy-improvement'], order: 1 },
  { id: 'rc-marl-2.5-2', lessonId: 'marl-2.5', prompt: 'How does value iteration differ from policy iteration?', answer: '**Value iteration** combines one sweep of policy evaluation and policy improvement into a single update using the **Bellman optimality equation**: V_{k+1}(s) = max_a sum_s\' T(s\'|s,a)[R(s,a,s\') + gamma*V_k(s\')]. It converges directly to the optimal value function **V*** without explicitly maintaining a separate policy.', type: 'recall', bloomLevel: 'remember', tags: ['value-iteration', 'Bellman-optimality', 'dynamic-programming'], order: 2 },
  { id: 'rc-marl-2.5-3', lessonId: 'marl-2.5', prompt: 'What is "bootstrapping" in the context of dynamic programming, and why is it important?', answer: '**Bootstrapping** is the property of updating the value estimate for a state s using value estimates of other states s\'. This is central to both iterative policy evaluation and value iteration, allowing algorithms to **propagate value information** through the state space without needing to run complete episodes.', type: 'concept', bloomLevel: 'understand', tags: ['bootstrapping', 'dynamic-programming', 'value-updates'], order: 3 },
  { id: 'rc-marl-2.5-4', lessonId: 'marl-2.5', prompt: 'You have complete knowledge of an MDP with 1000 states. During iterative policy evaluation, after sweep k you find that V_k and V_{k+1} are identical for all states. What can you conclude?', answer: 'The value function has **converged** to the true value function V^pi for the current policy. Since the Bellman operator is a **gamma-contraction mapping**, convergence to a unique fixed point is guaranteed. You can now proceed to the **policy improvement** step to make the policy greedy with respect to V^pi.', type: 'application', bloomLevel: 'apply', tags: ['convergence', 'contraction-mapping', 'policy-evaluation'], order: 4 },
  { id: 'rc-marl-2.5-5', lessonId: 'marl-2.5', prompt: 'Dynamic programming algorithms require complete _____ of the MDP model, including the reward function R and state transition probabilities T.', answer: '**knowledge**', type: 'cloze', bloomLevel: 'remember', tags: ['dynamic-programming', 'model-based', 'MDP-knowledge'], order: 5 },

  // --- Lesson marl-2.6: Temporal-Difference Learning ---
  { id: 'rc-marl-2.6-1', lessonId: 'marl-2.6', prompt: 'What is the Q-learning update rule?', answer: 'The **Q-learning** update rule is: Q(s_t, a_t) <- Q(s_t, a_t) + alpha * [r_t + gamma * max_{a\'} Q(s_{t+1}, a\') - Q(s_t, a_t)]. It uses the **Bellman optimality equation** as the update target, replacing summations with sampled experience tuples.', type: 'recall', bloomLevel: 'remember', tags: ['Q-learning', 'update-rule', 'temporal-difference'], order: 1 },
  { id: 'rc-marl-2.6-2', lessonId: 'marl-2.6', prompt: 'What is the key difference between Sarsa and Q-learning?', answer: '**Sarsa** is an on-policy TD algorithm that uses the update target r_t + gamma*Q(s_{t+1}, a_{t+1}) based on the Bellman equation for Q^pi. **Q-learning** is an off-policy algorithm that uses r_t + gamma*max_{a\'} Q(s_{t+1}, a\'), based on the Bellman optimality equation for Q*. Q-learning can use any exploration policy while still converging to the optimal policy.', type: 'recall', bloomLevel: 'remember', tags: ['Sarsa', 'Q-learning', 'on-policy-off-policy'], order: 2 },
  { id: 'rc-marl-2.6-3', lessonId: 'marl-2.6', prompt: 'Why do TD algorithms use an epsilon-greedy policy rather than a fully greedy policy?', answer: 'A fully greedy policy would violate the requirement of trying all **state-action combinations** infinitely often, which is needed for convergence. The **epsilon-greedy** policy chooses the greedy action with probability (1-epsilon) and a random action with probability epsilon, ensuring continued **exploration** while mostly **exploiting** the current best estimates.', type: 'concept', bloomLevel: 'understand', tags: ['epsilon-greedy', 'exploration', 'convergence-conditions'], order: 3 },
  { id: 'rc-marl-2.6-4', lessonId: 'marl-2.6', prompt: 'An agent using Q-learning with epsilon = 0.1 and alpha = 0.05 observes: state s=3, takes action a=left, receives reward r=-1, arrives at state s\'=5 where max_a Q(5,a) = 8.0, and current Q(3,left) = 2.0 with gamma = 0.9. What is the updated Q(3,left)?', answer: 'The update target is r + gamma * max Q(s\',a) = -1 + 0.9 * 8.0 = **6.2**. The TD error is 6.2 - 2.0 = 4.2. The updated Q(3,left) = 2.0 + 0.05 * 4.2 = **2.21**. The value moves toward the update target, weighted by the learning rate alpha.', type: 'application', bloomLevel: 'apply', tags: ['Q-learning', 'update-calculation', 'TD-error'], order: 4 },
  { id: 'rc-marl-2.6-5', lessonId: 'marl-2.6', prompt: 'Unlike dynamic programming, temporal-difference algorithms do not require complete knowledge of the MDP and instead learn from _____ collected during interaction with the environment.', answer: '**experiences** (i.e., observed states, actions, rewards, and next states)', type: 'cloze', bloomLevel: 'remember', tags: ['temporal-difference', 'model-free', 'experience-based'], order: 5 },

  // --- Lesson marl-2.7: Evaluation with Learning Curves ---
  { id: 'rc-marl-2.7-1', lessonId: 'marl-2.7', prompt: 'What does a learning curve show in RL evaluation?', answer: 'A **learning curve** shows the performance of the learned policy over increasing training time. The x-axis typically shows **cumulative training time steps** across episodes, and the y-axis shows performance metrics such as **expected discounted returns** or other secondary metrics.', type: 'recall', bloomLevel: 'remember', tags: ['learning-curve', 'evaluation', 'performance'], order: 1 },
  { id: 'rc-marl-2.7-2', lessonId: 'marl-2.7', prompt: 'Why should learning curves use cumulative time steps rather than number of episodes on the x-axis?', answer: 'Using episodes on the x-axis can **skew comparisons** between algorithms. One algorithm may complete more time steps (and thus more learning updates) per episode than another, making it appear to learn faster per episode when it actually just collected more **training data**. Cumulative time steps provide a **fairer comparison**.', type: 'recall', bloomLevel: 'remember', tags: ['learning-curve', 'fair-comparison', 'time-steps'], order: 2 },
  { id: 'rc-marl-2.7-3', lessonId: 'marl-2.7', prompt: 'What is the difference between "evaluation returns" and "training returns," and why might we show undiscounted returns even when training with a discount factor?', answer: '**Evaluation returns** are computed by extracting the greedy policy at a given training time and running independent evaluation episodes. Undiscounted returns (gamma=1) may be shown because they are easier to **interpret** (e.g., counting destroyed enemies in a game), even though the policy was trained to maximize **discounted returns** with a specific gamma.', type: 'concept', bloomLevel: 'understand', tags: ['evaluation-returns', 'undiscounted', 'interpretation'], order: 3 },
  { id: 'rc-marl-2.7-4', lessonId: 'marl-2.7', prompt: 'You are comparing two RL algorithms and Algorithm A\'s learning curve rises faster when plotted by episodes, but both curves look similar when plotted by cumulative time steps. What is the most likely explanation?', answer: 'Algorithm A likely explores more **time steps per episode** in early training (longer episodes with more actions), collecting more experiences and performing more learning updates per episode. When normalized by cumulative time steps, the **actual learning efficiency** is similar, showing that the per-episode comparison was misleading.', type: 'application', bloomLevel: 'apply', tags: ['learning-curve', 'algorithm-comparison', 'evaluation-methodology'], order: 4 },
  { id: 'rc-marl-2.7-5', lessonId: 'marl-2.7', prompt: 'The standard approach to evaluate RL algorithm performance involves averaging results over multiple independent training runs, each using a different random _____.', answer: '**seed**', type: 'cloze', bloomLevel: 'remember', tags: ['evaluation', 'random-seed', 'reproducibility'], order: 5 },

  // ============================================================
  // Module 3: Games - Models of Multi-Agent Interaction
  // ============================================================

  // --- Lesson marl-3.1: Normal-Form Games ---
  { id: 'rc-marl-3.1-1', lessonId: 'marl-3.1', prompt: 'What are the components of a normal-form game?', answer: 'A **normal-form game** consists of: (1) a finite set of **agents** I = {1, ..., n}, (2) for each agent i, a finite set of **actions** A_i, and (3) for each agent i, a **reward function** R_i: A -> R where A = A_1 x ... x A_n is the joint action space.', type: 'recall', bloomLevel: 'remember', tags: ['normal-form-game', 'components', 'definition'], order: 1 },
  { id: 'rc-marl-3.1-2', lessonId: 'marl-3.1', prompt: 'What is a "dominant action" in the Prisoner\'s Dilemma?', answer: 'In the Prisoner\'s Dilemma, **defect (D)** is the dominant action because it always achieves a higher reward compared to cooperate (C), regardless of what the other agent chooses. Despite mutual cooperation yielding the second-highest reward for both, each agent is individually incentivized to **defect**.', type: 'recall', bloomLevel: 'remember', tags: ['prisoners-dilemma', 'dominant-action', 'game-theory'], order: 2 },
  { id: 'rc-marl-3.1-3', lessonId: 'marl-3.1', prompt: 'Explain the difference between zero-sum games, common-reward games, and general-sum games.', answer: 'In **zero-sum** games, agents\' rewards always sum to zero (one agent\'s gain is another\'s loss). In **common-reward** games, all agents receive the same reward (pure cooperation). **General-sum** games have no restrictions on reward relationships, allowing for mixed cooperative-competitive dynamics.', type: 'concept', bloomLevel: 'understand', tags: ['zero-sum', 'common-reward', 'general-sum'], order: 3 },
  { id: 'rc-marl-3.1-4', lessonId: 'marl-3.1', prompt: 'Two agents play a coordination game where both choosing A gives reward (10, 10), both choosing B gives (5, 5), and mismatched choices give (0, 0). What type of game is this, and why is coordination challenging?', answer: 'This is a **common-reward** game since both agents always receive the same reward. Coordination is challenging because the agents choose actions **simultaneously** without knowing the other\'s choice. Both (A,A) and (B,B) are viable outcomes, but agents must converge on the **same** action to avoid the zero-reward mismatch.', type: 'application', bloomLevel: 'apply', tags: ['coordination-game', 'common-reward', 'simultaneous-action'], order: 4 },
  { id: 'rc-marl-3.1-5', lessonId: 'marl-3.1', prompt: 'Normal-form games with two agents are also referred to as _____ games because the reward function can be compactly represented as a matrix.', answer: '**matrix**', type: 'cloze', bloomLevel: 'remember', tags: ['matrix-game', 'two-agent', 'normal-form'], order: 5 },

  // --- Lesson marl-3.2: Repeated Normal-Form Games ---
  { id: 'rc-marl-3.2-1', lessonId: 'marl-3.2', prompt: 'What is a repeated normal-form game?', answer: 'A **repeated normal-form game** repeats the same normal-form game over T time steps. At each time step, each agent selects an action based on a policy conditioned on the **joint-action history** h_t = (a^0, ..., a^{t-1}), which contains all joint actions from previous time steps.', type: 'recall', bloomLevel: 'remember', tags: ['repeated-game', 'joint-action-history', 'sequential'], order: 1 },
  { id: 'rc-marl-3.2-2', lessonId: 'marl-3.2', prompt: 'What is the "Tit-for-Tat" policy in the repeated Prisoner\'s Dilemma?', answer: '**Tit-for-Tat** is a policy that cooperates on the first move and then simply mirrors the other agent\'s **most recent action**: cooperating if the other agent cooperated, and defecting if the other agent defected. It is one of the most famous and effective strategies in the repeated Prisoner\'s Dilemma.', type: 'recall', bloomLevel: 'remember', tags: ['tit-for-tat', 'prisoners-dilemma', 'repeated-game'], order: 2 },
  { id: 'rc-marl-3.2-3', lessonId: 'marl-3.2', prompt: 'Why are finitely repeated games not equivalent to infinitely repeated games?', answer: 'In finitely repeated games, **end-game effects** occur: agents that know the game finishes after T steps may change their behavior near the end (e.g., defecting in the last round of Prisoner\'s Dilemma since there is no future retaliation). In infinitely repeated games, each time step has a **termination probability** (1-gamma), so no definitive "last round" exists, supporting sustained cooperation.', type: 'concept', bloomLevel: 'understand', tags: ['finite-vs-infinite', 'end-game-effects', 'repeated-game'], order: 3 },
  { id: 'rc-marl-3.2-4', lessonId: 'marl-3.2', prompt: 'In a repeated Prisoner\'s Dilemma with 100 rounds, both agents know the total number of rounds. Why might mutual cooperation break down, and what design change could help sustain cooperation?', answer: 'In round 100, there is no future round for retaliation, so both agents are incentivized to **defect**. By backward induction, this logic unravels to all rounds, causing **complete defection**. Introducing a **probabilistic termination** (agents unsure when the game ends) eliminates the definitive last round and can sustain cooperation through strategies like Tit-for-Tat.', type: 'application', bloomLevel: 'apply', tags: ['backward-induction', 'cooperation', 'game-design'], order: 4 },
  { id: 'rc-marl-3.2-5', lessonId: 'marl-3.2', prompt: 'In repeated normal-form games, policies can be conditioned on the entire _____ of past joint actions, giving rise to a complex space of possible strategies.', answer: '**history**', type: 'cloze', bloomLevel: 'remember', tags: ['history-dependent', 'policy-space', 'repeated-game'], order: 5 },

  // --- Lesson marl-3.3: Stochastic Games ---
  { id: 'rc-marl-3.3-1', lessonId: 'marl-3.3', prompt: 'What additional components does a stochastic game have compared to a normal-form game?', answer: 'A **stochastic game** adds: (1) a finite set of **states** S with terminal states, (2) a **state transition probability** function T: S x A x S -> [0,1], and (3) an **initial state distribution** mu. Reward functions are now conditioned on states: R_i: S x A x S -> R, allowing rewards to vary across states.', type: 'recall', bloomLevel: 'remember', tags: ['stochastic-game', 'states', 'transitions'], order: 1 },
  { id: 'rc-marl-3.3-2', lessonId: 'marl-3.3', prompt: 'What property of stochastic games is known as "full observability"?', answer: 'In stochastic games, **full observability** means that the state-action history h_t = (s_0, a_0, s_1, a_1, ..., s_t) is observed by **all agents**. Each agent can see the current state s_t and the previous joint actions of all other agents.', type: 'recall', bloomLevel: 'remember', tags: ['full-observability', 'stochastic-game', 'state-history'], order: 2 },
  { id: 'rc-marl-3.3-3', lessonId: 'marl-3.3', prompt: 'How do stochastic games relate to both MDPs and repeated normal-form games?', answer: 'Stochastic games include **MDPs** as the special case with only a single agent. They include **repeated normal-form games** as the special case with only a single state and no terminal states. Each state in a stochastic game can be viewed as a non-repeated normal-form game, making normal-form games the **basic building block** of stochastic games.', type: 'concept', bloomLevel: 'understand', tags: ['game-hierarchy', 'MDP', 'normal-form-game'], order: 3 },
  { id: 'rc-marl-3.3-4', lessonId: 'marl-3.3', prompt: 'You are modeling the level-based foraging environment as a stochastic game. How would you define the state, and what happens to the state when two agents successfully collect an item?', answer: 'The state is a vector specifying the **x-y positions** of all agents and items, plus **binary flags** for each item indicating whether it has been collected. When two agents successfully collect an item, the transition function modifies the state by switching that item\'s binary flag from 1 to 0, indicating the item **no longer exists**.', type: 'application', bloomLevel: 'apply', tags: ['level-based-foraging', 'state-representation', 'stochastic-game'], order: 4 },
  { id: 'rc-marl-3.3-5', lessonId: 'marl-3.3', prompt: 'Stochastic games are also sometimes called _____ games because they satisfy the Markov property for state transitions.', answer: '**Markov**', type: 'cloze', bloomLevel: 'remember', tags: ['Markov-game', 'stochastic-game', 'Markov-property'], order: 5 },

  // --- Lesson marl-3.4: Partially Observable Stochastic Games ---
  { id: 'rc-marl-3.4-1', lessonId: 'marl-3.4', prompt: 'What additional component does a POSG define beyond a stochastic game?', answer: 'A **POSG** additionally defines for each agent i: a finite set of **observations** O_i and an **observation function** O_i: A x S x O_i -> [0,1]. Instead of directly observing the full state, agents receive individual observations that carry some **incomplete information** about the environment state.', type: 'recall', bloomLevel: 'remember', tags: ['POSG', 'observation-function', 'partial-observability'], order: 1 },
  { id: 'rc-marl-3.4-2', lessonId: 'marl-3.4', prompt: 'What is a "belief state" in a POSG?', answer: 'A **belief state** b_i^t is a probability distribution over possible states s in S that the environment may be in at time t, from the perspective of agent i. It is updated via **Bayesian posterior** computation and serves as a **sufficient statistic** carrying enough information to choose optimal actions.', type: 'recall', bloomLevel: 'remember', tags: ['belief-state', 'Bayesian-update', 'sufficient-statistic'], order: 2 },
  { id: 'rc-marl-3.4-3', lessonId: 'marl-3.4', prompt: 'Why is computing exact belief states in POSGs generally intractable, and what practical alternative do deep RL algorithms use?', answer: 'The space complexity of storing exact belief states and the time complexity of Bayesian updates are **exponential** in the number of state variables. Additionally, in multi-agent settings, agents must also infer other agents\' observations and actions. Deep RL algorithms instead use **recurrent neural networks** to sequentially process observations, learning approximate belief representations.', type: 'concept', bloomLevel: 'understand', tags: ['belief-states', 'intractability', 'recurrent-networks'], order: 3 },
  { id: 'rc-marl-3.4-4', lessonId: 'marl-3.4', prompt: 'In robot soccer modeled as a POSG, players can see the full field but cannot directly observe other players\' chosen actions. How would you define the observation function for this scenario?', answer: 'The observation would be o_i^t = (s_t, a_{i}^{t-1}), meaning each agent observes the full **state** (field positions of all players and ball) and its **own previous action**, but not the actions of other players. Agents must **infer** other players\' actions from observed state changes (e.g., inferring a pass from ball trajectory).', type: 'application', bloomLevel: 'apply', tags: ['robot-soccer', 'unobserved-actions', 'POSG'], order: 4 },
  { id: 'rc-marl-3.4-5', lessonId: 'marl-3.4', prompt: 'POSGs with common rewards, in which all agents have the same reward function, are also known as _____.', answer: '**Decentralized POMDPs** (Dec-POMDPs)', type: 'cloze', bloomLevel: 'remember', tags: ['Dec-POMDP', 'common-reward', 'POSG'], order: 5 },

  // --- Lesson marl-3.5: Modeling Communication ---
  { id: 'rc-marl-3.5-1', lessonId: 'marl-3.5', prompt: 'How is communication modeled within the action space of a stochastic game or POSG?', answer: 'Communication is modeled by splitting each agent\'s action space into **A_i = X_i x M_i**, where X_i contains environment actions that affect the state and M_i contains **communication actions** (messages). Each action (x_i, m_i) specifies both an environment action and a message simultaneously.', type: 'recall', bloomLevel: 'remember', tags: ['communication', 'action-space', 'messages'], order: 1 },
  { id: 'rc-marl-3.5-2', lessonId: 'marl-3.5', prompt: 'What key property distinguishes communication actions from environment actions?', answer: 'Communication actions do **not affect the next state** of the environment. Formally, the state transition probabilities are **independent** of the communication actions M: T(s\'|s,a) remains the same regardless of the messages sent. However, messages can be **observed** by other agents.', type: 'recall', bloomLevel: 'remember', tags: ['communication-actions', 'state-independence', 'observation'], order: 2 },
  { id: 'rc-marl-3.5-3', lessonId: 'marl-3.5', prompt: 'How can a POSG model noisy and unreliable communication between agents?', answer: 'Through the **observation function** O_i, a POSG can model: (1) **noisy communication** by modifying messages with random noise (e.g., adding Gaussian noise to continuous messages), (2) **message loss** by setting received messages to empty with some probability, and (3) **limited range** by only delivering messages when agents are within a specified communication range.', type: 'concept', bloomLevel: 'understand', tags: ['noisy-communication', 'observation-function', 'unreliable-channels'], order: 3 },
  { id: 'rc-marl-3.5-4', lessonId: 'marl-3.5', prompt: 'In a level-based foraging environment, you want agents to communicate their intended goal locations. The agents do not know the meaning of communication actions in advance. How will they learn to use these messages effectively?', answer: 'Since RL agents do not know the meaning of any actions in advance, communication actions in M_i are **abstract actions** just like environment actions. Through **trial-and-error** learning, agents must discover both how to **send meaningful messages** and how to **interpret** messages from others, potentially evolving a shared communication protocol or language.', type: 'application', bloomLevel: 'apply', tags: ['emergent-communication', 'learned-language', 'foraging'], order: 4 },
  { id: 'rc-marl-3.5-5', lessonId: 'marl-3.5', prompt: 'In stochastic games and POSGs, communications are _____ in that they last only for a single time step, though agents can remember past messages through the history.', answer: '**ephemeral**', type: 'cloze', bloomLevel: 'remember', tags: ['ephemeral-communication', 'temporal', 'message-persistence'], order: 5 },

  // --- Lesson marl-3.6: Knowledge Assumptions in Games ---
  { id: 'rc-marl-3.6-1', lessonId: 'marl-3.6', prompt: 'What does a "complete knowledge game" mean in game theory?', answer: 'A **complete knowledge game** is one in which all agents have knowledge of all components that define the game: the action spaces, reward functions, state space, state transition function, and observation functions of **all agents** (not just their own).', type: 'recall', bloomLevel: 'remember', tags: ['complete-knowledge', 'game-theory', 'information'], order: 1 },
  { id: 'rc-marl-3.6-2', lessonId: 'marl-3.6', prompt: 'What is the standard knowledge assumption in MARL?', answer: 'In MARL, agents typically do **not know** the reward functions of other agents (nor even their own), and have no knowledge of the state transition and observation functions. Instead, agents only experience **immediate effects** of their actions via their own rewards and observations. This is also known as an **incomplete information game**.', type: 'recall', bloomLevel: 'remember', tags: ['incomplete-information', 'MARL-assumptions', 'unknown-dynamics'], order: 2 },
  { id: 'rc-marl-3.6-3', lessonId: 'marl-3.6', prompt: 'How can knowledge of other agents\' reward functions be useful in MARL, and why is this typically infeasible?', answer: 'If agent i knows agent j\'s reward function R_j, it can estimate j\'s **best-response action**, which informs i\'s own optimal action. However, in most real-world applications, obtaining accurate and complete specifications of reward functions, transition dynamics, and observation functions is **infeasible**. At best, agents may have access to a **simulator** that generates samples.', type: 'concept', bloomLevel: 'understand', tags: ['reward-knowledge', 'best-response', 'simulator'], order: 3 },
  { id: 'rc-marl-3.6-4', lessonId: 'marl-3.6', prompt: 'You are deploying MARL agents for autonomous driving. You have a high-fidelity driving simulator but no closed-form model of the transition dynamics. How can agents learn without complete game knowledge?', answer: 'Agents can learn by interacting with the **simulator**, which generates samples of states, rewards, and observations without requiring closed-form knowledge of T or R. Each agent experiences immediate effects of its actions (reward, next observation) and uses these **sampled experiences** to update its policy, consistent with the standard MARL assumption of **incomplete information**.', type: 'application', bloomLevel: 'apply', tags: ['simulator-based', 'incomplete-information', 'autonomous-driving'], order: 4 },
  { id: 'rc-marl-3.6-5', lessonId: 'marl-3.6', prompt: 'In game theory, when all agents know the game components, know that all agents know them, know that all agents know that all agents know them, and so on, this recursive property is called _____.', answer: '**common knowledge**', type: 'cloze', bloomLevel: 'remember', tags: ['common-knowledge', 'game-theory', 'information-structure'], order: 5 },
  // =============================================
  // MODULE 4: Solution Concepts for Games
  // =============================================

  // --- Lesson 4.1: Joint policies, action profiles, mixed strategies ---
  { id: 'rc-marl-4.1-1', lessonId: 'marl-4.1', prompt: 'What is a joint policy in a multi-agent game?', answer: 'A joint policy is **pi = (pi_1, ..., pi_n)**, consisting of one policy for each agent, that together defines the behavior of all agents in the game.', type: 'recall', bloomLevel: 'remember', tags: ['joint-policy', 'solution-concepts'], order: 1 },
  { id: 'rc-marl-4.1-2', lessonId: 'marl-4.1', prompt: 'What does U_i(pi) represent in multi-agent game theory?', answer: 'U_i(pi) is the **expected return** for agent i under the joint policy pi, defined as the probability-weighted sum of discounted returns across all possible histories.', type: 'recall', bloomLevel: 'remember', tags: ['expected-return', 'joint-policy'], order: 2 },
  { id: 'rc-marl-4.1-3', lessonId: 'marl-4.1', prompt: 'Explain the difference between history-based and recursive definitions of expected return.', answer: 'The **history-based** definition enumerates all possible full histories and sums agent i\'s discounted return weighted by each history\'s probability. The **recursive** (Bellman-style) definition uses interlocked V and Q functions that recursively compute values from any state forward. Both are **mathematically equivalent**.', type: 'concept', bloomLevel: 'understand', tags: ['expected-return', 'bellman'], order: 3 },
  { id: 'rc-marl-4.1-4', lessonId: 'marl-4.1', prompt: 'Given a 2-agent normal-form game where pi(a) = product of pi_i(a_i) * pi_j(a_j), and both agents play uniformly over 3 actions, what is pi(a) for any specific joint action a?', answer: 'Since each agent assigns probability **1/3** to each action, pi(a) = (1/3)(1/3) = **1/9** for any specific joint action.', type: 'application', bloomLevel: 'apply', tags: ['joint-policy', 'mixed-strategy'], order: 4 },
  { id: 'rc-marl-4.1-5', lessonId: 'marl-4.1', prompt: 'A MARL problem is defined by the combination of a _____ and a _____.', answer: 'A MARL problem is defined by the combination of a **game model** and a **solution concept**.', type: 'cloze', bloomLevel: 'remember', tags: ['marl-problem', 'game-model'], order: 5 },

  // --- Lesson 4.2: Best response, dominant strategy, minimax theorem ---
  { id: 'rc-marl-4.2-1', lessonId: 'marl-4.2', prompt: 'What is a best-response policy for agent i?', answer: 'A best-response policy for agent i to pi_{-i} is a policy pi_i that **maximizes the expected return** for agent i when played against the fixed policies of all other agents: BR_i(pi_{-i}) = argmax_{pi_i} U_i(pi_i, pi_{-i}).', type: 'recall', bloomLevel: 'remember', tags: ['best-response', 'solution-concepts'], order: 1 },
  { id: 'rc-marl-4.2-2', lessonId: 'marl-4.2', prompt: 'What is a minimax solution in a two-agent zero-sum game?', answer: 'A joint policy (pi_i, pi_j) is a minimax solution if each agent uses a policy optimized against a **worst-case opponent**. The maxmin value equals the minmax value: max_{pi_i} min_{pi_j} U_i = min_{pi_j} max_{pi_i} U_i, and U_i(pi) = -U_j(pi).', type: 'recall', bloomLevel: 'remember', tags: ['minimax', 'zero-sum'], order: 2 },
  { id: 'rc-marl-4.2-3', lessonId: 'marl-4.2', prompt: 'Why does the order of min/max operators not matter in a minimax solution?', answer: 'In a minimax solution, the **maxmin value** (the best agent i can guarantee) equals the **minmax value** (the worst agent j can force on i). This means neither agent gains from announcing its policy first -- the game value is the same regardless of who commits first.', type: 'concept', bloomLevel: 'understand', tags: ['minimax', 'game-value'], order: 3 },
  { id: 'rc-marl-4.2-4', lessonId: 'marl-4.2', prompt: 'In Rock-Paper-Scissors, what is the minimax solution and what expected return does it give?', answer: 'The unique minimax solution is for both agents to choose each action **uniformly at random** (probability 1/3 each). This gives an expected return of **0** to both agents. Any policy by one agent is a best response against the other\'s uniform policy.', type: 'application', bloomLevel: 'apply', tags: ['minimax', 'rock-paper-scissors'], order: 4 },
  { id: 'rc-marl-4.2-5', lessonId: 'marl-4.2', prompt: 'For non-repeated zero-sum normal-form games, a minimax solution can be computed by solving two _____, one for each agent.', answer: 'For non-repeated zero-sum normal-form games, a minimax solution can be computed by solving two **linear programs**, one for each agent.', type: 'cloze', bloomLevel: 'remember', tags: ['minimax', 'linear-programming'], order: 5 },

  // --- Lesson 4.3: Nash equilibrium definition, Nash's theorem, finding NE ---
  { id: 'rc-marl-4.3-1', lessonId: 'marl-4.3', prompt: 'State the definition of a Nash equilibrium.', answer: 'A joint policy pi = (pi_1, ..., pi_n) is a Nash equilibrium if **no agent i can improve its expected return** by unilaterally changing its policy: for all i and pi\'_i, U_i(pi\'_i, pi_{-i}) <= U_i(pi).', type: 'recall', bloomLevel: 'remember', tags: ['nash-equilibrium', 'definition'], order: 1 },
  { id: 'rc-marl-4.3-2', lessonId: 'marl-4.3', prompt: 'What did Nash (1950) prove about Nash equilibria in normal-form games?', answer: 'Nash proved that **every finite normal-form game has at least one Nash equilibrium**. This equilibrium may be deterministic (pure) or probabilistic (mixed).', type: 'recall', bloomLevel: 'remember', tags: ['nash-theorem', 'existence'], order: 2 },
  { id: 'rc-marl-4.3-3', lessonId: 'marl-4.3', prompt: 'What is the relationship between minimax solutions and Nash equilibria in two-agent zero-sum games?', answer: 'In two-agent zero-sum games, the set of **minimax solutions coincides exactly with the set of Nash equilibria**. Nash equilibrium generalizes minimax to the broader class of general-sum games with any number of agents.', type: 'concept', bloomLevel: 'understand', tags: ['nash-equilibrium', 'minimax'], order: 3 },
  { id: 'rc-marl-4.3-4', lessonId: 'marl-4.3', prompt: 'In the Prisoner\'s Dilemma, what is the Nash equilibrium and why does (C,C) fail to be one?', answer: 'The only Nash equilibrium is **(D, D)**. The joint action (C, C) is not a NE because each agent can unilaterally deviate to D and improve its return from **-1 to 0**, so the no-deviation condition is violated.', type: 'application', bloomLevel: 'apply', tags: ['nash-equilibrium', 'prisoners-dilemma'], order: 4 },
  { id: 'rc-marl-4.3-5', lessonId: 'marl-4.3', prompt: 'To check if a joint policy pi is a Nash equilibrium, for each agent i keep pi_{-i} fixed and compute an optimal _____. If it achieves higher expected return than pi_i, then pi is not a NE.', answer: 'To check if a joint policy pi is a Nash equilibrium, for each agent i keep pi_{-i} fixed and compute an optimal **best-response policy pi\'_i**. If it achieves higher expected return than pi_i, then pi is not a NE.', type: 'cloze', bloomLevel: 'remember', tags: ['nash-equilibrium', 'verification'], order: 5 },

  // --- Lesson 4.4: Correlated equilibrium, correlation device, CE vs NE ---
  { id: 'rc-marl-4.4-1', lessonId: 'marl-4.4', prompt: 'What is a correlated equilibrium?', answer: 'A correlated equilibrium is a joint policy pi_c(a) over joint actions such that **no agent can unilaterally deviate from its recommended action** to increase its expected return, given that it knows only its own recommendation and the distribution pi_c.', type: 'recall', bloomLevel: 'remember', tags: ['correlated-equilibrium', 'definition'], order: 1 },
  { id: 'rc-marl-4.4-2', lessonId: 'marl-4.4', prompt: 'How does a correlated equilibrium differ from a Nash equilibrium?', answer: 'In a Nash equilibrium, agent policies must be **probabilistically independent**. A correlated equilibrium allows **correlation between agents\' actions** via a joint distribution pi_c(a), which can achieve **higher expected returns** than any Nash equilibrium.', type: 'recall', bloomLevel: 'remember', tags: ['correlated-equilibrium', 'nash-equilibrium'], order: 2 },
  { id: 'rc-marl-4.4-3', lessonId: 'marl-4.4', prompt: 'Explain why the set of correlated equilibria contains the set of Nash equilibria.', answer: 'A Nash equilibrium is a **special case** of correlated equilibrium where the joint policy pi_c factors into independent agent policies: pi_c(a) = product of pi_i(a_i). Since knowing your own action gives no information about others\' actions under independence, the CE deviation condition automatically holds for any NE.', type: 'concept', bloomLevel: 'understand', tags: ['correlated-equilibrium', 'nash-equilibrium'], order: 3 },
  { id: 'rc-marl-4.4-4', lessonId: 'marl-4.4', prompt: 'In the Chicken game, the correlated equilibrium pi_c(L,L) = pi_c(S,L) = pi_c(L,S) = 1/3, pi_c(S,S) = 0 gives each agent expected return 5. Why can\'t any agent profitably deviate?', answer: 'If agent i receives recommendation L, it infers the other agent plays S or L with equal probability. Expected return for following L is 2(1/2) + 6(1/2) = **4**, while deviating to S gives 0(1/2) + 7(1/2) = **3.5**. So following the recommendation is strictly better. Similarly for receiving S.', type: 'application', bloomLevel: 'apply', tags: ['correlated-equilibrium', 'chicken-game'], order: 4 },
  { id: 'rc-marl-4.4-5', lessonId: 'marl-4.4', prompt: 'A coarse correlated equilibrium requires the no-deviation condition to hold only for _____ action modifiers, meaning each agent decides whether to follow pi_c before seeing its recommended action.', answer: 'A coarse correlated equilibrium requires the no-deviation condition to hold only for **unconditional** action modifiers, meaning each agent decides whether to follow pi_c before seeing its recommended action.', type: 'cloze', bloomLevel: 'remember', tags: ['coarse-correlated-equilibrium'], order: 5 },

  // --- Lesson 4.5: Equilibrium selection, Pareto-dominated NE, rationality ---
  { id: 'rc-marl-4.5-1', lessonId: 'marl-4.5', prompt: 'What is the equilibrium selection problem?', answer: 'The equilibrium selection problem is the challenge of **which equilibrium the agents should converge to** when a game has multiple equilibria with different expected returns, and **how they can agree** on a specific equilibrium.', type: 'recall', bloomLevel: 'remember', tags: ['equilibrium-selection'], order: 1 },
  { id: 'rc-marl-4.5-2', lessonId: 'marl-4.5', prompt: 'What is the epsilon-Nash equilibrium?', answer: 'An epsilon-Nash equilibrium (for epsilon > 0) is a joint policy pi where **no agent can improve its expected return by more than epsilon** by unilaterally deviating: U_i(pi\'_i, pi_{-i}) - epsilon <= U_i(pi).', type: 'recall', bloomLevel: 'remember', tags: ['epsilon-nash', 'approximate-equilibrium'], order: 2 },
  { id: 'rc-marl-4.5-3', lessonId: 'marl-4.5', prompt: 'Why might an epsilon-Nash equilibrium not be a good approximation of a true Nash equilibrium?', answer: 'An epsilon-NE may be **arbitrarily far** from any true NE in terms of expected returns, even if the NE is unique. The returns under an epsilon-NE can differ from all NE returns by much more than epsilon, because epsilon only bounds the **deviation gain**, not the distance to a true equilibrium.', type: 'concept', bloomLevel: 'understand', tags: ['epsilon-nash', 'approximation'], order: 3 },
  { id: 'rc-marl-4.5-4', lessonId: 'marl-4.5', prompt: 'In a game with NE at (A,C) giving (100,100) and an epsilon-NE at (B,D) giving (1,1) with epsilon=1, is the epsilon-NE a meaningful approximation?', answer: 'No. The epsilon-NE (B,D) gives returns of **(1,1)**, which is 99 away from the true NE returns of **(100,100)**. Despite epsilon being small, the epsilon-NE is **not close to the true NE** in terms of achieved returns. This demonstrates that epsilon only bounds deviation gain, not proximity to the actual equilibrium.', type: 'application', bloomLevel: 'apply', tags: ['epsilon-nash', 'approximation'], order: 4 },
  { id: 'rc-marl-4.5-5', lessonId: 'marl-4.5', prompt: 'In games with more than two agents, the action probabilities in a Nash equilibrium may be _____ numbers, which computer systems cannot fully represent.', answer: 'In games with more than two agents, the action probabilities in a Nash equilibrium may be **irrational** numbers, which computer systems cannot fully represent.', type: 'cloze', bloomLevel: 'remember', tags: ['nash-equilibrium', 'computation'], order: 5 },

  // --- Lesson 4.6: Pareto optimality, social welfare, price of anarchy ---
  { id: 'rc-marl-4.6-1', lessonId: 'marl-4.6', prompt: 'What does it mean for a joint policy to be Pareto-optimal?', answer: 'A joint policy pi is Pareto-optimal if **no other joint policy exists** that improves the expected return for at least one agent without reducing the expected return for any other agent. No agent can be made better off without making someone worse off.', type: 'recall', bloomLevel: 'remember', tags: ['pareto-optimality'], order: 1 },
  { id: 'rc-marl-4.6-2', lessonId: 'marl-4.6', prompt: 'How is social welfare defined for a joint policy?', answer: 'The social welfare W(pi) of a joint policy is the **sum of expected returns** of all agents: W(pi) = sum_{i in I} U_i(pi). A welfare-optimal joint policy maximizes this sum.', type: 'recall', bloomLevel: 'remember', tags: ['social-welfare', 'welfare-optimality'], order: 2 },
  { id: 'rc-marl-4.6-3', lessonId: 'marl-4.6', prompt: 'Explain the relationship between welfare optimality and Pareto optimality.', answer: '**Welfare optimality implies Pareto optimality**, but not vice versa. If pi is welfare-optimal but not Pareto-optimal, there would exist pi\' with all returns at least as high and one strictly higher, meaning a higher sum -- contradicting welfare optimality. However, a Pareto-optimal policy may not maximize the total sum of returns.', type: 'concept', bloomLevel: 'understand', tags: ['pareto-optimality', 'social-welfare'], order: 3 },
  { id: 'rc-marl-4.6-4', lessonId: 'marl-4.6', prompt: 'For joint policies with expected returns (1,5), (2,4), and (3,3), which is fairness-optimal using F(pi) = product of U_i(pi)?', answer: 'The fairness values are: F(1,5) = **5**, F(2,4) = **8**, F(3,3) = **9**. The joint policy giving **(3,3)** is fairness-optimal because it maximizes the product of returns, promoting equity among agents.', type: 'application', bloomLevel: 'apply', tags: ['fairness', 'social-welfare'], order: 4 },
  { id: 'rc-marl-4.6-5', lessonId: 'marl-4.6', prompt: 'The fairness of a joint policy is defined as F(pi) = _____ of U_i(pi) over all agents, promoting equity by favoring equal distribution of returns.', answer: 'The fairness of a joint policy is defined as F(pi) = **product** of U_i(pi) over all agents, promoting equity by favoring equal distribution of returns.', type: 'cloze', bloomLevel: 'remember', tags: ['fairness', 'definition'], order: 5 },

  // --- Lesson 4.7: External regret, no-regret property, connection to CCE ---
  { id: 'rc-marl-4.7-1', lessonId: 'marl-4.7', prompt: 'What is external (unconditional) regret in a normal-form game?', answer: 'External regret measures the difference between the rewards agent i actually received and the rewards it **would have received if it had played the single best action** in all past episodes: Regret^z_i = max_{a_i} sum_e [R_i(a_i, a^e_{-i}) - R_i(a^e)].', type: 'recall', bloomLevel: 'remember', tags: ['regret', 'external-regret'], order: 1 },
  { id: 'rc-marl-4.7-2', lessonId: 'marl-4.7', prompt: 'When does an agent satisfy the no-regret property?', answer: 'An agent has **no-regret** if its average regret in the limit of infinitely many episodes is at most zero: lim_{z->inf} (1/z) Regret^z_i <= 0.', type: 'recall', bloomLevel: 'remember', tags: ['no-regret', 'definition'], order: 2 },
  { id: 'rc-marl-4.7-3', lessonId: 'marl-4.7', prompt: 'What is the connection between no-regret learning and equilibrium concepts?', answer: 'If all agents have **no external regret**, the empirical distribution of joint actions converges to the set of **coarse correlated equilibria**. If all agents have **no internal (conditional) regret**, the empirical distribution converges to the set of **correlated equilibria**.', type: 'concept', bloomLevel: 'understand', tags: ['no-regret', 'correlated-equilibrium'], order: 3 },
  { id: 'rc-marl-4.7-4', lessonId: 'marl-4.7', prompt: 'In 10 episodes of Prisoner\'s Dilemma, agent 1 received total reward -21. Always playing D would have given -15. What is agent 1\'s regret?', answer: 'Regret = (best alternative total) - (actual total) = **-15 - (-21) = 6**. The average regret is 6/10 = **0.6**. Agent 1 regrets not having always played D.', type: 'application', bloomLevel: 'apply', tags: ['regret', 'prisoners-dilemma'], order: 4 },
  { id: 'rc-marl-4.7-5', lessonId: 'marl-4.7', prompt: 'Internal (conditional) regret replaces only specific occurrences of action a\'_i with a different action a_i, while external (unconditional) regret replaces _____ of agent i\'s past actions.', answer: 'Internal (conditional) regret replaces only specific occurrences of action a\'_i with a different action a_i, while external (unconditional) regret replaces **all** of agent i\'s past actions.', type: 'cloze', bloomLevel: 'remember', tags: ['regret', 'internal-vs-external'], order: 5 },

  // --- Lesson 4.8: PPAD-completeness, LP for CE, Lemke-Howson ---
  { id: 'rc-marl-4.8-1', lessonId: 'marl-4.8', prompt: 'What complexity class does computing a Nash equilibrium in a general-sum normal-form game belong to?', answer: 'Computing a Nash equilibrium (NASH) is **PPAD-complete**. This was proven first for games with three or more agents, and then for two-agent games.', type: 'recall', bloomLevel: 'remember', tags: ['ppad', 'computational-complexity'], order: 1 },
  { id: 'rc-marl-4.8-2', lessonId: 'marl-4.8', prompt: 'What is the PPAD-complete problem END-OF-LINE?', answer: 'END-OF-LINE is defined on a directed graph with 2^k nodes. Given Parent/Child circuit functions and a **source node** (no parent), the task is to find a **sink node** (no child) or a different source node. Following the path may require **exponential time** in k.', type: 'recall', bloomLevel: 'remember', tags: ['ppad', 'end-of-line'], order: 2 },
  { id: 'rc-marl-4.8-3', lessonId: 'marl-4.8', prompt: 'Why is the PPAD-completeness of NASH significant for MARL?', answer: 'Since no efficient (polynomial-time) algorithms are known for PPAD-complete problems, it is unlikely that **efficient MARL algorithms exist** to compute Nash equilibria for general games. Any MARL algorithm likely requires **exponential time in the worst case** without exploiting special game structure.', type: 'concept', bloomLevel: 'understand', tags: ['ppad', 'marl-implications'], order: 3 },
  { id: 'rc-marl-4.8-4', lessonId: 'marl-4.8', prompt: 'Which equilibrium types can be computed in polynomial time for non-repeated normal-form games, and which cannot?', answer: '**Minimax** solutions in two-agent zero-sum games and **correlated equilibria** in general-sum games can be computed in polynomial time via **linear programming**. Computing **Nash equilibria** in general-sum games is PPAD-complete and has no known polynomial-time algorithm.', type: 'application', bloomLevel: 'apply', tags: ['computational-complexity', 'linear-programming'], order: 4 },
  { id: 'rc-marl-4.8-5', lessonId: 'marl-4.8', prompt: 'PPAD stands for "polynomial parity argument for _____" and describes a class of total search problems that always have solutions.', answer: 'PPAD stands for "polynomial parity argument for **directed graphs**" and describes a class of total search problems that always have solutions.', type: 'cloze', bloomLevel: 'remember', tags: ['ppad', 'complexity-class'], order: 5 },

  // =============================================
  // MODULE 5: MARL First Steps and Challenges
  // =============================================

  // --- Lesson 5.1: MARL loop, non-stationary env, moving target ---
  { id: 'rc-marl-5.1-1', lessonId: 'marl-5.1', prompt: 'What are the four main elements of the general MARL learning process?', answer: 'The four elements are: (1) **Game model** defining the multi-agent environment, (2) **Data** consisting of histories from episodes, (3) **Learning algorithm** L that updates the joint policy, and (4) **Learning goal** -- a joint policy satisfying a chosen solution concept.', type: 'recall', bloomLevel: 'remember', tags: ['marl-loop', 'learning-process'], order: 1 },
  { id: 'rc-marl-5.1-2', lessonId: 'marl-5.1', prompt: 'How is the joint policy updated in each learning step?', answer: 'The learning algorithm L takes the collected data D^z and the current joint policy pi^z, and produces a new joint policy: **pi^{z+1} = L(D^z, pi^z)**. The initial joint policy pi^0 is typically random.', type: 'recall', bloomLevel: 'remember', tags: ['learning-algorithm', 'policy-update'], order: 2 },
  { id: 'rc-marl-5.1-3', lessonId: 'marl-5.1', prompt: 'Why does multi-agent RL make the environment appear non-stationary from each agent\'s perspective?', answer: 'In MARL, all agents simultaneously update their policies based on experience. From agent i\'s perspective, the other agents\' changing policies become part of the **environment dynamics** (via the effective transition function T_i). Since these policies change over time, the environment appears **non-stationary** and violates the Markov property.', type: 'concept', bloomLevel: 'understand', tags: ['non-stationarity', 'moving-target'], order: 3 },
  { id: 'rc-marl-5.1-4', lessonId: 'marl-5.1', prompt: 'An RL agent updates Q-values using target r + gamma * Q(s\', a\'). In MARL, why is this target called a "moving target"?', answer: 'The target depends on **value estimates that are themselves changing** as the agent\'s policy updates. In MARL, this is further exacerbated because other agents\' policies also change, making the **transition dynamics and reward expectations shift over time** -- hence the Q-value target keeps "moving."', type: 'application', bloomLevel: 'apply', tags: ['moving-target', 'td-learning'], order: 4 },
  { id: 'rc-marl-5.1-5', lessonId: 'marl-5.1', prompt: 'In MARL, from the perspective of agent i, the policies of other agents j become part of the environment\'s _____ function.', answer: 'In MARL, from the perspective of agent i, the policies of other agents j become part of the environment\'s **state transition** function.', type: 'cloze', bloomLevel: 'remember', tags: ['non-stationarity', 'transition-function'], order: 5 },

  // --- Lesson 5.2: Convergence criteria, Q-learning breakdown, self-play convergence ---
  { id: 'rc-marl-5.2-1', lessonId: 'marl-5.2', prompt: 'What is the standard (strongest) convergence criterion for MARL algorithms?', answer: 'The standard criterion is **pointwise convergence of the joint policy** to a solution: lim_{z->inf} pi^z = pi*, where pi* satisfies the chosen solution concept (e.g., Nash equilibrium).', type: 'recall', bloomLevel: 'remember', tags: ['convergence', 'policy-convergence'], order: 1 },
  { id: 'rc-marl-5.2-2', lessonId: 'marl-5.2', prompt: 'What is convergence of the empirical distribution in MARL?', answer: 'The empirical distribution bar{pi}^z is the **averaged joint policy across episodes**: bar{pi}^z(a|h) = (1/z) sum_e pi^e(a|h). Convergence means lim_{z->inf} bar{pi}^z = pi*, even if individual policies pi^z do not converge.', type: 'recall', bloomLevel: 'remember', tags: ['convergence', 'empirical-distribution'], order: 2 },
  { id: 'rc-marl-5.2-3', lessonId: 'marl-5.2', prompt: 'Why are weaker convergence types useful in MARL?', answer: 'Some algorithms cannot achieve pointwise policy convergence. For example, **fictitious play** learns deterministic best responses and cannot represent probabilistic NE directly, but its empirical distribution can converge. **Regret matching** produces chaotic policy changes, but empirical distributions converge to (coarse) correlated equilibria. Weaker criteria capture these useful behaviors.', type: 'concept', bloomLevel: 'understand', tags: ['convergence', 'weaker-criteria'], order: 3 },
  { id: 'rc-marl-5.2-4', lessonId: 'marl-5.2', prompt: 'An algorithm\'s average return converges to NE returns (Eq 5.8), but individual episode returns fluctuate wildly. Is this still useful?', answer: 'This is the **convergence of average return** criterion. While theoretically valid, it is practically weak because any **individual episode\'s return can be arbitrarily bad** as long as the average converges. This is why **policy convergence** (Eq 5.3) is preferred -- it guarantees good returns in each episode eventually.', type: 'application', bloomLevel: 'apply', tags: ['convergence', 'average-return'], order: 4 },
  { id: 'rc-marl-5.2-5', lessonId: 'marl-5.2', prompt: 'Pointwise policy convergence (Eq 5.3) implies all _____ types of convergence, but makes no claims about the performance of any individual pi^z for finite z.', answer: 'Pointwise policy convergence (Eq 5.3) implies all **weaker** types of convergence, but makes no claims about the performance of any individual pi^z for finite z.', type: 'cloze', bloomLevel: 'remember', tags: ['convergence', 'hierarchy'], order: 5 },

  // --- Lesson 5.3: Centralised vs independent learning, IQL, joint action explosion ---
  { id: 'rc-marl-5.3-1', lessonId: 'marl-5.3', prompt: 'What is central learning (CQL) in MARL?', answer: 'Central learning trains a **single central policy pi_c** that receives all agents\' observations and selects **joint actions** from A = A_1 x ... x A_n. It reduces the multi-agent problem to a single-agent problem by applying standard RL to the joint-action space.', type: 'recall', bloomLevel: 'remember', tags: ['central-learning', 'cql'], order: 1 },
  { id: 'rc-marl-5.3-2', lessonId: 'marl-5.3', prompt: 'What is independent learning (IQL) in MARL?', answer: 'In independent learning, each agent i learns its own policy pi_i using **only its local observations, actions, and rewards**, while ignoring the existence of other agents. Each agent applies single-agent RL independently.', type: 'recall', bloomLevel: 'remember', tags: ['independent-learning', 'iql'], order: 2 },
  { id: 'rc-marl-5.3-3', lessonId: 'marl-5.3', prompt: 'Compare the key trade-offs between central learning and independent learning.', answer: 'Central learning **avoids non-stationarity and credit assignment problems** but suffers from **exponential joint-action space** growth and requires **reward scalarization**. Independent learning **scales naturally** per agent and uses local rewards, but faces **non-stationarity** from other agents\' changing policies and may not converge.', type: 'concept', bloomLevel: 'understand', tags: ['central-learning', 'independent-learning', 'trade-offs'], order: 3 },
  { id: 'rc-marl-5.3-4', lessonId: 'marl-5.3', prompt: 'Three agents each have 6 actions. How many joint actions exist for CQL vs individual actions per agent for IQL?', answer: 'CQL must explore **6^3 = 216 joint actions** per state. Each IQL agent explores only **6 actions** per state. This explains why IQL can learn faster in practice despite lacking multi-agent coordination guarantees.', type: 'application', bloomLevel: 'apply', tags: ['joint-action-space', 'scalability'], order: 4 },
  { id: 'rc-marl-5.3-5', lessonId: 'marl-5.3', prompt: 'In independent Q-learning, agent i\'s effective transition function T_i(s\'|s, a_i) incorporates other agents\' policies via T_i proportional to sum_{a_{-i}} T(s\'|s, <a_i, a_{-i}>) _____.', answer: 'In independent Q-learning, agent i\'s effective transition function T_i(s\'|s, a_i) incorporates other agents\' policies via T_i proportional to sum_{a_{-i}} T(s\'|s, <a_i, a_{-i}>) **product_{j != i} pi_j(a_j | s)**.', type: 'cloze', bloomLevel: 'remember', tags: ['iql', 'effective-transition'], order: 5 },

  // --- Lesson 5.4: Non-stationarity, policy oscillation, equilibrium selection ---
  { id: 'rc-marl-5.4-1', lessonId: 'marl-5.4', prompt: 'What causes non-stationarity in multi-agent reinforcement learning?', answer: 'Non-stationarity results from the **continual co-adaptation of multiple learning agents**. As each agent updates its policy based on interactions, the other agents\' changing policies alter the effective environment dynamics, creating **cyclic dynamics** and violating the Markov assumption.', type: 'recall', bloomLevel: 'remember', tags: ['non-stationarity', 'co-adaptation'], order: 1 },
  { id: 'rc-marl-5.4-2', lessonId: 'marl-5.4', prompt: 'What are risk-dominant and reward-dominant equilibria?', answer: 'In a game like Stag Hunt, the **reward-dominant** equilibrium (S,S) yields maximum payoffs but requires coordination. The **risk-dominant** equilibrium (H,H) has lower payoffs but each agent can guarantee a minimum reward independently, making it "safer."', type: 'recall', bloomLevel: 'remember', tags: ['equilibrium-selection', 'stag-hunt'], order: 2 },
  { id: 'rc-marl-5.4-3', lessonId: 'marl-5.4', prompt: 'Why might independent Q-learning converge to a risk-dominant rather than reward-dominant equilibrium?', answer: 'In early learning, agents explore **randomly** and quickly discover that risky cooperative actions (like S in Stag Hunt) can yield 0 reward when uncoordinated, while safe actions (H) guarantee at least 2. This **reinforces the safe action** in a feedback loop, steering agents toward the risk-dominant equilibrium even though the reward-dominant one is better for all.', type: 'concept', bloomLevel: 'understand', tags: ['equilibrium-selection', 'iql'], order: 3 },
  { id: 'rc-marl-5.4-4', lessonId: 'marl-5.4', prompt: 'In the Stag Hunt game, both (S,S) and (H,H) are NE. How could agent modeling help achieve the better equilibrium (S,S)?', answer: 'If agent 1 **models agent 2\'s behavior** and predicts that agent 2 is likely to choose S when agent 1 cooperates, then agent 1 will learn to choose S more frequently. This can create a **positive feedback loop** where both agents gradually converge to the reward-dominant equilibrium (S,S).', type: 'application', bloomLevel: 'apply', tags: ['equilibrium-selection', 'agent-modeling'], order: 4 },
  { id: 'rc-marl-5.4-5', lessonId: 'marl-5.4', prompt: 'The non-stationarity problem in MARL is also called the _____ problem, because value estimates and update targets keep changing as agents learn.', answer: 'The non-stationarity problem in MARL is also called the **moving target** problem, because value estimates and update targets keep changing as agents learn.', type: 'cloze', bloomLevel: 'remember', tags: ['non-stationarity', 'moving-target'], order: 5 },

  // --- Lesson 5.5: Credit assignment, lazy agent, relative overgeneralisation ---
  { id: 'rc-marl-5.5-1', lessonId: 'marl-5.5', prompt: 'What is the multi-agent credit assignment problem?', answer: 'Multi-agent credit assignment is the problem of determining **whose actions among multiple agents contributed to a received reward**. This is distinct from temporal credit assignment (which past actions contributed) and is especially prominent in **common-reward** settings.', type: 'recall', bloomLevel: 'remember', tags: ['credit-assignment', 'multi-agent'], order: 1 },
  { id: 'rc-marl-5.5-2', lessonId: 'marl-5.5', prompt: 'How do joint-action value functions help with credit assignment?', answer: 'A joint-action value function Q_i(s, a_1, ..., a_n) explicitly models the **impact of each agent\'s action on the received reward**, enabling counterfactual reasoning like "What reward would I have received if agent j did X instead of Y?" This is used in central Q-learning and joint-action learning.', type: 'recall', bloomLevel: 'remember', tags: ['credit-assignment', 'joint-action-values'], order: 2 },
  { id: 'rc-marl-5.5-3', lessonId: 'marl-5.5', prompt: 'Explain the "lazy agent" problem in common-reward multi-agent settings.', answer: 'In common-reward settings, if one agent\'s actions consistently lead to positive rewards regardless of what a second agent does, the second agent\'s action gets **repeatedly reinforced** even though it made **no contribution**. The second agent becomes "lazy," receiving credit for outcomes it did not cause.', type: 'concept', bloomLevel: 'understand', tags: ['credit-assignment', 'lazy-agent'], order: 3 },
  { id: 'rc-marl-5.5-4', lessonId: 'marl-5.5', prompt: 'In level-based foraging, three agents all choose "collect" and receive reward +1. Only two agents on the right actually collected the item. How does IQL misattribute credit?', answer: 'IQL uses individual Q-values Q(s, a_i) that **ignore other agents\' actions**. The left agent\'s "collect" action gets the same +1 reward signal, so IQL **incorrectly reinforces** that action for the left agent, even though it contributed nothing. A joint-action value Q(s, a_1, a_2, a_3) could correctly differentiate the contributions.', type: 'application', bloomLevel: 'apply', tags: ['credit-assignment', 'iql', 'level-based-foraging'], order: 4 },
  { id: 'rc-marl-5.5-5', lessonId: 'marl-5.5', prompt: 'Difference rewards address credit assignment by asking the counterfactual question: "What reward would I have received if agent j had done a _____ instead?"', answer: 'Difference rewards address credit assignment by asking the counterfactual question: "What reward would I have received if agent j had done a **default action** instead?"', type: 'cloze', bloomLevel: 'remember', tags: ['credit-assignment', 'difference-rewards'], order: 5 },

  // --- Lesson 5.6: Self-play benefits/limits, cyclic strategies, mixed-play ---
  { id: 'rc-marl-5.6-1', lessonId: 'marl-5.6', prompt: 'What are the two definitions of self-play in MARL?', answer: '**Algorithm self-play**: all agents use the same learning algorithm (but may learn different policies). **Policy self-play**: an agent\'s policy is directly trained against itself, sharing one policy across all agent roles. Policy self-play implies algorithm self-play but not vice versa.', type: 'recall', bloomLevel: 'remember', tags: ['self-play', 'algorithm-self-play', 'policy-self-play'], order: 1 },
  { id: 'rc-marl-5.6-2', lessonId: 'marl-5.6', prompt: 'What is mixed-play in MARL?', answer: 'Mixed-play describes settings where agents use **different learning algorithms**. Examples include trading markets (different organizations control agents) and ad hoc teamwork (collaborating with previously unknown agents).', type: 'recall', bloomLevel: 'remember', tags: ['mixed-play', 'heterogeneous-agents'], order: 2 },
  { id: 'rc-marl-5.6-3', lessonId: 'marl-5.6', prompt: 'Why might policy self-play learn faster than algorithm self-play, and what are its limitations?', answer: 'Policy self-play can be faster because **all agents\' experiences train a single shared policy**, effectively multiplying the training data. However, it requires agents to have **symmetrical roles and egocentric observations**, so the same policy makes sense from each agent\'s perspective. Algorithm self-play has no such restriction.', type: 'concept', bloomLevel: 'understand', tags: ['self-play', 'efficiency'], order: 3 },
  { id: 'rc-marl-5.6-4', lessonId: 'marl-5.6', prompt: 'A MARL algorithm aims to converge to an equilibrium in self-play and converge to a best-response policy against stationary opponents. Why is this dual goal useful?', answer: 'This dual goal (proposed by Bowling and Veloso, 2002) ensures the algorithm is **robust**: in self-play it finds a stable equilibrium, but if other agents use fixed policies (mixed-play), it doesn\'t just match them -- it **exploits** them by computing an optimal best response, achieving the highest possible return.', type: 'application', bloomLevel: 'apply', tags: ['self-play', 'mixed-play', 'best-response'], order: 4 },
  { id: 'rc-marl-5.6-5', lessonId: 'marl-5.6', prompt: 'Population-based training extends self-play by training policies against a _____ of other policies, including past versions of themselves.', answer: 'Population-based training extends self-play by training policies against a **distribution** of other policies, including past versions of themselves.', type: 'cloze', bloomLevel: 'remember', tags: ['population-based-training', 'self-play'], order: 5 },

  // --- Lesson 5.7: MARL taxonomy, challenge-solution map ---
  { id: 'rc-marl-5.7-1', lessonId: 'marl-5.7', prompt: 'List the four core challenges specific to MARL (beyond single-agent RL challenges).', answer: 'The four core MARL challenges are: (1) **Non-stationarity** from concurrent learning agents, (2) **Equilibrium selection** when multiple equilibria exist, (3) **Multi-agent credit assignment** determining whose actions caused rewards, and (4) **Scaling to many agents** with exponential joint-action growth.', type: 'recall', bloomLevel: 'remember', tags: ['marl-challenges', 'taxonomy'], order: 1 },
  { id: 'rc-marl-5.7-2', lessonId: 'marl-5.7', prompt: 'How does the joint-action space grow with the number of agents?', answer: 'The joint-action space grows **exponentially**: |A| = |A_1| * |A_2| * ... * |A_n|. For n agents with k actions each, there are **k^n** possible joint actions. This affects algorithms using joint-action values and even independent learners through increased non-stationarity.', type: 'recall', bloomLevel: 'remember', tags: ['scalability', 'joint-action-space'], order: 2 },
  { id: 'rc-marl-5.7-3', lessonId: 'marl-5.7', prompt: 'Explain how the two basic MARL reductions (central and independent learning) each address some challenges while creating others.', answer: 'Central learning **eliminates non-stationarity and helps credit assignment** via joint-action values, but creates **exponential action spaces** and needs reward scalarization. Independent learning **scales well per-agent** and uses local rewards, but introduces **non-stationarity** (environment changes as others learn) and **poor credit assignment** (cannot distinguish own vs. others\' contributions).', type: 'concept', bloomLevel: 'understand', tags: ['central-learning', 'independent-learning', 'trade-offs'], order: 3 },
  { id: 'rc-marl-5.7-4', lessonId: 'marl-5.7', prompt: 'A power plant has 1000 control variables each taking k values. Factoring into n=10 agent groups of 100 variables each, does the total joint-action count change?', answer: 'No. The total joint-action count is **k^1000 regardless of n**. Factoring into 10 agents gives each |A_i| = k^100, and |A| = (k^100)^10 = k^1000. The MARL decomposition makes each agent\'s problem **smaller** but the total space is unchanged. This shows exponential growth is inherent to the problem, not unique to MARL.', type: 'application', bloomLevel: 'apply', tags: ['scalability', 'decomposition'], order: 4 },
  { id: 'rc-marl-5.7-5', lessonId: 'marl-5.7', prompt: 'MARL algorithms inherit challenges from single-agent RL such as unknown dynamics and exploration-exploitation, plus additional challenges from learning in a _____ multi-agent system.', answer: 'MARL algorithms inherit challenges from single-agent RL such as unknown dynamics and exploration-exploitation, plus additional challenges from learning in a **dynamic** multi-agent system.', type: 'cloze', bloomLevel: 'remember', tags: ['marl-challenges', 'overview'], order: 5 },

  // =============================================
  // MODULE 6: Foundational Algorithms
  // =============================================

  // --- Lesson 6.1: Minimax value iteration, Shapley's theorem ---
  { id: 'rc-marl-6.1-1', lessonId: 'marl-6.1', prompt: 'What does the value iteration algorithm for stochastic games compute?', answer: 'It computes the optimal expected returns (minimax values) **V*_i(s)** for each agent i and state s in **two-agent zero-sum stochastic games**, by iteratively solving normal-form games constructed from value estimates at each state.', type: 'recall', bloomLevel: 'remember', tags: ['value-iteration', 'stochastic-games'], order: 1 },
  { id: 'rc-marl-6.1-2', lessonId: 'marl-6.1', prompt: 'What two sweeps does the value iteration algorithm for stochastic games perform?', answer: 'Sweep 1: Compute matrices **M_{s,i}(a)** for each state and agent, estimating expected returns for each joint action. Sweep 2: Update **V_i(s)** using the minimax value of the normal-form game defined by the M matrices, i.e., V_i(s) <- Value_i(M_{s,1}, ..., M_{s,n}).', type: 'recall', bloomLevel: 'remember', tags: ['value-iteration', 'algorithm-steps'], order: 2 },
  { id: 'rc-marl-6.1-3', lessonId: 'marl-6.1', prompt: 'How does value iteration for stochastic games reduce to MDP value iteration when there is only one agent?', answer: 'With a single agent, **Value_i(M_{s,i}) = max_{a_i} M_{s,i}(a_i)** since there are no opponents. The minimax operator reduces to a simple **max** over the agent\'s actions, which is exactly the Bellman optimality update V(s) <- max_a [R + gamma * V(s\')] used in MDP value iteration.', type: 'concept', bloomLevel: 'understand', tags: ['value-iteration', 'mdp-connection'], order: 3 },
  { id: 'rc-marl-6.1-4', lessonId: 'marl-6.1', prompt: 'In the value iteration algorithm, the matrix entry M_{s,i}(a) = sum_{s\'} T(s\'|s,a)[R_i(s,a,s\') + gamma * V_i(s\')]. What does this entry represent?', answer: 'M_{s,i}(a) represents the **approximate expected return** for agent i after selecting joint action a in state s and then following the current value estimates thereafter. It combines the **immediate reward** with the **discounted future value**, forming a one-step lookahead from state s.', type: 'application', bloomLevel: 'apply', tags: ['value-iteration', 'matrix-entry'], order: 4 },
  { id: 'rc-marl-6.1-5', lessonId: 'marl-6.1', prompt: 'Shapley (1953) proved that value iteration for zero-sum stochastic games converges because the update operator is a _____ mapping.', answer: 'Shapley (1953) proved that value iteration for zero-sum stochastic games converges because the update operator is a **contraction** mapping.', type: 'cloze', bloomLevel: 'remember', tags: ['shapley', 'contraction-mapping'], order: 5 },

  // --- Lesson 6.2: Joint-action Q-values, TD update ---
  { id: 'rc-marl-6.2-1', lessonId: 'marl-6.2', prompt: 'What is joint-action learning (JAL) in MARL?', answer: 'JAL is a family of MARL algorithms based on **temporal-difference learning** that estimate **joint-action value functions** Q_i(s, a_1, ..., a_n) representing expected returns for each agent when agents select a specific joint action in a given state.', type: 'recall', bloomLevel: 'remember', tags: ['joint-action-learning', 'td-learning'], order: 1 },
  { id: 'rc-marl-6.2-2', lessonId: 'marl-6.2', prompt: 'In JAL-GT algorithms, what role does the normal-form game Gamma_s play?', answer: 'For each state s, the joint-action values Q_1(s,.), ..., Q_n(s,.) define a **normal-form game Gamma_s** with R_i(a) = Q_i(s,a). This game is solved using a game-theoretic solution concept (minimax, Nash, or CE) to derive **action selection** and **TD update targets**.', type: 'recall', bloomLevel: 'remember', tags: ['jal-gt', 'normal-form-game'], order: 2 },
  { id: 'rc-marl-6.2-3', lessonId: 'marl-6.2', prompt: 'Explain why joint-action values alone are insufficient to reconstruct equilibrium policies in all stochastic games.', answer: 'In "NoSDE" (No Stationary Deterministic Equilibrium) games, the unique equilibrium requires **specific action probabilities** that cannot be derived from Q-values alone. Two different games can have **identical Q-functions** but different equilibrium policies, because equilibrium probabilities depend on the reward structure in ways not captured by Q-values.', type: 'concept', bloomLevel: 'understand', tags: ['jal-limitations', 'nosde'], order: 3 },
  { id: 'rc-marl-6.2-4', lessonId: 'marl-6.2', prompt: 'Write the TD update rule for agent j in the JAL-GT algorithm.', answer: 'Q_j(s_t, a_t) <- Q_j(s_t, a_t) + alpha * [**r^t_j + gamma * Value_j(Gamma_{s_{t+1}}) - Q_j(s_t, a_t)**], where Value_j is the expected return for agent j under the equilibrium solution of the normal-form game Gamma at the next state s_{t+1}.', type: 'application', bloomLevel: 'apply', tags: ['td-update', 'jal-gt'], order: 4 },
  { id: 'rc-marl-6.2-5', lessonId: 'marl-6.2', prompt: 'JAL-GT algorithms view the set of joint-action values Q_1(s,.), ..., Q_n(s,.) as a _____ for state s, which is solved using game-theoretic solution concepts.', answer: 'JAL-GT algorithms view the set of joint-action values Q_1(s,.), ..., Q_n(s,.) as a **normal-form game** for state s, which is solved using game-theoretic solution concepts.', type: 'cloze', bloomLevel: 'remember', tags: ['jal-gt', 'state-game'], order: 5 },

  // --- Lesson 6.3: Minimax-Q, Nash-Q, Correlated-Q ---
  { id: 'rc-marl-6.3-1', lessonId: 'marl-6.3', prompt: 'What is Minimax Q-learning and when does it converge?', answer: 'Minimax Q-learning (Littman 1994) is a JAL-GT algorithm that solves Gamma_s via **minimax** (e.g., linear programming). It applies to **two-agent zero-sum stochastic games** and converges to the unique minimax value when all state-action pairs are visited infinitely often.', type: 'recall', bloomLevel: 'remember', tags: ['minimax-q', 'convergence'], order: 1 },
  { id: 'rc-marl-6.3-2', lessonId: 'marl-6.3', prompt: 'What restrictive assumptions does Nash Q-learning require to guarantee convergence?', answer: 'Nash Q-learning requires that all encountered normal-form games Gamma_s either **(a) all have a global optimum** or **(b) all have a saddle point**. These conditions ensure a unique equilibrium value, circumventing the equilibrium selection problem. Both conditions are unlikely to hold in general.', type: 'recall', bloomLevel: 'remember', tags: ['nash-q', 'convergence-assumptions'], order: 2 },
  { id: 'rc-marl-6.3-3', lessonId: 'marl-6.3', prompt: 'Compare the trade-offs between Minimax-Q, Nash-Q, and Correlated-Q learning.', answer: '**Minimax-Q**: strong convergence guarantees but limited to two-agent zero-sum games. **Nash-Q**: applies to general-sum games but requires very restrictive assumptions (global optima or saddle points). **Correlated-Q**: spans a wider solution space and uses efficient LP computation, but has **no known convergence guarantees** for stochastic games and faces a larger equilibrium selection problem.', type: 'concept', bloomLevel: 'understand', tags: ['minimax-q', 'nash-q', 'correlated-q'], order: 3 },
  { id: 'rc-marl-6.3-4', lessonId: 'marl-6.3', prompt: 'In Littman\'s soccer game experiment, minimax-Q won 37.5% against an optimal opponent while IQL won 0%. Why?', answer: 'Minimax-Q learns a **probabilistic (mixed) policy** robust against worst-case opponents, achieving close to the theoretical 50% minimax win rate. IQL learns a **deterministic policy** that can be fully predicted and exploited by an optimal opponent, resulting in 0% wins. The gap from 50% shows minimax-Q hadn\'t fully converged.', type: 'application', bloomLevel: 'apply', tags: ['minimax-q', 'soccer-game', 'robustness'], order: 4 },
  { id: 'rc-marl-6.3-5', lessonId: 'marl-6.3', prompt: 'Correlated Q-learning solves Gamma_s by computing a _____, which can be done efficiently via linear programming and spans a wider solution space than Nash equilibrium.', answer: 'Correlated Q-learning solves Gamma_s by computing a **correlated equilibrium**, which can be done efficiently via linear programming and spans a wider solution space than Nash equilibrium.', type: 'cloze', bloomLevel: 'remember', tags: ['correlated-q', 'correlated-equilibrium'], order: 5 },

  // --- Lesson 6.4: Agent modelling, fictitious play ---
  { id: 'rc-marl-6.4-1', lessonId: 'marl-6.4', prompt: 'What is fictitious play?', answer: 'Fictitious play (Brown 1951) is a learning algorithm for non-repeated normal-form games where each agent models others\' policies as the **empirical distribution of past actions**: pi_hat_j(a_j) = C(a_j) / sum C(a\'_j), then plays a **deterministic best response** against these models.', type: 'recall', bloomLevel: 'remember', tags: ['fictitious-play', 'agent-modeling'], order: 1 },
  { id: 'rc-marl-6.4-2', lessonId: 'marl-6.4', prompt: 'What convergence properties does fictitious play have?', answer: 'In fictitious play: (1) If actions converge, they form a **Nash equilibrium**. (2) Once actions form a NE, they **remain there**. (3) If empirical distributions converge, they converge to a **NE**. (4) Empirical distributions converge in several game classes, including **two-agent zero-sum games**.', type: 'recall', bloomLevel: 'remember', tags: ['fictitious-play', 'convergence'], order: 2 },
  { id: 'rc-marl-6.4-3', lessonId: 'marl-6.4', prompt: 'Why can\'t fictitious play directly learn probabilistic equilibria, and how does it still approximate them?', answer: 'Fictitious play uses **deterministic best-response actions** (argmax), so it cannot represent randomized policies. However, the **empirical distribution** of actions across episodes can converge to a probabilistic equilibrium -- e.g., in Rock-Paper-Scissors, the varying best responses produce a spiral whose time-averaged distribution approaches the uniform Nash equilibrium.', type: 'concept', bloomLevel: 'understand', tags: ['fictitious-play', 'probabilistic-equilibria'], order: 3 },
  { id: 'rc-marl-6.4-4', lessonId: 'marl-6.4', prompt: 'In fictitious play for Rock-Paper-Scissors, agent 1 models agent 2 as (0.33, 0.67, 0.00) for (R,P,S). What is agent 1\'s best response action?', answer: 'Agent 1 computes expected rewards for each action against the model: R: 0.33(0) + 0.67(-1) + 0(1) = **-0.67**, P: 0.33(1) + 0.67(0) + 0(-1) = **0.33**, S: 0.33(-1) + 0.67(1) + 0(0) = **0.33**. Actions P and S tie at 0.33, so agent 1 plays **P** (or S, with tie-breaking).', type: 'application', bloomLevel: 'apply', tags: ['fictitious-play', 'best-response'], order: 4 },
  { id: 'rc-marl-6.4-5', lessonId: 'marl-6.4', prompt: 'Policy reconstruction is the most common type of agent modeling in MARL, which aims to learn models pi_hat_j of other agents\' _____ based on observed past actions.', answer: 'Policy reconstruction is the most common type of agent modeling in MARL, which aims to learn models pi_hat_j of other agents\' **policies** based on observed past actions.', type: 'cloze', bloomLevel: 'remember', tags: ['agent-modeling', 'policy-reconstruction'], order: 5 },

  // --- Lesson 6.5: Joint-action learning with models ---
  { id: 'rc-marl-6.5-1', lessonId: 'marl-6.5', prompt: 'What is JAL-AM (Joint-Action Learning with Agent Modeling)?', answer: 'JAL-AM combines **joint-action value functions** Q_i(s, a) with **learned agent models** pi_hat_j(a_j|s) to compute **best-response actions** and TD update targets for the learning agent, extending fictitious play ideas to stochastic games.', type: 'recall', bloomLevel: 'remember', tags: ['jal-am', 'agent-modeling'], order: 1 },
  { id: 'rc-marl-6.5-2', lessonId: 'marl-6.5', prompt: 'How does JAL-AM compute the action value AV_i(s, a_i)?', answer: 'AV_i(s, a_i) = **sum_{a_{-i}} Q_i(s, <a_i, a_{-i}>) * product_{j!=i} pi_hat_j(a_j|s)**, which weights the joint-action values by the predicted probabilities of other agents\' actions from the learned models.', type: 'recall', bloomLevel: 'remember', tags: ['jal-am', 'action-value'], order: 2 },
  { id: 'rc-marl-6.5-3', lessonId: 'marl-6.5', prompt: 'How does JAL-AM differ from JAL-GT in terms of information requirements?', answer: 'JAL-GT requires observing **rewards of all agents** and maintaining Q-functions for every agent, using game-theoretic solutions for targets. JAL-AM only needs to observe **other agents\' actions** (not rewards) and maintains a **single Q-function** for the learning agent, using agent models and best responses instead.', type: 'concept', bloomLevel: 'understand', tags: ['jal-am', 'jal-gt', 'comparison'], order: 3 },
  { id: 'rc-marl-6.5-4', lessonId: 'marl-6.5', prompt: 'In the level-based foraging experiment, JAL-AM converged faster than both CQL and IQL. What explains this?', answer: 'JAL-AM\'s agent models **reduced variance in update targets** compared to IQL (which ignores others) and CQL (which must explore the full joint-action space). By predicting other agents\' likely actions, JAL-AM focused its exploration more efficiently, converging to the optimal joint policy in about **500,000 steps** vs. 600,000 for IQL.', type: 'application', bloomLevel: 'apply', tags: ['jal-am', 'level-based-foraging'], order: 4 },
  { id: 'rc-marl-6.5-5', lessonId: 'marl-6.5', prompt: 'In JAL-AM, the agent model pi_hat_j(a_j|s) = C(s, a_j) / sum C(s, a\'_j) is the _____ distribution of agent j\'s past actions in state s.', answer: 'In JAL-AM, the agent model pi_hat_j(a_j|s) = C(s, a_j) / sum C(s, a\'_j) is the **empirical** distribution of agent j\'s past actions in state s.', type: 'cloze', bloomLevel: 'remember', tags: ['jal-am', 'empirical-distribution'], order: 5 },

  // --- Lesson 6.6: Bayesian agent modelling, value of information ---
  { id: 'rc-marl-6.6-1', lessonId: 'marl-6.6', prompt: 'What is Bayesian agent modeling in MARL?', answer: 'Bayesian agent modeling maintains a **probability distribution (belief)** over a space of possible agent models Pi_hat_j, updated via **Bayes\' rule** after observing actions: Pr(pi_hat_j | h_{t+1}) proportional to pi_hat_j(a^t_j | h_t) * Pr(pi_hat_j | h_t).', type: 'recall', bloomLevel: 'remember', tags: ['bayesian-learning', 'agent-modeling'], order: 1 },
  { id: 'rc-marl-6.6-2', lessonId: 'marl-6.6', prompt: 'What is the value of information (VI) in MARL?', answer: 'VI evaluates how the outcomes of an action may **influence the learning agent\'s beliefs** about other agents, and how the changed beliefs will **influence future actions**. It also accounts for how the action may **change other agents\' behavior**. VI optimally trades off between exploration for accurate beliefs and potential costs.', type: 'recall', bloomLevel: 'remember', tags: ['value-of-information', 'exploration'], order: 2 },
  { id: 'rc-marl-6.6-3', lessonId: 'marl-6.6', prompt: 'In the Prisoner\'s Dilemma example with Coop and Grim models, why might VI recommend cooperating even though defecting gives information?', answer: 'Defecting reveals which model agent 2 uses with certainty. But if agent 2 is Grim, defecting triggers permanent defection, yielding low returns for the rest of the episode. VI evaluates this **risk-reward trade-off**: the cost of triggering Grim (return -27) outweighs the benefit of identifying Coop (return 0), making cooperation (expected VI = **-9**) better than defection (expected VI = **-13.5**).', type: 'concept', bloomLevel: 'understand', tags: ['value-of-information', 'prisoners-dilemma'], order: 3 },
  { id: 'rc-marl-6.6-4', lessonId: 'marl-6.6', prompt: 'Agent 1 has a Dirichlet belief with pseudocounts (2, 4, 6) over agent 2\'s R/P/S actions. What is the mean predicted policy?', answer: 'The Dirichlet mean is beta_k / sum_l(beta_l). Sum = 2+4+6 = 12. So the predicted policy is: P(R) = 2/12 = **1/6**, P(P) = 4/12 = **1/3**, P(S) = 6/12 = **1/2**. Agent 2 is predicted to most likely play Scissors.', type: 'application', bloomLevel: 'apply', tags: ['dirichlet', 'bayesian-learning'], order: 4 },
  { id: 'rc-marl-6.6-5', lessonId: 'marl-6.6', prompt: 'The key condition for convergence in rational learning (Bayesian game theory) is that any history with positive probability under agents\' actual policies must have positive probability under their _____.', answer: 'The key condition for convergence in rational learning (Bayesian game theory) is that any history with positive probability under agents\' actual policies must have positive probability under their **beliefs**.', type: 'cloze', bloomLevel: 'remember', tags: ['rational-learning', 'absolute-continuity'], order: 5 },

  // --- Lesson 6.7: Policy gradients for MARL, IGA ---
  { id: 'rc-marl-6.7-1', lessonId: 'marl-6.7', prompt: 'What is Infinitesimal Gradient Ascent (IGA) for MARL?', answer: 'IGA is a policy-based learning method for **two-agent, two-action normal-form games** where each agent updates its policy parameter in the direction of the **gradient of its expected reward**: alpha_{k+1} = alpha_k + kappa * dU_i/dalpha. With infinitesimal step size (kappa -> 0), the joint policy follows a continuous trajectory.', type: 'recall', bloomLevel: 'remember', tags: ['iga', 'policy-gradient'], order: 1 },
  { id: 'rc-marl-6.7-2', lessonId: 'marl-6.7', prompt: 'What are the three types of trajectories that IGA can produce?', answer: 'Based on the matrix F properties: (1) **Divergent** if F is not invertible (u or u\' = 0). (2) **Divergent to/from center** if F has purely real eigenvalues (uu\' > 0, possible in common-reward/general-sum). (3) **Elliptical orbits** around center if F has purely imaginary eigenvalues (uu\' < 0, possible in zero-sum/general-sum).', type: 'recall', bloomLevel: 'remember', tags: ['iga', 'learning-dynamics'], order: 2 },
  { id: 'rc-marl-6.7-3', lessonId: 'marl-6.7', prompt: 'Why does IGA not always converge to a Nash equilibrium, and what weaker guarantee does it provide?', answer: 'When the matrix F has **purely imaginary eigenvalues**, the joint policy follows **elliptical orbits** around the center point (Nash equilibrium) and never converges. However, Singh et al. (2000) proved that even in this case, the **average rewards** received during learning converge to the expected rewards of a Nash equilibrium -- a weaker but still meaningful convergence guarantee.', type: 'concept', bloomLevel: 'understand', tags: ['iga', 'convergence', 'average-rewards'], order: 3 },
  { id: 'rc-marl-6.7-4', lessonId: 'marl-6.7', prompt: 'For a 2x2 game with u = r_{1,1} - r_{1,2} - r_{2,1} + r_{2,2} and u\' = c_{1,1} - c_{1,2} - c_{2,1} + c_{2,2}, if u = 2 and u\' = -3, what type of IGA trajectory results?', answer: 'Compute uu\' = 2 * (-3) = **-6 < 0**, so F has **purely imaginary eigenvalues** (lambda^2 = uu\' = -6). The joint policy will follow **elliptical orbits** around the center point. This can occur in zero-sum and general-sum games.', type: 'application', bloomLevel: 'apply', tags: ['iga', 'eigenvalues', 'trajectory'], order: 4 },
  { id: 'rc-marl-6.7-5', lessonId: 'marl-6.7', prompt: 'IGA requires each agent to know its own _____ and the _____ of the other agent in the current episode.', answer: 'IGA requires each agent to know its own **reward matrix** and the **policy** of the other agent in the current episode.', type: 'cloze', bloomLevel: 'remember', tags: ['iga', 'knowledge-requirements'], order: 5 },

  // --- Lesson 6.8: WoLF principle, WoLF-IGA, WoLF-PHC ---
  { id: 'rc-marl-6.8-1', lessonId: 'marl-6.8', prompt: 'What is the WoLF (Win or Learn Fast) principle?', answer: 'WoLF uses **variable learning rates**: learn quickly (use l_max) when **losing** and learn slowly (use l_min) when **winning**. Winning/losing is determined by comparing the agent\'s current expected reward to the expected reward under a Nash equilibrium policy (WoLF-IGA) or an average policy (WoLF-PHC).', type: 'recall', bloomLevel: 'remember', tags: ['wolf', 'variable-learning-rate'], order: 1 },
  { id: 'rc-marl-6.8-2', lessonId: 'marl-6.8', prompt: 'What convergence guarantee does WoLF-IGA achieve?', answer: 'WoLF-IGA is **guaranteed to converge to a Nash equilibrium** in general-sum games with two agents and two actions. Unlike basic IGA which may orbit indefinitely, the variable learning rate causes trajectories to **spiral inward** (tightening by factor sqrt(l_min/l_max) per quadrant) toward the center point.', type: 'recall', bloomLevel: 'remember', tags: ['wolf-iga', 'convergence'], order: 2 },
  { id: 'rc-marl-6.8-3', lessonId: 'marl-6.8', prompt: 'How does WoLF-PHC improve upon both IGA and WoLF-IGA?', answer: 'WoLF-PHC works in **general-sum stochastic games** with any finite number of agents and actions (IGA/WoLF-IGA are limited to 2 agents, 2 actions). It does **not require knowledge** of reward functions or other agents\' policies. Instead, it learns Q-values via standard Q-learning and compares against an **average policy** (replacing the unknown Nash policy).', type: 'concept', bloomLevel: 'understand', tags: ['wolf-phc', 'improvements'], order: 3 },
  { id: 'rc-marl-6.8-4', lessonId: 'marl-6.8', prompt: 'In WoLF-PHC, if the current policy\'s expected Q-value exceeds the average policy\'s expected Q-value, which learning rate is used and why?', answer: 'The agent uses the **smaller learning rate l_w** (winning rate) because the agent is "winning" -- its current policy outperforms the average. It should adapt **slowly** since the other agent is likely to change its policy in response, and a rapid change might overshoot the equilibrium.', type: 'application', bloomLevel: 'apply', tags: ['wolf-phc', 'learning-rate'], order: 4 },
  { id: 'rc-marl-6.8-5', lessonId: 'marl-6.8', prompt: 'In WoLF-IGA, trajectories in the problematic case (imaginary eigenvalues, center in unit square) become piecewise _____, spiraling toward the Nash equilibrium.', answer: 'In WoLF-IGA, trajectories in the problematic case (imaginary eigenvalues, center in unit square) become piecewise **elliptical**, spiraling toward the Nash equilibrium.', type: 'cloze', bloomLevel: 'remember', tags: ['wolf-iga', 'trajectories'], order: 5 },

  // --- Lesson 6.9: Regret matching, CFR, poker solving ---
  { id: 'rc-marl-6.9-1', lessonId: 'marl-6.9', prompt: 'How does unconditional regret matching compute action probabilities?', answer: 'Action probabilities are proportional to the **positive average unconditional regrets**: pi^{z+1}_i(a_i) = [R_bar^z_i(a_i)]+ / sum_{a\'_i} [R_bar^z_i(a\'_i)]+, where R_bar^z_i(a_i) = (1/z) * sum_e [R_i(a_i, a^e_{-i}) - R_i(a^e)]. If all regrets are non-positive, any distribution may be used.', type: 'recall', bloomLevel: 'remember', tags: ['regret-matching', 'unconditional'], order: 1 },
  { id: 'rc-marl-6.9-2', lessonId: 'marl-6.9', prompt: 'What is the difference between unconditional and conditional regret matching?', answer: '**Unconditional** regret compares replacing all past actions with a single alternative action. **Conditional** regret replaces only specific occurrences of action a\'_i with a_i: Regret^z_i(a\'_i, a_i) = sum_{e: a^e_i = a\'_i} [R_i(a_i, a^e_{-i}) - R_i(a^e)]. Conditional regret matching converges to **correlated equilibria**; unconditional to **coarse correlated equilibria**.', type: 'recall', bloomLevel: 'remember', tags: ['regret-matching', 'conditional-vs-unconditional'], order: 2 },
  { id: 'rc-marl-6.9-3', lessonId: 'marl-6.9', prompt: 'Why do the actual policies in regret matching not converge, yet the empirical distributions do?', answer: 'Regret matching causes **chaotic policy oscillations** as agents mutually adapt -- each agent shifts probability to high-regret actions, changing others\' regrets in response. However, these oscillations are **self-correcting on average**: the regret bound (proportional to 1/sqrt(z)) ensures average regrets converge to zero, which by Blackwell\'s Approachability Theorem implies the **empirical distribution converges** to equilibrium.', type: 'concept', bloomLevel: 'understand', tags: ['regret-matching', 'convergence-behavior'], order: 3 },
  { id: 'rc-marl-6.9-4', lessonId: 'marl-6.9', prompt: 'In Prisoner\'s Dilemma, D is a dominant action. What happens when agents use unconditional regret matching?', answer: 'Since D is a best response against both C and D, the unconditional regret for action C is **always non-positive** (playing C instead of D never helps). So [R_bar(C)]+ = 0 in all episodes, and the algorithm assigns **probability 1 to D** from the second episode onward, quickly converging to the unique NE (D, D).', type: 'application', bloomLevel: 'apply', tags: ['regret-matching', 'prisoners-dilemma'], order: 4 },
  { id: 'rc-marl-6.9-5', lessonId: 'marl-6.9', prompt: 'The regret bound for regret matching is proportional to kappa * 1/sqrt(z), so the average regret goes to zero because z grows faster than _____.', answer: 'The regret bound for regret matching is proportional to kappa * 1/sqrt(z), so the average regret goes to zero because z grows faster than **sqrt(z)**.', type: 'cloze', bloomLevel: 'remember', tags: ['regret-matching', 'regret-bound'], order: 5 },

  // --- Lesson 6.10: Algorithm taxonomy, convergence landscape ---
  { id: 'rc-marl-6.10-1', lessonId: 'marl-6.10', prompt: 'Name the four main families of foundational MARL algorithms covered in Chapter 6.', answer: 'The four families are: (1) **Joint-action learning** (JAL-GT: Minimax-Q, Nash-Q, Correlated-Q), (2) **Agent modeling** (fictitious play, JAL-AM, Bayesian learning), (3) **Policy-based learning** (IGA, WoLF-IGA, WoLF-PHC, GIGA), and (4) **No-regret learning** (unconditional/conditional regret matching).', type: 'recall', bloomLevel: 'remember', tags: ['algorithm-taxonomy', 'foundational-algorithms'], order: 1 },
  { id: 'rc-marl-6.10-2', lessonId: 'marl-6.10', prompt: 'Which foundational MARL algorithms achieve the strongest form of convergence (policy convergence to NE)?', answer: '**WoLF-IGA** provably converges to a Nash equilibrium in two-agent two-action general-sum games. **Minimax Q-learning** converges to minimax values in two-agent zero-sum stochastic games. **WoLF-PHC** empirically converges to NE in many settings but has limited formal guarantees.', type: 'recall', bloomLevel: 'remember', tags: ['convergence-landscape', 'policy-convergence'], order: 2 },
  { id: 'rc-marl-6.10-3', lessonId: 'marl-6.10', prompt: 'Summarize the convergence landscape: which algorithm families converge to which solution concepts?', answer: '**JAL-GT (Minimax-Q)**: minimax solutions in zero-sum games. **Fictitious play**: NE (empirical distribution) in zero-sum and some other game classes. **IGA**: NE average rewards. **WoLF-IGA**: NE (policy convergence, 2x2 games). **GIGA**: no-regret (empirical -> CCE). **Unconditional regret matching**: empirical -> coarse correlated equilibria. **Conditional regret matching**: empirical -> correlated equilibria.', type: 'concept', bloomLevel: 'understand', tags: ['convergence-landscape', 'solution-concepts'], order: 3 },
  { id: 'rc-marl-6.10-4', lessonId: 'marl-6.10', prompt: 'An engineer must choose a MARL algorithm for a two-agent zero-sum game requiring robust play. Which foundational algorithm is most appropriate and why?', answer: '**Minimax Q-learning** is most appropriate. It has proven convergence to the unique **minimax value** in zero-sum stochastic games, learns policies robust against worst-case opponents, and benefits from the **uniqueness** of minimax values which eliminates the equilibrium selection problem. The trade-off is it won\'t exploit suboptimal opponents.', type: 'application', bloomLevel: 'apply', tags: ['algorithm-selection', 'minimax-q'], order: 4 },
  { id: 'rc-marl-6.10-5', lessonId: 'marl-6.10', prompt: 'Value-based MARL methods estimate action or joint-action values and derive policies from them, while policy-based methods directly optimize _____ using gradient-ascent techniques.', answer: 'Value-based MARL methods estimate action or joint-action values and derive policies from them, while policy-based methods directly optimize **parameterized policy functions** using gradient-ascent techniques.', type: 'cloze', bloomLevel: 'remember', tags: ['value-based', 'policy-based'], order: 5 },
  // ============================================================
  // MODULE 7: Deep Learning (Lessons 7.1 - 7.9)
  // ============================================================

  // --- Lesson 7.1: Curse of dimensionality, function approximation, deadly triad ---
  { id: 'rc-marl-7.1-1', lessonId: 'marl-7.1', prompt: 'What are the two key limitations of tabular value functions in MARL?', answer: 'First, the table grows linearly with the number of possible inputs, making it infeasible for complex problems (e.g., Go has ~10^170 states). Second, tabular methods update each value estimate in isolation, so an agent must visit a state before it can learn its value.', type: 'recall', bloomLevel: 'remember', tags: ['tabular-limitations', 'curse-of-dimensionality'], order: 1 },
  { id: 'rc-marl-7.1-2', lessonId: 'marl-7.1', prompt: 'What three components constitute the "deadly triad" that can lead to diverging value estimates in RL?', answer: 'The deadly triad consists of (1) function approximation, (2) bootstrapped target values, and (3) off-policy learning. When all three are present together, value estimates can become unstable and diverge.', type: 'recall', bloomLevel: 'remember', tags: ['deadly-triad', 'stability'], order: 2 },
  { id: 'rc-marl-7.1-3', lessonId: 'marl-7.1', prompt: 'Why does function approximation enable generalization in RL, and why is this important?', answer: 'Function approximation uses shared parameters to compute values for all states, so updating for one state also affects similar states. This is important because agents can provide reasonable value estimates for states they have never visited, which is essential in environments with large or continuous state spaces where visiting every state is infeasible.', type: 'concept', bloomLevel: 'understand', tags: ['generalization', 'function-approximation'], order: 3 },
  { id: 'rc-marl-7.1-4', lessonId: 'marl-7.1', prompt: 'Given a maze where states s1 and s3 have been visited and have similar path lengths to the goal as unvisited states s2 and s4, explain how function approximation would handle value estimation differently than tabular methods.', answer: 'A tabular method would have no value estimates for s2 and s4 since they were never visited. Function approximation, by learning parameters that encode the relationship between path length and value, can generalize and estimate values for s2 and s4 to be similar to s1 and s3 respectively, without requiring explicit visits.', type: 'application', bloomLevel: 'apply', tags: ['generalization', 'maze-example'], order: 4 },
  { id: 'rc-marl-7.1-5', lessonId: 'marl-7.1', prompt: 'Linear function approximation represents the value function as V(s; theta) = _____, where theta and x(s) are the parameter and state feature vectors respectively.', answer: 'V(s; theta) = theta^T x(s), a dot product (linear combination) of the parameter vector and the state feature vector.', type: 'cloze', bloomLevel: 'remember', tags: ['linear-function-approximation'], order: 5 },

  // --- Lesson 7.2: Feedforward networks, activations, universal approximation ---
  { id: 'rc-marl-7.2-1', lessonId: 'marl-7.2', prompt: 'What does the universal approximation theorem state about feedforward neural networks?', answer: 'It states that feedforward neural networks with as few as a single hidden layer can approximate any continuous function on a closed and bounded subset of real-valued vectors, given sufficient hidden units.', type: 'recall', bloomLevel: 'remember', tags: ['universal-approximation', 'feedforward'], order: 1 },
  { id: 'rc-marl-7.2-2', lessonId: 'marl-7.2', prompt: 'What are the three alternative names for feedforward neural networks?', answer: 'Feedforward neural networks are also called deep feedforward networks, fully connected neural networks, or multi-layer perceptrons (MLPs).', type: 'recall', bloomLevel: 'remember', tags: ['feedforward', 'MLP', 'terminology'], order: 2 },
  { id: 'rc-marl-7.2-3', lessonId: 'marl-7.2', prompt: 'Why are non-linear activation functions essential in neural networks?', answer: 'Without non-linear activation functions, the composition of any number of neural units would only produce a linear function, since the composition of two linear functions is itself linear. Non-linear activations allow networks to represent complex, non-linear functions that cannot be captured by linear function approximation.', type: 'concept', bloomLevel: 'understand', tags: ['activation-functions', 'non-linearity'], order: 3 },
  { id: 'rc-marl-7.2-4', lessonId: 'marl-7.2', prompt: 'You need a neural network output restricted to the range (0, 1) for a probability estimate. Which activation function should you apply to the output layer, and why?', answer: 'Apply the sigmoid activation function, which maps any real-valued input to the range (0, 1). The sigmoid function is defined as 1/(1 + e^(-x)) and is commonly used when constraining outputs to this interval.', type: 'application', bloomLevel: 'apply', tags: ['sigmoid', 'activation-functions'], order: 4 },
  { id: 'rc-marl-7.2-5', lessonId: 'marl-7.2', prompt: 'A single neural unit computes f(x; w, b) = g(_____ + b), where g is the activation function, w is the weight vector, and x is the input.', answer: 'f(x; w, b) = g(w^T x + b), where w^T x is the weighted sum (dot product) of the weight vector and input.', type: 'cloze', bloomLevel: 'remember', tags: ['neural-unit', 'feedforward'], order: 5 },

  // --- Lesson 7.3: Loss functions, SGD/Adam, backpropagation ---
  { id: 'rc-marl-7.3-1', lessonId: 'marl-7.3', prompt: 'What is the mean squared error (MSE) loss function used for training a state-value function?', answer: 'L(theta) = (1/B) * sum_{k=1}^{B} (V^pi(s_k) - V_hat(s_k; theta))^2, which computes the average squared difference between the true state values and the approximate value function over a batch of B states.', type: 'recall', bloomLevel: 'remember', tags: ['MSE', 'loss-function'], order: 1 },
  { id: 'rc-marl-7.3-2', lessonId: 'marl-7.3', prompt: 'What gradient-based optimizer has emerged as the most common default choice in deep learning?', answer: 'The Adam optimizer (Kingma and Ba, 2015) has emerged as the most common default optimizer in deep learning, applying adaptive learning rate schemes.', type: 'recall', bloomLevel: 'remember', tags: ['Adam', 'optimizer'], order: 2 },
  { id: 'rc-marl-7.3-3', lessonId: 'marl-7.3', prompt: 'Explain the trade-off between batch size and gradient quality in mini-batch gradient descent.', answer: 'Smaller batch sizes approach SGD with fast but high-variance gradients. Larger batch sizes approach vanilla gradient descent with slower but lower-variance and more stable gradients. Common batch sizes (32-1028) balance computational cost against gradient stability. The optimal batch size depends on available data, computational resources, network architecture, and loss function.', type: 'concept', bloomLevel: 'understand', tags: ['mini-batch', 'batch-size', 'gradient-descent'], order: 3 },
  { id: 'rc-marl-7.3-4', lessonId: 'marl-7.3', prompt: 'You are training a neural network value function and notice the optimization is slow but very stable. You are using vanilla gradient descent. What change would you make and why?', answer: 'Switch to mini-batch gradient descent with a moderate batch size (e.g., 32 or 128). This computes gradients over a subset of data rather than the entire dataset, making each update much faster while still maintaining reasonable gradient stability. You could also add momentum to accelerate convergence in consistent gradient directions.', type: 'application', bloomLevel: 'apply', tags: ['mini-batch', 'optimization'], order: 4 },
  { id: 'rc-marl-7.3-5', lessonId: 'marl-7.3', prompt: 'The backpropagation algorithm efficiently computes gradients by applying the _____ of derivatives, traversing the network from the output layer to the input layer.', answer: 'The chain rule of derivatives. Backpropagation applies it iteratively to propagate gradients backwards through the network layers.', type: 'cloze', bloomLevel: 'remember', tags: ['backpropagation', 'chain-rule'], order: 5 },

  // --- Lesson 7.4: CNNs, RNNs/LSTMs, when to use which ---
  { id: 'rc-marl-7.4-1', lessonId: 'marl-7.4', prompt: 'How do convolutional neural networks (CNNs) achieve parameter efficiency compared to feedforward networks for image processing?', answer: 'CNNs use small filters/kernels that are shared across the entire image. The same filter parameters are reused at every spatial location via the convolution operation. For example, a CNN with 16 filters of 5x5 for a 3-channel image has only 1,216 parameters, while a feedforward network with 128 hidden units would need over 6 million parameters for a 128x128 RGB image.', type: 'recall', bloomLevel: 'remember', tags: ['CNN', 'parameter-sharing'], order: 1 },
  { id: 'rc-marl-7.4-2', lessonId: 'marl-7.4', prompt: 'What is the main challenge when training recurrent neural networks (RNNs) over long sequences?', answer: 'Gradients often vanish (become close to zero) or explode (become very large) due to repeated multiplication of gradients and Jacobians through the backpropagation over the sequence. LSTMs and GRUs address this by allowing the network to decide when to remember or discard accumulated information.', type: 'recall', bloomLevel: 'remember', tags: ['RNN', 'vanishing-gradients', 'LSTM'], order: 2 },
  { id: 'rc-marl-7.4-3', lessonId: 'marl-7.4', prompt: 'Why are CNNs better suited for image inputs than feedforward networks, beyond just parameter efficiency?', answer: 'CNNs exploit spatial relationships in images by processing patches of nearby pixels together through convolution operations. Nearby pixels are often correlated (belonging to similar objects), and CNNs encode this inductive bias. Additionally, pooling operations reduce dimensionality and make the output insensitive to small local translations, helping the network learn more generalizable features.', type: 'concept', bloomLevel: 'understand', tags: ['CNN', 'spatial-structure', 'inductive-bias'], order: 3 },
  { id: 'rc-marl-7.4-4', lessonId: 'marl-7.4', prompt: 'You are designing a deep RL agent for a partially observable environment where the agent receives a sequence of observations. What network architecture should you use for the policy, and why?', answer: 'Use a recurrent neural network (RNN), specifically an LSTM or GRU. In partially observable environments, the agent should condition its policy on its history of observations. An RNN can process observations sequentially, maintaining a hidden state as a compact representation of the full observation history, enabling the agent to use past information for decision-making.', type: 'application', bloomLevel: 'apply', tags: ['RNN', 'LSTM', 'partial-observability'], order: 4 },
  { id: 'rc-marl-7.4-5', lessonId: 'marl-7.4', prompt: 'In a recurrent neural network, the hidden state at time t is computed as h_t = f(x_t, _____; theta), and is initialized as a _____ vector.', answer: 'h_t = f(x_t, h_{t-1}; theta), where h_{t-1} is the previous hidden state. The initial hidden state h_0 is usually initialized as a zero-valued vector.', type: 'cloze', bloomLevel: 'remember', tags: ['RNN', 'hidden-state'], order: 5 },

  // --- Lesson 7.5: DQN, experience replay, target networks ---
  { id: 'rc-marl-7.5-1', lessonId: 'marl-7.5', prompt: 'What are the two key innovations that DQN adds to deep Q-learning to stabilize training?', answer: 'DQN adds (1) a target network -- a separate copy of the value network whose parameters are periodically updated, used to compute stable bootstrapped target values; and (2) an experience replay buffer -- a memory that stores past transitions from which random mini-batches are sampled for training, breaking temporal correlations between consecutive experiences.', type: 'recall', bloomLevel: 'remember', tags: ['DQN', 'target-network', 'replay-buffer'], order: 1 },
  { id: 'rc-marl-7.5-2', lessonId: 'marl-7.5', prompt: 'How does Double DQN (DDQN) address the overestimation problem of DQN?', answer: 'DDQN decouples action selection from value estimation. The main value network selects the greedy action in the next state (argmax_a Q(s_{t+1}, a; theta)), but the target network evaluates the value of that action (Q(s_{t+1}, a*; theta_bar)). This reduces overestimation since both networks are unlikely to overestimate the same action.', type: 'recall', bloomLevel: 'remember', tags: ['DDQN', 'overestimation'], order: 2 },
  { id: 'rc-marl-7.5-3', lessonId: 'marl-7.5', prompt: 'Why can experience replay only be used with off-policy algorithms and not with on-policy algorithms?', answer: 'The replay buffer contains experiences generated by older versions of the agent policy, making the stored data off-policy. On-policy algorithms like REINFORCE require data generated by the current policy to compute valid policy gradients. Using off-policy data from the replay buffer would violate the assumptions of the policy gradient theorem and produce incorrect gradient estimates.', type: 'concept', bloomLevel: 'understand', tags: ['replay-buffer', 'off-policy', 'on-policy'], order: 3 },
  { id: 'rc-marl-7.5-4', lessonId: 'marl-7.5', prompt: 'Your deep Q-learning agent shows unstable training: sometimes learning well, sometimes collapsing. Value estimates seem to oscillate. You are not using a target network. Explain the issue and the solution.', answer: 'The moving target problem: each update to theta changes all value estimates including the bootstrapped targets, creating a feedback loop of instability. Add a target network with parameters theta_bar that are only periodically copied from the main network. Target values remain stable between updates, reducing oscillations and providing consistent optimization targets.', type: 'application', bloomLevel: 'apply', tags: ['target-network', 'moving-target'], order: 4 },
  { id: 'rc-marl-7.5-5', lessonId: 'marl-7.5', prompt: 'In DQN, the loss function is L(theta) = (1/B) * sum_{k=1}^{B} (y_k - Q(s_k, a_k; theta))^2, where the target for non-terminal states is y_k = r_k + gamma * _____.', answer: 'y_k = r_k + gamma * max_{a\'} Q(s_{k+1}, a\'; theta_bar), where theta_bar are the parameters of the target network.', type: 'cloze', bloomLevel: 'remember', tags: ['DQN', 'target-computation'], order: 5 },

  // --- Lesson 7.6: Policy gradient theorem, REINFORCE, baselines ---
  { id: 'rc-marl-7.6-1', lessonId: 'marl-7.6', prompt: 'State the policy gradient theorem in its expectation form.', answer: 'The policy gradient is: nabla_phi J(phi) = E_{s~Pr(.|pi), a~pi(.|s;phi)} [Q^pi(s,a) * nabla_phi log pi(a|s;phi)]. It expresses the gradient of policy performance as the expected product of the action-value function and the gradient of the log-probability of the selected action.', type: 'recall', bloomLevel: 'remember', tags: ['policy-gradient-theorem'], order: 1 },
  { id: 'rc-marl-7.6-2', lessonId: 'marl-7.6', prompt: 'In the REINFORCE algorithm, what are the Monte Carlo return estimates used for, and what is their main drawback?', answer: 'Monte Carlo return estimates are used to approximate the expected returns Q^pi(s,a) in the policy gradient theorem, computed as the discounted cumulative sum of rewards from time step t until episode end. Their main drawback is high variance, because returns depend on the entire subsequent trajectory including stochastic transitions and actions, leading to unstable training.', type: 'recall', bloomLevel: 'remember', tags: ['REINFORCE', 'Monte-Carlo'], order: 2 },
  { id: 'rc-marl-7.6-3', lessonId: 'marl-7.6', prompt: 'Why does subtracting a state-dependent baseline b(s) from the returns in the policy gradient not change the expected gradient?', answer: 'Because the baseline term reduces to sum_s Pr(s|pi) * b(s) * nabla_phi sum_a pi(a|s;phi). Since probabilities sum to 1 (sum_a pi(a|s;phi) = 1), its gradient is nabla_phi 1 = 0. The entire baseline term thus equals zero in expectation, leaving the policy gradient unchanged while reducing gradient variance.', type: 'concept', bloomLevel: 'understand', tags: ['baseline', 'variance-reduction'], order: 3 },
  { id: 'rc-marl-7.6-4', lessonId: 'marl-7.6', prompt: 'Your REINFORCE agent learns but with very high variance across episodes. Propose a concrete improvement using a baseline and describe the training process.', answer: 'Add a state-value function V(s; theta) trained to approximate expected returns via MSE loss: L(theta) = (1/T) sum_t (u_t - V(s_t; theta))^2. Replace the raw returns in the REINFORCE loss with (u_t - V(s_t; theta)) * log pi(a_t|s_t; phi). The baseline V(s) centers the returns around zero, reducing variance without changing the expected gradient direction.', type: 'application', bloomLevel: 'apply', tags: ['REINFORCE', 'baseline', 'variance-reduction'], order: 4 },
  { id: 'rc-marl-7.6-5', lessonId: 'marl-7.6', prompt: 'The REINFORCE loss is L(phi) = -(1/T) * sum_{t=0}^{T-1} _____ * log pi(a_t | s_t; phi), where the blank represents the discounted episodic return from time step t.', answer: 'The discounted episodic return: sum_{tau=t}^{T-1} gamma^{tau-t} * r_tau, which is the Monte Carlo estimate of Q^pi(s_t, a_t).', type: 'cloze', bloomLevel: 'remember', tags: ['REINFORCE', 'loss-function'], order: 5 },

  // --- Lesson 7.7: Actor-critic, A2C advantage, PPO clipping ---
  { id: 'rc-marl-7.7-1', lessonId: 'marl-7.7', prompt: 'What is the advantage function in A2C and how is it estimated using only a state-value function?', answer: 'The advantage is Adv(s,a) = Q^pi(s,a) - V^pi(s), representing how much better action a is compared to following the policy. It is estimated as Adv(s_t, a_t) = r_t + gamma*V(s_{t+1}; theta) - V(s_t; theta) for non-terminal states, using bootstrapped one-step returns to approximate Q without needing a separate action-value network.', type: 'recall', bloomLevel: 'remember', tags: ['A2C', 'advantage'], order: 1 },
  { id: 'rc-marl-7.7-2', lessonId: 'marl-7.7', prompt: 'How does PPO prevent large policy updates?', answer: 'PPO uses clipped importance sampling weights in its actor loss: L(phi) = -min(rho*Adv, clip(rho, 1-epsilon, 1+epsilon)*Adv), where rho = pi(a|s;phi) / pi_beta(a|s) is the ratio of new to old action probabilities. The clipping restricts the effective policy change, preventing large updates that could degrade performance. The hyperparameter epsilon (typically 0.2) controls the allowed deviation.', type: 'recall', bloomLevel: 'remember', tags: ['PPO', 'clipping', 'trust-region'], order: 2 },
  { id: 'rc-marl-7.7-3', lessonId: 'marl-7.7', prompt: 'What are the two main benefits of using bootstrapped return estimates (as in actor-critic methods) over Monte Carlo estimates (as in REINFORCE)?', answer: 'First, bootstrapped estimates allow updating the policy after every time step rather than waiting until episode end, enabling more frequent updates and more efficient training in long episodes. Second, bootstrapped estimates have lower variance than Monte Carlo returns because they only depend on the immediate reward and next-state value, not the entire episode trajectory. However, this comes at the cost of introduced bias from imperfect value function estimates.', type: 'concept', bloomLevel: 'understand', tags: ['actor-critic', 'bootstrapping', 'bias-variance'], order: 3 },
  { id: 'rc-marl-7.7-4', lessonId: 'marl-7.7', prompt: 'You are training PPO and notice the agent keeps making large policy changes that hurt performance. The importance sampling ratio frequently exceeds 3.0. What hyperparameter should you adjust and how?', answer: 'Reduce the clipping parameter epsilon (e.g., from 0.2 to 0.1). This tightens the trust region by clipping the importance sampling ratio closer to 1.0, preventing the policy from changing too drastically in a single update. You could also reduce the number of PPO epochs (N_e) per batch of data to limit how far the policy moves from the behavior policy.', type: 'application', bloomLevel: 'apply', tags: ['PPO', 'hyperparameters', 'clipping'], order: 4 },
  { id: 'rc-marl-7.7-5', lessonId: 'marl-7.7', prompt: 'In actor-critic methods, the policy network is called the _____ and the value function network is called the _____.', answer: 'The policy network is called the actor and the value function network is called the critic.', type: 'cloze', bloomLevel: 'remember', tags: ['actor-critic', 'terminology'], order: 5 },

  // --- Lesson 7.8: Hyperparameters, normalisation, debugging ---
  { id: 'rc-marl-7.8-1', lessonId: 'marl-7.8', prompt: 'What is entropy regularization in policy gradient methods, and what problem does it address?', answer: 'Entropy regularization adds the negative entropy of the policy -H(pi(.|s;phi)) = sum_a pi(a|s;phi) * log pi(a|s;phi) to the actor loss. Maximizing entropy penalizes the policy for assigning very high probability to any single action, preventing premature convergence to a suboptimal near-deterministic policy and thereby encouraging exploration.', type: 'recall', bloomLevel: 'remember', tags: ['entropy-regularization', 'exploration'], order: 1 },
  { id: 'rc-marl-7.8-2', lessonId: 'marl-7.8', prompt: 'What are N-step return estimates and what trade-off do they offer?', answer: 'N-step returns aggregate rewards over N consecutive steps before bootstrapping a value estimate: sum_{tau=0}^{N-1} gamma^tau * R(s_{t+tau}) + gamma^N * V(s_{t+N}). For N=1, you get low variance but high bias (pure bootstrapping). For N=T (episode length), you get high variance but no bias (Monte Carlo). Values like N=5 or N=10 provide a practical trade-off between bias and variance.', type: 'recall', bloomLevel: 'remember', tags: ['N-step-returns', 'bias-variance'], order: 2 },
  { id: 'rc-marl-7.8-3', lessonId: 'marl-7.8', prompt: 'Why can reward standardization distort the goals of an RL agent, despite generally improving training?', answer: 'Standardization shifts the mean of rewards to zero. In environments with only negative rewards, this makes some rewards appear positive, potentially changing the agent behavior. For example, if the agent should end episodes quickly (all rewards are negative), standardized rewards might make it prefer staying in the environment longer because some standardized rewards become positive. The preference ordering is preserved, but the implicit goal (episode length preference) can change.', type: 'concept', bloomLevel: 'understand', tags: ['reward-standardization', 'training-tips'], order: 3 },
  { id: 'rc-marl-7.8-4', lessonId: 'marl-7.8', prompt: 'You are training a MARL algorithm and notice that one agent network converges faster than the others, and the combined SGD optimization is slow due to separate optimizer instances per agent. Propose a practical solution.', answer: 'Use a single centralized optimizer that encompasses all trainable parameters from all agent networks. Combine all parameters into one list and sum all agent losses into a single loss before calling backward() and step(). This enables parallel gradient computation and is significantly faster than maintaining separate optimizers, while the individual losses still correctly update each agent network through their respective gradients.', type: 'application', bloomLevel: 'apply', tags: ['centralized-optimization', 'training-tips'], order: 4 },
  { id: 'rc-marl-7.8-5', lessonId: 'marl-7.8', prompt: 'When using synchronous parallel environments for on-policy training, the agent interacts with K environment instances simultaneously. Increasing K provides _____ batches for more stable gradients but increases _____ time as threads wait for others.', answer: 'Increasing K provides larger batches for more stable gradients but increases idle time as threads wait for the slowest environment instance to finish its transition.', type: 'cloze', bloomLevel: 'remember', tags: ['synchronous-environments', 'parallelization'], order: 5 },

  // --- Lesson 7.9: State vs observation vs history, partial observability ---
  { id: 'rc-marl-7.9-1', lessonId: 'marl-7.9', prompt: 'In a partially observable environment, what should policies and value functions be conditioned on instead of the state?', answer: 'They should be conditioned on the episodic history of observations h_t = (o_0, o_1, ..., o_t), since the agent does not observe the full state and must use all past observations to make informed decisions.', type: 'recall', bloomLevel: 'remember', tags: ['partial-observability', 'observation-history'], order: 1 },
  { id: 'rc-marl-7.9-2', lessonId: 'marl-7.9', prompt: 'What are the three practical options for handling observation histories as neural network inputs in partially observable environments?', answer: '(1) Stack a small fixed number of recent observations (e.g., o_{t-5:t}). (2) Use a recurrent neural network (LSTM or GRU) that maintains a hidden state representing the history. (3) Ignore previous observations and assume the current observation o_t carries all needed information. The best choice depends on the environment and how important past information is.', type: 'recall', bloomLevel: 'remember', tags: ['partial-observability', 'practical-approaches'], order: 2 },
  { id: 'rc-marl-7.9-3', lessonId: 'marl-7.9', prompt: 'Explain why simply concatenating the full observation history into a vector is impractical for feedforward neural networks in partially observable environments.', answer: 'Feedforward networks require a fixed input dimensionality, but the observation history grows over time. Even with zero-padding to a maximum episode length, the resulting input vector would be very high-dimensional and mostly sparse (filled with zeros), making it extremely difficult for the network to learn useful representations. Recurrent networks solve this by processing one observation at a time and maintaining a compact hidden state.', type: 'concept', bloomLevel: 'understand', tags: ['partial-observability', 'RNN-motivation'], order: 3 },
  { id: 'rc-marl-7.9-4', lessonId: 'marl-7.9', prompt: 'You are building an RL agent for a navigation task in a maze where the agent can only see a small area around it, and it must remember which corridors it has explored. Which architecture should you use and how should you set it up?', answer: 'Use an LSTM or GRU recurrent neural network. Initialize the hidden state to zeros at the start of each episode. At each time step, feed the current partial observation and the previous hidden state into the network to produce actions and an updated hidden state. The hidden state will learn to encode relevant history (e.g., which corridors have been explored), enabling informed navigation despite partial observability.', type: 'application', bloomLevel: 'apply', tags: ['LSTM', 'partial-observability', 'architecture'], order: 4 },
  { id: 'rc-marl-7.9-5', lessonId: 'marl-7.9', prompt: 'In multi-agent partially observable environments, the local observation history of agent i is h_t_i = (o_0_i, ..., o_t_i), while the joint-observation history is h_t = (o_0, o_1, ..., o_t). The full environment state can be approximated by _____ when the state is not directly available.', answer: 'The full environment state can be approximated by the joint-observation history: s_t approximately equals h_t.', type: 'cloze', bloomLevel: 'remember', tags: ['joint-observation', 'state-approximation'], order: 5 },

  // ============================================================
  // MODULE 8: Multi-Agent Deep RL (Lessons 8.1 - 8.10)
  // ============================================================

  // --- Lesson 8.1: CTDE paradigm, training vs execution ---
  { id: 'rc-marl-8.1-1', lessonId: 'marl-8.1', prompt: 'What are the three main paradigms of MARL algorithms based on their training and execution modes?', answer: '(1) Centralized training and execution -- both training and policies use centrally shared information. (2) Decentralized training and execution -- training and policies are fully local with no shared information. (3) Centralized training with decentralized execution (CTDE) -- shared information is used during training, but policies only require local observations for execution.', type: 'recall', bloomLevel: 'remember', tags: ['CTDE', 'training-paradigms'], order: 1 },
  { id: 'rc-marl-8.1-2', lessonId: 'marl-8.1', prompt: 'What are the three main limitations of centralized training and execution (central learning)?', answer: '(1) Joint rewards across all agents must be combined into a single reward, which may be difficult or impossible in general-sum games. (2) The central policy operates over the joint-action space, which typically grows exponentially with the number of agents. (3) Agents may be physically distributed entities that cannot communicate with a central controller in real time.', type: 'recall', bloomLevel: 'remember', tags: ['centralized-learning', 'limitations'], order: 2 },
  { id: 'rc-marl-8.1-3', lessonId: 'marl-8.1', prompt: 'Why is CTDE particularly common in deep MARL, and how does it combine the benefits of centralized and decentralized approaches?', answer: 'CTDE allows conditioning value functions on privileged information (joint observations, state) during training to provide better value estimates and stabilize learning, while keeping policies conditioned only on local observations for decentralized execution. This combines the learning benefits of centralized training (reduced non-stationarity, better credit assignment) with the practical deployment advantages of decentralized execution (scalability, no communication requirements).', type: 'concept', bloomLevel: 'understand', tags: ['CTDE', 'paradigm-benefits'], order: 3 },
  { id: 'rc-marl-8.1-4', lessonId: 'marl-8.1', prompt: 'You are designing a MARL system for autonomous delivery drones that must coordinate but cannot reliably communicate during flight. Which training/execution paradigm would you choose, and what would the architecture look like?', answer: 'Use CTDE. During training (in simulation), use a centralized critic conditioned on the joint observations of all drones and potentially the full state to learn accurate value estimates. Train decentralized actor networks for each drone conditioned only on its local observation (camera feed, GPS, nearby drones). At deployment, each drone uses only its own actor network and local observations, requiring no communication.', type: 'application', bloomLevel: 'apply', tags: ['CTDE', 'system-design'], order: 4 },
  { id: 'rc-marl-8.1-5', lessonId: 'marl-8.1', prompt: 'In CTDE, the _____ can be conditioned on centralized information during training, but the _____ must only use local observations to enable decentralized execution.', answer: 'The critic (value function) can be conditioned on centralized information during training, but the actor (policy) must only use local observations to enable decentralized execution.', type: 'cloze', bloomLevel: 'remember', tags: ['CTDE', 'actor-critic'], order: 5 },

  // --- Lesson 8.2: IDQN, IPPO, replay staleness ---
  { id: 'rc-marl-8.2-1', lessonId: 'marl-8.2', prompt: 'What is Independent DQN (IDQN) and what problem arises from its use of replay buffers in multi-agent settings?', answer: 'IDQN applies DQN independently for each agent, where each agent trains its own Q-network from its own observations, actions, and rewards. The replay buffer becomes problematic because stored experiences may be outdated: other agents policies change during training, so past experiences no longer reflect the current environment dynamics. The agent may learn from stale data that leads to incorrect value estimates.', type: 'recall', bloomLevel: 'remember', tags: ['IDQN', 'replay-staleness'], order: 1 },
  { id: 'rc-marl-8.2-2', lessonId: 'marl-8.2', prompt: 'What advantage do on-policy algorithms like independent REINFORCE or IPPO have over off-policy algorithms like IDQN in multi-agent settings?', answer: 'On-policy algorithms always learn from the most up-to-date policies of other agents because the policy gradient is computed from the most recent experiences generated by all agents current policies. This makes them less susceptible to the non-stationarity caused by changing opponent policies, unlike IDQN where the replay buffer contains experiences from outdated policies.', type: 'recall', bloomLevel: 'remember', tags: ['on-policy', 'non-stationarity', 'IPPO'], order: 2 },
  { id: 'rc-marl-8.2-3', lessonId: 'marl-8.2', prompt: 'Explain how hysteretic Q-learning and lenient learning address non-stationarity in independent multi-agent value-based learning.', answer: 'Both methods address the fact that decreasing value estimates may result from other agents exploratory or changing behavior rather than genuinely bad actions. Hysteretic Q-learning uses a smaller learning rate for updates that decrease Q-values, making the agent less sensitive to temporary drops caused by other agents. Lenient learning ignores decreasing Q-value updates with a probability that decreases over training, assuming early negative signals are due to policy stochasticity.', type: 'concept', bloomLevel: 'understand', tags: ['hysteretic', 'leniency', 'non-stationarity'], order: 3 },
  { id: 'rc-marl-8.2-4', lessonId: 'marl-8.2', prompt: 'Two agents are learning chess with IDQN. Agent 1 found a strong opening, but Agent 2 has since learned to counter it. Agent 1 replay buffer still contains old successes with that opening. What will happen and how can you mitigate it?', answer: 'Agent 1 will keep reinforcing the now-countered opening by learning from outdated successful experiences in the replay buffer. Mitigation options: (1) Use smaller replay buffers so old experiences are replaced faster. (2) Use importance sampling weights that store action probabilities of all agents to reweight stale experiences. (3) Switch to an on-policy algorithm like IPPO that always learns from current policies.', type: 'application', bloomLevel: 'apply', tags: ['replay-staleness', 'IDQN', 'chess-example'], order: 4 },
  { id: 'rc-marl-8.2-5', lessonId: 'marl-8.2', prompt: 'In independent learning, each agent treats other agents as part of the _____, which makes the environment appear _____ from each agent perspective as other agents policies change.', answer: 'Each agent treats other agents as part of the environment, which makes the environment appear non-stationary from each agent perspective.', type: 'cloze', bloomLevel: 'remember', tags: ['independent-learning', 'non-stationarity'], order: 5 },

  // --- Lesson 8.3: Centralised critics, MADDPG, MAPPO ---
  { id: 'rc-marl-8.3-1', lessonId: 'marl-8.3', prompt: 'What is a centralized critic in multi-agent actor-critic, and what information does it receive?', answer: 'A centralized critic is a value function for agent i that is conditioned on information beyond the individual agent observation, such as the observation histories of all agents (h_1^t, ..., h_n^t), the full environment state, or additional centralized information z. It outputs a scalar value approximating the returns of agent i policy while using privileged information only available during training.', type: 'recall', bloomLevel: 'remember', tags: ['centralized-critic', 'CTDE'], order: 1 },
  { id: 'rc-marl-8.3-2', lessonId: 'marl-8.3', prompt: 'According to Lyu et al. (2023), what is the minimum information a centralized critic should be conditioned on, and why?', answer: 'At minimum, the critic should be conditioned on the agent own observation history h_t_i, which is the same input as the actor. Without it, the critic may be biased because it has less information than the actor and may be unable to infer which policy it is evaluating. For example, with only the current observation in a partially observable environment, the critic cannot distinguish between different trajectories leading to the same observation.', type: 'recall', bloomLevel: 'remember', tags: ['centralized-critic', 'bias', 'observation-history'], order: 2 },
  { id: 'rc-marl-8.3-3', lessonId: 'marl-8.3', prompt: 'Why can additional centralized information sometimes help in practice even though it theoretically introduces higher variance in the policy gradient?', answer: 'In practice, the theoretical assumptions (e.g., critics converging to true value functions) may not hold in deep learning settings. Additional information can help the critic avoid local minima, learn more informative representations, and adapt faster to non-stationary policies. The theoretical variance increase may be offset by practical benefits of having richer input features, especially in partially observable environments where centralized information reduces uncertainty.', type: 'concept', bloomLevel: 'understand', tags: ['centralized-critic', 'variance-tradeoff'], order: 3 },
  { id: 'rc-marl-8.3-4', lessonId: 'marl-8.3', prompt: 'In the speaker-listener game, a speaker sees the goal landmark but cannot move, while a listener can move but does not know the goal. Independent A2C fails. Explain how a centralized critic helps solve this task.', answer: 'A centralized critic conditioned on both agents observations (the speaker knowledge of the goal landmark and the listener positions/landmarks) can learn more precise value estimates despite partial observability. This allows the critic to provide informative advantage signals: it understands that the speaker communicating the correct landmark leads to high value. These better gradient signals help both the speaker learn to transmit useful messages and the listener learn to follow them.', type: 'application', bloomLevel: 'apply', tags: ['centralized-critic', 'speaker-listener', 'partial-observability'], order: 4 },
  { id: 'rc-marl-8.3-5', lessonId: 'marl-8.3', prompt: 'In a CTDE actor-critic setup, the actor is defined as pi(h_t_i; phi_i) using only local observations, while the centralized critic is V(h_t_i, _____; theta_i), where the blank represents additional _____ information.', answer: 'V(h_t_i, z_t; theta_i), where z_t represents additional centralized information such as other agents observations, the full state, or other privileged data.', type: 'cloze', bloomLevel: 'remember', tags: ['centralized-critic', 'CTDE-architecture'], order: 5 },

  // --- Lesson 8.4: COMA counterfactual baselines, credit assignment ---
  { id: 'rc-marl-8.4-1', lessonId: 'marl-8.4', prompt: 'What is the COMA counterfactual baseline and how does it compute the advantage for agent i?', answer: 'COMA computes the advantage as Adv_i(h_i, z, a) = Q(h_i, z, a; theta) - sum_{a_i\'} pi(a_i\'|h_i; phi_i) * Q(h_i, z, <a_i\', a_{-i}>; theta). The baseline marginalizes out agent i action by computing the expected Q-value over all of agent i actions under its policy, while keeping other agents actions a_{-i} fixed. This addresses credit assignment by measuring how much better the actual action was compared to what the agent would have done on average.', type: 'recall', bloomLevel: 'remember', tags: ['COMA', 'counterfactual-baseline'], order: 1 },
  { id: 'rc-marl-8.4-2', lessonId: 'marl-8.4', prompt: 'What is the aristocrat utility in the context of multi-agent credit assignment?', answer: 'The aristocrat utility is the difference between the reward received and the expected reward agent i would have received by following its current policy: d_i = R(s, <a_i, a_{-i}>) - E_{a_i\' ~ pi_i}[R(s, <a_i\', a_{-i}>)]. It measures whether the chosen action led to better or worse outcomes than what the agent policy would produce on average, without requiring a specific default action.', type: 'recall', bloomLevel: 'remember', tags: ['aristocrat-utility', 'credit-assignment'], order: 2 },
  { id: 'rc-marl-8.4-3', lessonId: 'marl-8.4', prompt: 'Why is the multi-agent credit assignment problem particularly challenging in common-reward games?', answer: 'In common-reward games, all agents receive the same reward, making it difficult to determine which agent actions actually contributed to the outcome. An agent might receive high reward not because of its own good action, but because of other agents actions. The shared reward signal conflates individual contributions, and without a mechanism to isolate each agent effect, agents may learn suboptimal policies because they cannot attribute success or failure to their own behavior.', type: 'concept', bloomLevel: 'understand', tags: ['credit-assignment', 'common-reward'], order: 3 },
  { id: 'rc-marl-8.4-4', lessonId: 'marl-8.4', prompt: 'You have 5 agents in a cooperative game with common rewards, and you notice agents are not learning their individual contributions effectively. Design a centralized action-value critic architecture that enables counterfactual reasoning.', answer: 'Design a centralized action-value critic Q(h_i, z, a; theta) that takes agent i observation history, centralized information, and the actions of all other agents a_{-i} as input, and outputs one Q-value per action of agent i. This architecture allows computing the COMA-style counterfactual baseline efficiently: iterate over each action of agent i while holding a_{-i} fixed, weighting by the policy probability. The advantage tells each agent how its specific action compared to its average behavior.', type: 'application', bloomLevel: 'apply', tags: ['COMA', 'centralized-action-value', 'architecture'], order: 4 },
  { id: 'rc-marl-8.4-5', lessonId: 'marl-8.4', prompt: 'Difference rewards approximate how much an agent contributed by comparing the actual reward to the reward agent i would have received if it had chosen a different _____ action, while other agents actions a_{-i} remain _____.', answer: 'A different default action, while other agents actions a_{-i} remain fixed.', type: 'cloze', bloomLevel: 'remember', tags: ['difference-rewards', 'credit-assignment'], order: 5 },

  // --- Lesson 8.5: Value decomposition, VDN, QMIX ---
  { id: 'rc-marl-8.5-1', lessonId: 'marl-8.5', prompt: 'What is the Individual-Global-Max (IGM) property in value decomposition?', answer: 'The IGM property states that the greedy joint action with respect to the centralized action-value function Q(h, z, a; theta) equals the joint action composed of all agents individually greedy actions with respect to their utility functions: a in A*(h,z;theta) if and only if for all i, a_i in A*_i(h_i;theta_i). This ensures decentralized action selection produces the same result as centralized action selection.', type: 'recall', bloomLevel: 'remember', tags: ['IGM', 'value-decomposition'], order: 1 },
  { id: 'rc-marl-8.5-2', lessonId: 'marl-8.5', prompt: 'How does VDN decompose the centralized action-value function?', answer: 'VDN uses a linear decomposition: Q(h_t, z_t, a_t; theta) = sum_{i in I} Q(h_t_i, a_t_i; theta_i). The centralized value equals the sum of individual agent utility functions, each conditioned only on the individual observation history and action. This satisfies the IGM property because maximizing the sum is equivalent to each agent independently maximizing its own utility.', type: 'recall', bloomLevel: 'remember', tags: ['VDN', 'linear-decomposition'], order: 2 },
  { id: 'rc-marl-8.5-3', lessonId: 'marl-8.5', prompt: 'Why can VDN fail to represent the true centralized action-value function in some environments, while still learning the optimal policy?', answer: 'VDN linear decomposition cannot represent non-linear relationships between agent contributions. If the true centralized Q-function is not linearly decomposable, VDN will learn inaccurate value estimates. However, VDN may still learn the correct ordering of actions (optimal policy) because the IGM property only requires that the greedy joint action is correct, not that the value estimates themselves are accurate. In multi-step games, inaccurate values can propagate through bootstrapping and lead to suboptimal policies.', type: 'concept', bloomLevel: 'understand', tags: ['VDN', 'limitations', 'IGM'], order: 3 },
  { id: 'rc-marl-8.5-4', lessonId: 'marl-8.5', prompt: 'You have a common-reward cooperative game where agent contributions are nonlinearly related (e.g., two agents collecting an item together is worth more than the sum of individual contributions). VDN fails. Which algorithm should you try next and why?', answer: 'Use QMIX, which allows a monotonic (non-linear) decomposition of the centralized Q-function. QMIX uses a mixing network with positive weights (ensuring monotonicity) to combine individual utilities. The mixing network parameters are generated by a hypernetwork conditioned on centralized information. QMIX can represent any monotonic relationship between individual utilities and the joint value, which is a strict superset of VDN linear decomposition.', type: 'application', bloomLevel: 'apply', tags: ['QMIX', 'non-linear-decomposition'], order: 4 },
  { id: 'rc-marl-8.5-5', lessonId: 'marl-8.5', prompt: 'QMIX ensures the monotonicity property by constraining the mixing network to have only _____ weights, so that an increase in any agent utility always leads to an increase in the centralized Q-value.', answer: 'Only positive weights, enforced by applying an absolute value activation function on the hypernetwork outputs corresponding to the mixing network weight matrices.', type: 'cloze', bloomLevel: 'remember', tags: ['QMIX', 'monotonicity'], order: 5 },

  // --- Lesson 8.6: QMIX limitations, QTRAN, QPLEX ---
  { id: 'rc-marl-8.6-1', lessonId: 'marl-8.6', prompt: 'What is the main limitation of QMIX monotonicity constraint demonstrated by the Climbing game?', answer: 'The monotonicity constraint can be too restrictive to represent centralized action-value functions where increasing one agent utility should sometimes decrease the joint value. In the Climbing game, which has no obvious monotonic decomposition, both VDN and QMIX learn inaccurate value functions and converge to suboptimal policies (VDN to (C,C) with reward +5, QMIX to (C,B) with reward +6, instead of the optimal (A,A) with reward +11).', type: 'recall', bloomLevel: 'remember', tags: ['QMIX-limitations', 'Climbing-game'], order: 1 },
  { id: 'rc-marl-8.6-2', lessonId: 'marl-8.6', prompt: 'What three components does QTRAN use to define sufficient and necessary conditions for the IGM property?', answer: 'QTRAN uses: (1) individual utility functions Q(h_i, a_i; theta_i) for each agent, combined as a linear sum; (2) an unrestricted centralized action-value function Q(h, z, a; theta_q) that does not need to be decomposable; and (3) a utility function V(h, z; theta_v) that corrects the difference between the linear sum and the centralized Q-function for greedy actions.', type: 'recall', bloomLevel: 'remember', tags: ['QTRAN', 'IGM-conditions'], order: 2 },
  { id: 'rc-marl-8.6-3', lessonId: 'marl-8.6', prompt: 'Explain why QTRAN conditions are only asymptotically satisfied during training, and what practical issue this creates.', answer: 'QTRAN relaxes its formal conditions into soft regularization terms (L_opt and L_nopt) that are added to the loss function. These losses reach their minimum when the IGM conditions are exactly satisfied, but during training they are only approximately minimized. This means the IGM property is not guaranteed throughout training, which can lead to temporarily inconsistent value decompositions where individual greedy actions do not correspond to the globally greedy joint action, potentially degrading performance.', type: 'concept', bloomLevel: 'understand', tags: ['QTRAN', 'practical-limitations'], order: 3 },
  { id: 'rc-marl-8.6-4', lessonId: 'marl-8.6', prompt: 'Your QMIX implementation performs poorly on a matrix game where the optimal joint action (A,A) has value 11 but nearby actions have value -30. Agents converge to a safer suboptimal equilibrium. Suggest an alternative algorithm and explain why it might help.', answer: 'Try QPLEX, which uses a duplex decomposition splitting Q-values into value and advantage components: Q(h,z,a) = V(h,z) + A(h,z,a). QPLEX defines an advantage-based IGM property that can represent more complex value landscapes than QMIX monotonicity. It uses multi-head attention for mixing advantage functions. Unlike QMIX, QPLEX can represent non-monotonic relationships between individual and joint values, which may better capture the sharp penalty structure of this game.', type: 'application', bloomLevel: 'apply', tags: ['QPLEX', 'value-decomposition', 'non-monotonic'], order: 4 },
  { id: 'rc-marl-8.6-5', lessonId: 'marl-8.6', prompt: 'Weighted QMIX improves on QMIX by putting more emphasis on learning value estimates for joint actions that are considered potentially _____, rather than giving equal importance to all joint actions.', answer: 'Potentially optimal joint actions. Weighted QMIX uses an unconstrained mixing network to identify which joint actions might be optimal and weights the value loss accordingly.', type: 'cloze', bloomLevel: 'remember', tags: ['weighted-QMIX'], order: 5 },

  // --- Lesson 8.7: Neural agent models, opponent modelling, recursive reasoning ---
  { id: 'rc-marl-8.7-1', lessonId: 'marl-8.7', prompt: 'In deep joint-action learning with agent models, how does each agent model the policies of other agents?', answer: 'Each agent i maintains neural network agent models pi_hat_j^i for each other agent j. These models take agent i observation history h_t_i as input and output a probability distribution over agent j actions. They are trained by minimizing the cross-entropy loss between predicted action probabilities and the true actions of other agents observed during training.', type: 'recall', bloomLevel: 'remember', tags: ['agent-modeling', 'policy-reconstruction'], order: 1 },
  { id: 'rc-marl-8.7-2', lessonId: 'marl-8.7', prompt: 'What is the encoder-decoder approach to learning representations of other agent policies?', answer: 'An encoder network f_e takes the agent observation history and outputs a compact representation m_t_i of other agents policies. A decoder network f_d takes this representation and predicts action probabilities of all other agents. The encoder-decoder is trained via cross-entropy loss on true actions. During execution, only the encoder representation m_t_i is used to condition the agent policy and value function, providing a compact summary of other agents behavior.', type: 'recall', bloomLevel: 'remember', tags: ['encoder-decoder', 'agent-representation'], order: 2 },
  { id: 'rc-marl-8.7-3', lessonId: 'marl-8.7', prompt: 'What is the key difference between policy reconstruction (direct agent modeling) and learning policy representations (encoder-decoder), and when might representations be preferred?', answer: 'Policy reconstruction directly predicts other agents action probabilities and uses them to compute expected action values. Policy representations learn a compact embedding of other agents behavior that conditions the agent own policy and value function. Representations are preferred when: (1) direct reconstruction is too complex for large action spaces, (2) the exact policy is less useful than behavioral patterns, or (3) the representation needs to be used during decentralized execution where true policies are unavailable.', type: 'concept', bloomLevel: 'understand', tags: ['agent-modeling', 'representation-learning'], order: 3 },
  { id: 'rc-marl-8.7-4', lessonId: 'marl-8.7', prompt: 'You are building a multi-agent system where agents need to adapt to changing co-player strategies. Design a system using encoder-decoder agent modeling integrated with centralized A2C.', answer: 'For each agent i: (1) Train an encoder f_e(h_t_i; psi_e_i) to produce representation m_t_i from observation history. (2) Train a decoder f_d(m_t_i; psi_d_i) to predict other agents action probabilities, using cross-entropy loss against true actions (stop gradients from MARL to encoder). (3) Condition the actor as pi(.|h_t_i, m_t_i; phi_i) and centralized critic as V(h_t_i, z, m_t_i; theta_i) on the representation. The representation adapts as other agents policies change, enabling dynamic adaptation.', type: 'application', bloomLevel: 'apply', tags: ['encoder-decoder', 'centralized-A2C', 'adaptation'], order: 4 },
  { id: 'rc-marl-8.7-5', lessonId: 'marl-8.7', prompt: 'Recursive reasoning in MARL refers to agents considering how their actions might affect other agents _____ and how other agents might _____ about their own knowledge, inspired by theory of mind.', answer: 'How their actions might affect other agents learning/behavior, and how other agents might believe/reason about their own knowledge.', type: 'cloze', bloomLevel: 'remember', tags: ['recursive-reasoning', 'theory-of-mind'], order: 5 },

  // --- Lesson 8.8: Parameter sharing, experience sharing, symmetry breaking ---
  { id: 'rc-marl-8.8-1', lessonId: 'marl-8.8', prompt: 'What is parameter sharing in MARL and what types of agent homogeneity does it assume?', answer: 'Parameter sharing means all agents use a single shared neural network (same parameters) instead of separate networks. It assumes at minimum weakly homogeneous agents (same capabilities, interchangeable policies with same outcome). In strongly homogeneous environments, the optimal policy is identical for all agents. A one-hot agent ID is often appended to inputs to allow differentiated behavior despite shared parameters.', type: 'recall', bloomLevel: 'remember', tags: ['parameter-sharing', 'homogeneity'], order: 1 },
  { id: 'rc-marl-8.8-2', lessonId: 'marl-8.8', prompt: 'What is the difference between weakly homogeneous and strongly homogeneous agents?', answer: 'Weakly homogeneous: for any joint policy and any permutation of agents, each agent returns remain the same (agents are interchangeable). Strongly homogeneous: additionally, the optimal joint policy consists of identical individual policies (all agents should behave the same way). Weakly homogeneous agents may need different policies (e.g., going to different landmarks), while strongly homogeneous agents can share identical policies.', type: 'recall', bloomLevel: 'remember', tags: ['homogeneity', 'weak-vs-strong'], order: 2 },
  { id: 'rc-marl-8.8-3', lessonId: 'marl-8.8', prompt: 'Why might parameter sharing be problematic in environments with weakly (but not strongly) homogeneous agents?', answer: 'When agents need different optimal policies (weakly homogeneous), a single shared network must produce different behaviors for different agents receiving similar observations. While a one-hot agent ID can help break symmetry, the shared network may struggle to learn sufficiently differentiated policies. The agents may converge to identical suboptimal behavior instead of the optimal heterogeneous policies, especially if the environment requires agents to specialize in different roles.', type: 'concept', bloomLevel: 'understand', tags: ['parameter-sharing', 'symmetry-breaking'], order: 3 },
  { id: 'rc-marl-8.8-4', lessonId: 'marl-8.8', prompt: 'You have 10 homogeneous agents in a cooperative task and training is slow because you maintain 10 separate networks. Implement parameter sharing while still allowing behavioral diversity.', answer: 'Replace the 10 separate networks with a single shared network. Append a one-hot encoded agent ID (10-dimensional vector) to each agent observation before feeding it to the shared network. All agents experiences can be pooled together for training (experience sharing), effectively multiplying the training data by 10x. The agent ID allows the network to learn agent-specific behaviors when needed, while the shared parameters enable efficient generalization across agents.', type: 'application', bloomLevel: 'apply', tags: ['parameter-sharing', 'agent-ID', 'experience-sharing'], order: 4 },
  { id: 'rc-marl-8.8-5', lessonId: 'marl-8.8', prompt: 'In the original QMIX implementation, individual utility networks are _____ across agents and receive an additional _____ encoded agent ID to differentiate between agents.', answer: 'Shared across agents, and receive an additional one-hot encoded agent ID.', type: 'cloze', bloomLevel: 'remember', tags: ['QMIX', 'parameter-sharing'], order: 5 },

  // --- Lesson 8.9: AlphaGo/AlphaZero, MCTS + neural nets ---
  { id: 'rc-marl-8.9-1', lessonId: 'marl-8.9', prompt: 'What are the four phases of Monte Carlo Tree Search (MCTS)?', answer: '(1) Selection: traverse the tree from the root using a selection policy (e.g., UCB) until reaching a leaf node. (2) Expansion: add one or more child nodes to the leaf. (3) Simulation/Rollout: play out the game from the new node using a rollout policy (e.g., random) until a terminal state. (4) Backpropagation: update the statistics (visit counts, values) of all nodes along the path from the new node back to the root.', type: 'recall', bloomLevel: 'remember', tags: ['MCTS', 'tree-search'], order: 1 },
  { id: 'rc-marl-8.9-2', lessonId: 'marl-8.9', prompt: 'How does AlphaZero combine MCTS with deep neural networks?', answer: 'AlphaZero uses a neural network that takes a board state as input and outputs both a policy (prior probabilities over moves) and a value estimate. During MCTS, the neural network policy guides the selection phase (replacing random rollouts), and the value estimate is used to evaluate leaf nodes instead of random simulation. The network is trained through self-play, using MCTS-improved policies as training targets for the policy head and game outcomes as targets for the value head.', type: 'recall', bloomLevel: 'remember', tags: ['AlphaZero', 'MCTS', 'neural-network'], order: 2 },
  { id: 'rc-marl-8.9-3', lessonId: 'marl-8.9', prompt: 'Why is self-play effective for training agents in two-player zero-sum games?', answer: 'In self-play, a single agent trains by playing against copies of its own policy. As the agent improves, its opponent (a copy of itself) also improves, creating an arms race that drives continuous improvement. In zero-sum games, any weakness in the policy will be exploited by the copy, providing a natural curriculum of increasingly challenging opponents. This avoids the need for hand-crafted opponents and can theoretically converge to optimal play.', type: 'concept', bloomLevel: 'understand', tags: ['self-play', 'zero-sum'], order: 3 },
  { id: 'rc-marl-8.9-4', lessonId: 'marl-8.9', prompt: 'You want to build an AlphaZero-style system for a board game. Describe the training loop including the interaction between MCTS and the neural network.', answer: 'Training loop: (1) Generate self-play games: at each position, run MCTS guided by the current neural network to produce an improved policy (visit count distribution) and select moves. Store (state, MCTS_policy, game_outcome) tuples. (2) Train the network: use states as input, MCTS policies as policy targets (cross-entropy loss), and game outcomes (+1/-1) as value targets (MSE loss). (3) Update the MCTS player with the new network. (4) Repeat. The MCTS improves upon the network policy through search, and the network learns from these improved policies, creating a virtuous cycle.', type: 'application', bloomLevel: 'apply', tags: ['AlphaZero', 'training-loop', 'self-play'], order: 4 },
  { id: 'rc-marl-8.9-5', lessonId: 'marl-8.9', prompt: 'In AlphaZero, the neural network replaces the _____ phase of traditional MCTS with a learned value estimate, and replaces the random rollout policy with a learned _____ to guide tree exploration.', answer: 'The simulation/rollout phase with a learned value estimate, and the random rollout policy with a learned prior policy.', type: 'cloze', bloomLevel: 'remember', tags: ['AlphaZero', 'MCTS-replacement'], order: 5 },

  // --- Lesson 8.10: Double Oracle, PSRO, AlphaStar league ---
  { id: 'rc-marl-8.10-1', lessonId: 'marl-8.10', prompt: 'What is Policy Space Response Oracles (PSRO) and how does it extend the double oracle method?', answer: 'PSRO maintains a population of policies for each agent. In each iteration, it computes a meta-strategy (mixture over existing policies) using game-theoretic methods, then trains new best-response policies against the current meta-strategies using deep RL. These new policies are added to the population and the meta-game is recomputed. PSRO extends double oracle by using approximate best responses (via deep RL) instead of exact solutions, making it applicable to complex games.', type: 'recall', bloomLevel: 'remember', tags: ['PSRO', 'double-oracle', 'population-training'], order: 1 },
  { id: 'rc-marl-8.10-2', lessonId: 'marl-8.10', prompt: 'How did AlphaStar use population-based training to reach grandmaster level in StarCraft II?', answer: 'AlphaStar maintained a league of agents organized into main agents, main exploiters, and league exploiters. Main agents trained against the full league through self-play and were added to the league when they could beat all existing league members. Main exploiters found weaknesses in main agents, and league exploiters found weaknesses across the league. This diverse population prevented strategy cycling and produced robust agents that reached grandmaster level.', type: 'recall', bloomLevel: 'remember', tags: ['AlphaStar', 'league-training'], order: 2 },
  { id: 'rc-marl-8.10-3', lessonId: 'marl-8.10', prompt: 'Why can naive self-play fail in complex games, and how does population-based training address this?', answer: 'Naive self-play can lead to strategy cycling: agent A beats B, B is replaced by C that beats A, C is replaced by A that beats C, and so on. The agent never converges to a robust strategy. Population-based training maintains a diverse pool of opponents representing different strategies. Training against this diverse population forces agents to develop robust strategies that work against many different approaches rather than specializing against a single opponent style.', type: 'concept', bloomLevel: 'understand', tags: ['self-play-limitations', 'population-training'], order: 3 },
  { id: 'rc-marl-8.10-4', lessonId: 'marl-8.10', prompt: 'You are building a competitive game-playing AI and find that your self-play agent keeps cycling between rock-paper-scissors-like strategies. Design a PSRO-based solution.', answer: 'Initialize the population with one or more initial policies. In each PSRO iteration: (1) Build the meta-game payoff matrix by evaluating all policy pairs. (2) Compute the Nash equilibrium of the meta-game as the meta-strategy (mixture weights over existing policies). (3) Train a new best-response policy using deep RL against opponents sampled from the meta-strategy distribution. (4) Add the new policy to the population. Repeat. The meta-strategy ensures training against a balanced mixture of all known strategies, and the growing population captures diverse counter-strategies.', type: 'application', bloomLevel: 'apply', tags: ['PSRO', 'strategy-cycling', 'game-AI'], order: 4 },
  { id: 'rc-marl-8.10-5', lessonId: 'marl-8.10', prompt: 'In AlphaStar league, the three types of agents are: _____ agents that train against the full league, main _____ that find weaknesses in main agents, and league _____ that find weaknesses across the entire league.', answer: 'Main agents, main exploiters, and league exploiters.', type: 'cloze', bloomLevel: 'remember', tags: ['AlphaStar', 'league-roles'], order: 5 },

  // ============================================================
  // MODULE 9: MARL in Practice (Lessons 9.1 - 9.4)
  // ============================================================

  // --- Lesson 9.1: PettingZoo, parallel/AEC API, training loop ---
  { id: 'rc-marl-9.1-1', lessonId: 'marl-9.1', prompt: 'What are the two main functions of the standard agent-environment interface used in MARL?', answer: 'reset() -- initializes the environment and returns initial observations for each agent. step(actions) -- advances the environment by one time step given the joint action, returning next observations, rewards, a terminal signal indicating if the episode ended, and an optional info dictionary.', type: 'recall', bloomLevel: 'remember', tags: ['environment-interface', 'Gym-API'], order: 1 },
  { id: 'rc-marl-9.1-2', lessonId: 'marl-9.1', prompt: 'What is Petting Zoo and what types of environments does it include?', answer: 'Petting Zoo is a library for MARL research containing a large number of multi-agent environments, including multi-agent Atari games, classic games (Connect Four, Go, Texas Holdem), continuous control tasks, and the multi-agent particle environment (MPE). It covers full and partial observability, discrete and continuous actions, and dense and sparse rewards, all unified under a common interface.', type: 'recall', bloomLevel: 'remember', tags: ['PettingZoo', 'environment-library'], order: 2 },
  { id: 'rc-marl-9.1-3', lessonId: 'marl-9.1', prompt: 'Why is there no single unified environment interface in MARL unlike in single-agent RL, and what challenges does this create?', answer: 'Different MARL frameworks use different interfaces because multi-agent environments have diverse requirements: some are simultaneous-move, others are turn-based; some have variable numbers of agents; some need to communicate available actions or additional state information. This makes it challenging to implement algorithms that work seamlessly across all environments and often requires environment wrappers to adapt different interfaces to a common format used by the MARL algorithm.', type: 'concept', bloomLevel: 'understand', tags: ['environment-interface', 'standardization'], order: 3 },
  { id: 'rc-marl-9.1-4', lessonId: 'marl-9.1', prompt: 'Write the pseudocode for a basic MARL training loop that interacts with a Gym-style multi-agent environment for one episode.', answer: 'observations = env.reset(); done = False; while not done: actions = [agent_i.select_action(obs_i) for each agent]; next_observations, rewards, done, info = env.step(actions); for each agent: store_transition(obs_i, action_i, reward_i, next_obs_i, done); if update_condition: update_agent_networks(); observations = next_observations. After episode: perform any end-of-episode updates (e.g., for REINFORCE-style algorithms).', type: 'application', bloomLevel: 'apply', tags: ['training-loop', 'implementation'], order: 4 },
  { id: 'rc-marl-9.1-5', lessonId: 'marl-9.1', prompt: 'In the Gym-style MARL interface, env.observation_space returns a _____ of observation spaces (one per agent), and env.action_space returns a _____ of action spaces.', answer: 'A Tuple of observation spaces and a Tuple of action spaces (with n elements corresponding to each agent).', type: 'cloze', bloomLevel: 'remember', tags: ['Gym-API', 'observation-action-spaces'], order: 5 },

  // --- Lesson 9.2: Centralised value implementation, QMIX tips, debugging ---
  { id: 'rc-marl-9.2-1', lessonId: 'marl-9.2', prompt: 'How is a centralized critic implemented in PyTorch for a two-agent MARL scenario?', answer: 'Concatenate the observations of both agents into a single tensor (centr_obs = torch.cat([obs1, obs2])). Create a MultiAgentFCNetwork with input size equal to the sum of individual observation sizes (e.g., 10 for two agents with obs size 5 each) and output size of 1 (scalar value). The critic takes the concatenated observations and outputs a state-value estimate for each agent.', type: 'recall', bloomLevel: 'remember', tags: ['centralized-critic', 'PyTorch-implementation'], order: 1 },
  { id: 'rc-marl-9.2-2', lessonId: 'marl-9.2', prompt: 'How is VDN value decomposition implemented in practice using PyTorch?', answer: 'Compute Q-values for each agent network, stack them into a tensor with an agent dimension, then use .sum(0) to sum across the agent dimension. For target computation: sum the max Q-values of each agent next-state network. The VDN target becomes: target = rewards + gamma * summed_max_next_Q * (1 - terminal_signal). The loss is the MSE between summed current Q-values and this target.', type: 'recall', bloomLevel: 'remember', tags: ['VDN', 'implementation'], order: 2 },
  { id: 'rc-marl-9.2-3', lessonId: 'marl-9.2', prompt: 'Why is using a single centralized optimizer for all agent networks faster than using separate optimizers per agent?', answer: 'Multiple separate optimizers each maintain their own internal state and parameter lists, and the backward pass and optimization step must be called separately for each. A single optimizer encompasses all trainable parameters, allowing PyTorch to compute all gradients in one backward pass and perform a single optimization step. The final losses from all agents are simply summed before calling backward(), leveraging parallel computation and avoiding redundant overhead.', type: 'concept', bloomLevel: 'understand', tags: ['centralized-optimization', 'efficiency'], order: 3 },
  { id: 'rc-marl-9.2-4', lessonId: 'marl-9.2', prompt: 'You are implementing QMIX and the agent is not learning. Debugging shows the individual Q-values are all similar regardless of observation. What implementation details from the original QMIX paper should you verify?', answer: 'Check: (1) Parameter sharing with one-hot agent ID appended to observations, enabling differentiated behavior. (2) Recurrent utility networks (GRU/LSTM) that process observation histories, not just current observations. (3) Include the last action in the observation to help the utility function during stochastic exploration. (4) Use an episodic replay buffer sampling entire episodes. (5) Verify the hypernetwork correctly generates positive mixing weights via absolute value activation. (6) Verify target networks are being updated at the correct interval.', type: 'application', bloomLevel: 'apply', tags: ['QMIX', 'debugging', 'implementation-details'], order: 4 },
  { id: 'rc-marl-9.2-5', lessonId: 'marl-9.2', prompt: 'In a CTDE actor-critic implementation, the actor network receives only agent i _____ as input, while the critic receives the _____ of all agents observations as input.', answer: 'The actor receives only agent i observation as input, while the critic receives the concatenation of all agents observations.', type: 'cloze', bloomLevel: 'remember', tags: ['CTDE', 'actor-critic-implementation'], order: 5 },

  // --- Lesson 9.3: Reporting results, matrix game diagnostics ---
  { id: 'rc-marl-9.3-1', lessonId: 'marl-9.3', prompt: 'What are the two main metrics that can be extracted from a learning curve to compare MARL algorithms?', answer: '(1) Maximum value of the curve -- indicates whether the algorithm at any point solved the environment or achieved a desired behavior. (2) Average value of the curve -- indicates how fast the algorithm learned and how stable its performance was over training. Multiple algorithms may reach the same maximum, in which case the average helps distinguish them based on learning speed and stability.', type: 'recall', bloomLevel: 'remember', tags: ['evaluation-metrics', 'learning-curves'], order: 1 },
  { id: 'rc-marl-9.3-2', lessonId: 'marl-9.3', prompt: 'Why are standard learning curves uninformative for evaluating agents in zero-sum games?', answer: 'In zero-sum games where agents train against each other, the learning curves of opponents mirror each other (one gains returns while the other loses), making it impossible to determine if agents are actually improving. Even if both agents are getting stronger, neither may show increasing returns since improvements are offset by the opponent improving equally. The curves might appear flat or oscillating even when significant learning is occurring.', type: 'recall', bloomLevel: 'remember', tags: ['zero-sum-evaluation', 'learning-curves'], order: 2 },
  { id: 'rc-marl-9.3-3', lessonId: 'marl-9.3', prompt: 'Explain why matrix games are useful as diagnostic tools for MARL algorithms despite not being representative of complex environments.', answer: 'Matrix games allow direct visualization and analysis of learned value decompositions and policies. Because exact solutions (Nash equilibria, Pareto optima) can be computed, you can precisely evaluate whether the algorithm converges to the correct solution concept. They reveal fundamental limitations such as QMIX inability to represent non-monotonic value functions (Climbing game) or VDN inability to represent non-linear decompositions. These insights explain algorithm failures in complex environments where direct analysis is intractable.', type: 'concept', bloomLevel: 'understand', tags: ['matrix-games', 'diagnostics'], order: 3 },
  { id: 'rc-marl-9.3-4', lessonId: 'marl-9.3', prompt: 'You need to compare three MARL algorithms fairly. One colleague ran Algorithm A with extensive hyperparameter tuning, while Algorithms B and C used default parameters. How should you proceed?', answer: 'Run a grid search with equal computational budget for all three algorithms, varying key hyperparameters (learning rate, entropy coefficient, etc.) across multiple random seeds. For each algorithm, select the best hyperparameter combination based on a metric (e.g., maximum evaluation return). Then compare the algorithms using their best hyperparameters, reporting mean and standard deviation/error across seeds. An unequal hyperparameter search makes comparisons unfair.', type: 'application', bloomLevel: 'apply', tags: ['fair-comparison', 'hyperparameter-search'], order: 4 },
  { id: 'rc-marl-9.3-5', lessonId: 'marl-9.3', prompt: 'When reporting MARL experimental results, learning curves should show the _____ and standard _____ across multiple runs with different random seeds.', answer: 'The mean and standard deviation (or standard error) across multiple runs with different random seeds.', type: 'cloze', bloomLevel: 'remember', tags: ['reporting-results', 'statistical-significance'], order: 5 },

  // --- Lesson 9.4: SMAC, LBF, Hanabi, MPE environments ---
  { id: 'rc-marl-9.4-1', lessonId: 'marl-9.4', prompt: 'What is the StarCraft Multi-Agent Challenge (SMAC) and what is its main learning challenge?', answer: 'SMAC contains common-reward tasks based on StarCraft II where a team of agents controls units fighting against a built-in AI. Tasks vary in unit types and maps. The main challenge is credit assignment: common rewards make it difficult to determine which agent actions contributed to the outcome. Actions can have long-term consequences (e.g., destroying a unit early), and the common reward conflates individual contributions, making value decomposition methods particularly suitable.', type: 'recall', bloomLevel: 'remember', tags: ['SMAC', 'credit-assignment'], order: 1 },
  { id: 'rc-marl-9.4-2', lessonId: 'marl-9.4', prompt: 'What makes Hanabi a uniquely challenging multi-agent environment?', answer: 'Hanabi is a cooperative card game where each player can see other players cards but not their own. Players must use limited hint actions (constrained by information tokens) to communicate information. This creates a challenging partial observability problem where agents must establish implicit communication conventions beyond the limited hint mechanism. It requires cooperative self-play, ad hoc teamwork, and acting under imperfect information.', type: 'recall', bloomLevel: 'remember', tags: ['Hanabi', 'partial-observability', 'communication'], order: 2 },
  { id: 'rc-marl-9.4-3', lessonId: 'marl-9.4', prompt: 'What key criteria should be considered when choosing a multi-agent environment for evaluating MARL algorithms?', answer: 'Consider: (1) What properties to test (convergence to solution concepts, scalability in number of agents, handling large state/action spaces, partial observability, sparse rewards). (2) What skills agents must learn (cooperation timing, information sharing, role distribution, coordination). (3) Whether exact solutions can be computed for comparison. (4) Configurability to create tasks of varying difficulty. (5) The combination of challenges: the hardest tasks usually combine large spaces, limited observability, and sparse rewards.', type: 'concept', bloomLevel: 'understand', tags: ['environment-selection', 'evaluation-criteria'], order: 3 },
  { id: 'rc-marl-9.4-4', lessonId: 'marl-9.4', prompt: 'You want to test whether your MARL algorithm can handle cooperative tasks requiring different levels of coordination. Design a testing plan using LBF environments of increasing difficulty.', answer: 'Start with easy tasks: small grid (8x8), 2 agents, 1 item, random levels where agents can collect items individually. Increase difficulty progressively: (1) Add more items and agents, increasing state space. (2) Reduce grid observability from full to partial (limited observation radius). (3) Enable forced cooperation by setting item levels such that all agents must cooperate. (4) Increase grid size (15x15) with random initial positions. (5) Add collection penalties to create suboptimal equilibria. Track evaluation returns at each difficulty level to identify where the algorithm fails.', type: 'application', bloomLevel: 'apply', tags: ['LBF', 'evaluation-plan', 'progressive-difficulty'], order: 4 },
  { id: 'rc-marl-9.4-5', lessonId: 'marl-9.4', prompt: 'In the multi-robot warehouse (RWARE) environment, the main learning challenge is _____ rewards, because agents must execute very specific long sequences of actions to deliver shelves before receiving any non-zero reward.', answer: 'Very sparse rewards. Delivering shelves requires precise long action sequences, making exploration extremely challenging.', type: 'cloze', bloomLevel: 'remember', tags: ['RWARE', 'sparse-rewards'], order: 5 },
];

/** Review cards indexed by lesson ID for efficient lookup. */
export const reviewCardsByLesson: Record<string, ReviewCard[]> = {};
for (const card of reviewCards) {
  if (!reviewCardsByLesson[card.lessonId]) {
    reviewCardsByLesson[card.lessonId] = [];
  }
  reviewCardsByLesson[card.lessonId].push(card);
}

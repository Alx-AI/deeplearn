import type { ReviewCard } from '@/lib/db/schema';

export const reviewCards: ReviewCard[] = [
  // ========== MODULE 1 - FOUNDATIONS ==========

  // --- Lesson 1.1: What is AI, ML, and Deep Learning? ---
  { id: 'rc-1.1-1', lessonId: '1.1', prompt: 'What are the three concentric fields: AI, ML, and deep learning? How do they relate?', answer: 'AI is the broadest field: the effort to automate intellectual tasks normally performed by humans. ML is a subset of AI where systems learn rules from data instead of being explicitly programmed. Deep learning is a subset of ML that uses successive layers of learned representations (deep neural networks). DL ⊂ ML ⊂ AI.', type: 'recall', bloomLevel: 'remember', tags: ['ai', 'ml', 'deep-learning'], order: 1 },
  { id: 'rc-1.1-2', lessonId: '1.1', prompt: 'Why is the "classical programming" paradigm insufficient for tasks like image recognition, and how does machine learning address this?', answer: 'Classical programming requires hand-crafted rules, which become impossibly complex for perceptual tasks like image recognition. ML flips the paradigm: instead of humans writing rules, the system learns rules automatically by being shown many examples of inputs paired with expected outputs. This data-driven approach scales to problems where explicit rules are infeasible.', type: 'concept', bloomLevel: 'understand', tags: ['classical-programming', 'paradigm-shift'], order: 2 },
  { id: 'rc-1.1-3', lessonId: '1.1', prompt: 'A company wants to automatically categorize thousands of customer support emails into topics. Should they use classical programming or ML? Justify your choice.', answer: 'ML is the better approach. Email language is varied, ambiguous, and evolving -- hand-writing rules for every possible phrasing is impractical. An ML model can learn from labeled examples of emails and their categories, generalize to unseen emails, and be retrained as new topics emerge.', type: 'application', bloomLevel: 'apply', tags: ['ml-applications', 'text-classification'], order: 3 },
  { id: 'rc-1.1-4', lessonId: '1.1', prompt: 'Deep learning is distinguished from other ML approaches by learning successive _____ of increasingly meaningful _____ from data.', answer: 'layers; representations', type: 'cloze', bloomLevel: 'remember', tags: ['deep-learning', 'representations'], order: 4 },

  // --- Lesson 1.2: Learning Representations from Data ---
  { id: 'rc-1.2-1', lessonId: '1.2', prompt: 'What three things does a machine learning system need in order to learn?', answer: '1. **Input data** (e.g., images, text, sensor readings). 2. **Expected outputs** (labels or target values). 3. **A way to measure performance** (a loss function comparing predictions to expected outputs). The model uses the loss signal to adjust its parameters.', type: 'recall', bloomLevel: 'remember', tags: ['ml-components', 'loss-function'], order: 1 },
  { id: 'rc-1.2-2', lessonId: '1.2', prompt: 'Explain what "learning representations" means. Why is deep learning also called "representation learning"?', answer: 'A representation is a different way to look at or encode data that may make certain tasks easier. Deep learning automatically discovers transformations of data into useful representations, rather than relying on hand-engineered features. Each layer transforms data into a progressively more abstract and useful representation for the task.', type: 'concept', bloomLevel: 'understand', tags: ['representations', 'feature-learning'], order: 2 },
  { id: 'rc-1.2-3', lessonId: '1.2', prompt: 'Describe what kinds of representations the successive layers of a deep network might learn when classifying photos of animals.', answer: 'Early layers learn low-level features like edges, corners, and color gradients. Middle layers combine these into textures, parts (eyes, ears, fur patterns). Later layers assemble parts into object-level representations (faces, bodies). The final layer maps high-level representations to class probabilities (cat, dog, bird).', type: 'application', bloomLevel: 'apply', tags: ['hierarchical-features', 'image-classification'], order: 3 },
  { id: 'rc-1.2-4', lessonId: '1.2', prompt: 'In ML, the feedback signal used to adjust the model is called the _____, which measures the distance between predictions and _____.', answer: 'loss function (also called objective function or cost function); targets (or expected outputs)', type: 'cloze', bloomLevel: 'remember', tags: ['loss-function', 'optimization'], order: 4 },

  // --- Lesson 1.3: How Deep Learning Works (The Big Picture) ---
  { id: 'rc-1.3-1', lessonId: '1.3', prompt: 'Describe the four steps of the core neural network training loop.', answer: '1. **Forward pass**: Input flows through layers to produce predictions. 2. **Loss computation**: The loss function compares predictions to true targets. 3. **Backward pass**: Backpropagation computes the gradient of the loss with respect to each weight. 4. **Weight update**: The optimizer adjusts weights in the direction that reduces the loss. This loop repeats over many batches.', type: 'recall', bloomLevel: 'remember', tags: ['training-loop', 'forward-pass', 'backpropagation'], order: 1 },
  { id: 'rc-1.3-2', lessonId: '1.3', prompt: 'Why is deep learning described as learning a "chain of geometric transformations"? What does each layer do geometrically?', answer: 'Each layer applies a parameterized geometric transformation (rotation, scaling, translation, warping) to its input. Stacking layers chains simple transformations that together implement very complex mappings. The network learns to "unfold" the data manifold so that the output representation is linearly separable or matches desired targets.', type: 'concept', bloomLevel: 'understand', tags: ['geometric-transformations', 'manifold'], order: 2 },
  { id: 'rc-1.3-3', lessonId: '1.3', prompt: 'A model\'s training loss stops decreasing after 5 epochs. Propose two possible diagnoses and a remedy for each.', answer: '1. **Learning rate too high**: The optimizer overshoots minima. Remedy: reduce the learning rate. 2. **Model too small (underfitting)**: The model lacks capacity to capture patterns. Remedy: increase model size (more layers or units). Other possibilities: poor data preprocessing or label noise.', type: 'application', bloomLevel: 'analyze', tags: ['training-debugging', 'learning-rate'], order: 3 },
  { id: 'rc-1.3-4', lessonId: '1.3', prompt: 'The _____ is the algorithm responsible for using computed gradients to decide how to update the network\'s weights each step.', answer: 'optimizer (e.g., SGD, Adam, RMSprop)', type: 'cloze', bloomLevel: 'remember', tags: ['optimizer', 'weight-update'], order: 4 },

  // --- Lesson 1.4: The Hype Cycle and Promise of AI ---
  { id: 'rc-1.4-1', lessonId: '1.4', prompt: 'Name three key factors that drove the deep learning revolution starting around 2012.', answer: '1. **Hardware**: GPUs provided massive parallelism for matrix operations. 2. **Data**: Large labeled datasets like ImageNet became available. 3. **Algorithmic advances**: ReLU activations, dropout, batch normalization, and better weight initialization made training deep networks practical.', type: 'recall', bloomLevel: 'remember', tags: ['deep-learning-revolution', 'gpu', 'imagenet'], order: 1 },
  { id: 'rc-1.4-2', lessonId: '1.4', prompt: 'What was an "AI winter" and what lessons does it offer for the current era of AI?', answer: 'AI winters were periods when AI funding and interest collapsed after inflated expectations were unmet. They occurred in the mid-1970s and late 1980s-90s when symbolic AI and early neural networks were oversold. The lesson: maintain realistic expectations, acknowledge limitations, and focus on genuine progress rather than hype.', type: 'concept', bloomLevel: 'understand', tags: ['ai-winter', 'hype-cycle'], order: 2 },
  { id: 'rc-1.4-3', lessonId: '1.4', prompt: 'A startup claims their AI can "understand any question and reason like a human." Analyze this claim using what you know about deep learning\'s current capabilities.', answer: 'This claim is overstated. Current deep learning models, including LLMs, excel at pattern matching and statistical correlation but lack true understanding, causal reasoning, and common sense. They can fail unpredictably on novel situations. A responsible assessment would specify the narrow tasks handled well and acknowledge limitations.', type: 'application', bloomLevel: 'analyze', tags: ['ai-hype', 'limitations'], order: 3 },
  { id: 'rc-1.4-4', lessonId: '1.4', prompt: 'The 2012 ImageNet breakthrough that catalyzed the deep learning era is attributed to a CNN called _____ by Alex Krizhevsky et al.', answer: 'AlexNet', type: 'cloze', bloomLevel: 'remember', tags: ['alexnet', 'imagenet'], order: 4 },

  // --- Lesson 1.5: Your First Neural Network (MNIST) ---
  { id: 'rc-1.5-1', lessonId: '1.5', prompt: 'Describe the MNIST dataset: what does it contain, how is it split, and what is the task?', answer: 'MNIST contains 70,000 grayscale images of handwritten digits (0-9), each $28 \\times 28$ pixels. It is split into 60,000 training and 10,000 test images. The task is 10-class classification: given an image, predict which digit (0-9) it depicts.', type: 'recall', bloomLevel: 'remember', tags: ['mnist', 'classification'], order: 1 },
  { id: 'rc-1.5-2', lessonId: '1.5', prompt: 'Why do we use a softmax activation on the final layer and categorical crossentropy loss for MNIST?', answer: 'Softmax converts raw logits into a probability distribution over 10 classes (outputs sum to 1). Categorical crossentropy measures the divergence between the predicted distribution and the true one-hot label. Together they train the model to output calibrated probabilities, heavily penalizing confident wrong predictions.', type: 'concept', bloomLevel: 'understand', tags: ['softmax', 'crossentropy'], order: 2 },
  { id: 'rc-1.5-3', lessonId: '1.5', prompt: 'Your MNIST model achieves 95% training accuracy but only 91% test accuracy. What might be happening and how would you address it?', answer: 'The gap indicates overfitting -- the model memorizes training data but does not generalize. Remedies: add dropout layers, reduce model capacity, add L2 regularization, apply data augmentation, or use early stopping. For MNIST, a well-tuned model should reach 98%+ test accuracy.', type: 'application', bloomLevel: 'analyze', tags: ['overfitting', 'mnist'], order: 3 },
  { id: 'rc-1.5-4', lessonId: '1.5', prompt: 'Before feeding MNIST images to a Dense network, each $28 \\times 28$ image is reshaped to shape _____ and pixel values are scaled to the range _____.', answer: '(784,); [0, 1] (by dividing by 255.0)', type: 'cloze', bloomLevel: 'remember', tags: ['preprocessing', 'normalization'], order: 4 },

  // --- Lesson 1.6: Tensors -- The Data Structures of Deep Learning ---
  { id: 'rc-1.6-1', lessonId: '1.6', prompt: 'Define a tensor and name the tensors of rank 0, 1, 2, and 3 with examples.', answer: 'A tensor is a multidimensional array of numbers. Rank 0: scalar (e.g., 5). Rank 1: vector (e.g., [1, 2, 3]). Rank 2: matrix (e.g., a $28 \\times 28$ image). Rank 3: 3D tensor (e.g., a batch of images with shape (60000, 28, 28)).', type: 'recall', bloomLevel: 'remember', tags: ['tensors', 'ndim'], order: 1 },
  { id: 'rc-1.6-2', lessonId: '1.6', prompt: 'What are the three key attributes of a tensor, and what do they tell you?', answer: '1. **ndim (rank)**: Number of axes. 2. **shape**: Size along each axis (e.g., (60000, 28, 28)). 3. **dtype**: Data type (e.g., float32, int8). Together they specify the tensor\'s structure and memory layout.', type: 'concept', bloomLevel: 'understand', tags: ['tensor-attributes', 'shape', 'dtype'], order: 2 },
  { id: 'rc-1.6-3', lessonId: '1.6', prompt: 'A color video dataset has 1000 videos, each 120 frames, resolution $256 \\times 256$, with 3 color channels. What is the tensor shape?', answer: '(1000, 120, 256, 256, 3) -- a rank-5 tensor. Axes: samples, frames, height, width, channels.', type: 'application', bloomLevel: 'apply', tags: ['tensor-shape', 'video-data'], order: 3 },
  { id: 'rc-1.6-4', lessonId: '1.6', prompt: 'Real-world data stored in tensors is almost always rank _____ through rank _____, and the first axis is typically the _____ axis.', answer: '0; 5; samples (or batch)', type: 'cloze', bloomLevel: 'remember', tags: ['tensors', 'batch-axis'], order: 4 },

  // --- Lesson 1.7: Tensor Operations and Batches ---
  { id: 'rc-1.7-1', lessonId: '1.7', prompt: 'Name four categories of tensor operations used in neural networks.', answer: '1. **Element-wise operations** (relu, addition). 2. **Broadcasting** (operating on tensors of different shapes). 3. **Tensor dot product** (matrix multiplication). 4. **Tensor reshaping** (changing the shape without changing data).', type: 'recall', bloomLevel: 'remember', tags: ['tensor-operations', 'broadcasting'], order: 1 },
  { id: 'rc-1.7-2', lessonId: '1.7', prompt: 'Explain broadcasting: what happens when you add a tensor of shape (32, 10) and a tensor of shape (10,)?', answer: 'The smaller tensor (10,) is virtually replicated along the missing axis to match shape (32, 10). Each of the 32 rows has the (10,) vector added to it element-wise. No actual memory copy occurs -- it is done implicitly for efficiency.', type: 'concept', bloomLevel: 'understand', tags: ['broadcasting', 'tensor-operations'], order: 2 },
  { id: 'rc-1.7-3', lessonId: '1.7', prompt: 'A Dense layer computes $\\text{output} = \\text{relu}(\\text{dot}(W, \\text{input}) + b)$. Given input shape (batch, 784), $W$ shape (784, 128), $b$ shape (128,), what is the output shape?', answer: '(batch, 128). The dot product of (batch, 784) and (784, 128) yields (batch, 128). The bias (128,) is broadcast-added to each sample. relu is applied element-wise, preserving shape.', type: 'application', bloomLevel: 'apply', tags: ['dense-layer', 'matrix-multiply'], order: 3 },
  { id: 'rc-1.7-4', lessonId: '1.7', prompt: 'Training data is split into small _____ (typically 32-128 samples) because processing the entire dataset at once would require too much _____.', answer: 'batches (or mini-batches); memory (GPU memory)', type: 'cloze', bloomLevel: 'remember', tags: ['mini-batch', 'training'], order: 4 },

  // --- Lesson 1.8: Gradient-Based Optimization ---
  { id: 'rc-1.8-1', lessonId: '1.8', prompt: 'What is the gradient of a function, and how is it used to minimize a loss function?', answer: 'The gradient is a vector of partial derivatives indicating the direction of steepest ascent. To minimize loss, we move weights in the opposite direction of the gradient (gradient descent). The learning rate controls step size.', type: 'recall', bloomLevel: 'remember', tags: ['gradient', 'gradient-descent'], order: 1 },
  { id: 'rc-1.8-2', lessonId: '1.8', prompt: 'Explain the difference between batch gradient descent, mini-batch gradient descent, and stochastic gradient descent (SGD).', answer: 'Batch GD computes gradients over the entire dataset per update (slow, accurate). Mini-batch GD uses small random batches (e.g., 32 samples), balancing speed and accuracy. SGD technically means batch size of 1 (noisy but fast updates). In practice, "SGD" often refers to mini-batch GD.', type: 'concept', bloomLevel: 'understand', tags: ['sgd', 'mini-batch', 'batch-gradient-descent'], order: 2 },
  { id: 'rc-1.8-3', lessonId: '1.8', prompt: 'A model uses a learning rate of 0.1 and the gradient is 0.5. If the current weight is 2.0, what is the new weight after one step?', answer: '$w_{\\text{new}} = 2.0 - 0.1 \\times 0.5 = 1.95$. The weight decreases because we subtract $\\text{learning\\_rate} \\times \\text{gradient}$.', type: 'application', bloomLevel: 'apply', tags: ['gradient-descent', 'learning-rate'], order: 3 },
  { id: 'rc-1.8-4', lessonId: '1.8', prompt: 'The key idea behind gradient descent is: adjust each weight by a small amount in the _____ direction of the gradient to _____ the loss.', answer: 'opposite; decrease (or minimize)', type: 'cloze', bloomLevel: 'remember', tags: ['gradient-descent', 'optimization'], order: 4 },

  // --- Lesson 1.9: Backpropagation and the Chain Rule ---
  { id: 'rc-1.9-1', lessonId: '1.9', prompt: 'What is backpropagation and what mathematical principle does it rely on?', answer: 'Backpropagation is the algorithm for efficiently computing the gradient of the loss with respect to every weight in the network. It relies on the chain rule of calculus, which states that the derivative of a composition of functions equals the product of the derivatives of each function.', type: 'recall', bloomLevel: 'remember', tags: ['backpropagation', 'chain-rule'], order: 1 },
  { id: 'rc-1.9-2', lessonId: '1.9', prompt: 'Why is backpropagation computed backward (from loss to inputs) rather than forward?', answer: 'Computing backward allows reusing intermediate results efficiently. Starting from the loss and propagating gradients layer-by-layer backward avoids redundant computation. Each layer only needs the gradient from the layer above it and its own local gradient, making the algorithm O(n) in the number of operations.', type: 'concept', bloomLevel: 'understand', tags: ['backpropagation', 'computational-efficiency'], order: 2 },
  { id: 'rc-1.9-3', lessonId: '1.9', prompt: 'Given $f(x) = \\text{relu}(W_2 \\cdot \\text{relu}(W_1 \\cdot x + b_1) + b_2)$, explain how the chain rule is applied to compute $\\frac{\\partial f}{\\partial W_1}$.', answer: 'Apply the chain rule layer by layer: $\\frac{\\partial f}{\\partial W_1} = \\frac{\\partial f}{\\partial \\text{output}} \\cdot \\frac{\\partial \\text{output}}{\\partial \\text{hidden}} \\cdot \\frac{\\partial \\text{hidden}}{\\partial W_1}$. Each factor is a local derivative: the loss gradient, the derivative through the second relu and $W_2$, then the derivative through the first relu and input $x$. Backpropagation computes these products efficiently right-to-left.', type: 'application', bloomLevel: 'analyze', tags: ['chain-rule', 'gradient-computation'], order: 3 },
  { id: 'rc-1.9-4', lessonId: '1.9', prompt: 'Modern deep learning frameworks compute gradients automatically using a technique called _____ differentiation, which records operations during the forward pass.', answer: 'automatic (specifically, reverse-mode automatic differentiation)', type: 'cloze', bloomLevel: 'remember', tags: ['autodiff', 'computational-graph'], order: 4 },

  // --- Lesson 1.10: Reimplementing the First Example from Scratch ---
  { id: 'rc-1.10-1', lessonId: '1.10', prompt: 'When implementing a Dense layer from scratch, what are the two learnable parameters and what shapes do they have?', answer: 'The weight matrix $W$ of shape (input_size, output_size) and the bias vector $b$ of shape (output_size,). The forward pass computes $\\text{output} = \\text{activation}(\\text{input} \\cdot W + b)$.', type: 'recall', bloomLevel: 'remember', tags: ['dense-layer', 'parameters'], order: 1 },
  { id: 'rc-1.10-2', lessonId: '1.10', prompt: 'In a from-scratch implementation, why must you use a GradientTape (or equivalent) rather than computing gradients analytically?', answer: 'GradientTape records all operations during the forward pass and uses automatic differentiation to compute exact gradients for any computation graph. Manual analytical gradients would be error-prone, tedious, and would need to be rederived for every architecture change.', type: 'concept', bloomLevel: 'understand', tags: ['gradient-tape', 'autodiff'], order: 2 },
  { id: 'rc-1.10-3', lessonId: '1.10', prompt: 'You implement a training loop from scratch and the loss is NaN after a few steps. What are two likely causes?', answer: '1. **Learning rate too high**: Large weight updates cause numerical overflow. 2. **Missing data normalization**: Large input values lead to large activations and gradients that overflow. Other causes: dividing by zero in the loss, or a bug in the gradient computation.', type: 'application', bloomLevel: 'analyze', tags: ['debugging', 'nan-loss'], order: 3 },
  { id: 'rc-1.10-4', lessonId: '1.10', prompt: 'In a manual training loop, after computing gradients, each weight is updated as: $W \\leftarrow W - \\text{\\_\\_\\_\\_\\_} \\times \\text{gradient}$, where the blank is a scalar controlling step size.', answer: 'learning_rate (or lr)', type: 'cloze', bloomLevel: 'remember', tags: ['learning-rate', 'weight-update'], order: 4 },

  // ========== MODULE 2 - GETTING STARTED ==========

  // --- Lesson 2.1: The Deep Learning Framework Landscape ---
  { id: 'rc-2.1-1', lessonId: '2.1', prompt: 'Name the three major deep learning frameworks and the organization behind each.', answer: '1. **TensorFlow** (Google). 2. **PyTorch** (Meta/Facebook). 3. **JAX** (Google). Keras is a high-level API that can run on top of TensorFlow, JAX, or PyTorch.', type: 'recall', bloomLevel: 'remember', tags: ['tensorflow', 'pytorch', 'jax'], order: 1 },
  { id: 'rc-2.1-2', lessonId: '2.1', prompt: 'What is the key difference between "define-and-run" (static graph) and "define-by-run" (eager execution) paradigms?', answer: 'Define-and-run (e.g., early TensorFlow) builds a computational graph first, then executes it -- harder to debug but easier to optimize. Define-by-run (e.g., PyTorch, TF eager mode) executes operations immediately like normal Python -- easier to debug and more flexible, but harder to optimize without JIT compilation.', type: 'concept', bloomLevel: 'understand', tags: ['eager-execution', 'static-graph'], order: 2 },
  { id: 'rc-2.1-3', lessonId: '2.1', prompt: 'You need to prototype a model quickly with minimal boilerplate, then deploy it to mobile and web. Which framework/API combination would you recommend and why?', answer: 'Keras (on TensorFlow backend) is ideal: Keras provides the fastest prototyping with its high-level API, and TensorFlow has mature deployment tools (TFLite for mobile, TF.js for web, TF Serving for production). This combination optimizes for both development speed and deployment flexibility.', type: 'application', bloomLevel: 'apply', tags: ['keras', 'deployment', 'framework-choice'], order: 3 },
  { id: 'rc-2.1-4', lessonId: '2.1', prompt: 'Keras 3 is framework-_____, meaning it can run on TensorFlow, JAX, or PyTorch as interchangeable backends.', answer: 'agnostic', type: 'cloze', bloomLevel: 'remember', tags: ['keras', 'multi-backend'], order: 4 },

  // --- Lesson 2.2: Introduction to Keras -- Layers, Models, and Compilation ---
  { id: 'rc-2.2-1', lessonId: '2.2', prompt: 'What are the three things you must specify when compiling a Keras model?', answer: '1. **Optimizer** (e.g., "adam", "rmsprop") -- how weights are updated. 2. **Loss function** (e.g., "categorical_crossentropy") -- what to minimize. 3. **Metrics** (e.g., ["accuracy"]) -- what to monitor during training and evaluation.', type: 'recall', bloomLevel: 'remember', tags: ['keras-compile', 'optimizer', 'loss'], order: 1 },
  { id: 'rc-2.2-2', lessonId: '2.2', prompt: 'What is a Keras "layer" conceptually, and what makes it the fundamental building block?', answer: 'A layer is a data-processing module that takes one or more tensors as input and outputs one or more tensors. It encapsulates both a state (weights, learned during training) and a computation (the forward pass). Layers are composable: complex models are built by stacking and connecting simple layers.', type: 'concept', bloomLevel: 'understand', tags: ['keras-layers', 'composability'], order: 2 },
  { id: 'rc-2.2-3', lessonId: '2.2', prompt: 'Write the Keras code to create a Sequential model with two Dense layers (64 units with relu, then 10 units with softmax) and compile it for classification.', answer: '```python\nmodel = keras.Sequential([\n    layers.Dense(64, activation="relu"),\n    layers.Dense(10, activation="softmax")\n])\nmodel.compile(optimizer="rmsprop",\n              loss="categorical_crossentropy",\n              metrics=["accuracy"])\n```', type: 'application', bloomLevel: 'apply', tags: ['keras-sequential', 'model-building'], order: 3 },
  { id: 'rc-2.2-4', lessonId: '2.2', prompt: 'In Keras, a layer\'s weights are created lazily the first time it is called on input, a process called weight _____.', answer: 'building (or lazy initialization / automatic shape inference)', type: 'cloze', bloomLevel: 'remember', tags: ['keras-layers', 'weight-building'], order: 4 },

  // --- Lesson 2.3: Training and Validation with fit() ---
  { id: 'rc-2.3-1', lessonId: '2.3', prompt: 'What does `model.fit()` return, and what key information does it contain?', answer: 'It returns a History object whose `.history` attribute is a dictionary mapping metric names (e.g., "loss", "val_loss", "accuracy", "val_accuracy") to lists of per-epoch values. This is used to plot training curves and diagnose overfitting.', type: 'recall', bloomLevel: 'remember', tags: ['model-fit', 'history'], order: 1 },
  { id: 'rc-2.3-2', lessonId: '2.3', prompt: 'Why is a validation set essential during training, and how does it differ from the test set?', answer: 'The validation set monitors generalization during training to tune hyperparameters and detect overfitting. The test set is held out until final evaluation and provides an unbiased performance estimate. Using the test set for tuning would cause information leakage, making the final metric optimistic.', type: 'concept', bloomLevel: 'understand', tags: ['validation-set', 'test-set', 'overfitting'], order: 2 },
  { id: 'rc-2.3-3', lessonId: '2.3', prompt: 'You plot training loss (decreasing) and validation loss (decreasing, then increasing after epoch 10). What does this tell you and what should you do?', answer: 'The model begins overfitting after epoch 10: it keeps memorizing training data while validation performance degrades. Use early stopping to stop training when validation loss stops improving (e.g., EarlyStopping callback with patience), or train for exactly 10 epochs.', type: 'application', bloomLevel: 'analyze', tags: ['overfitting', 'training-curves'], order: 3 },
  { id: 'rc-2.3-4', lessonId: '2.3', prompt: 'The `validation_split=0.2` argument in fit() reserves the last _____% of the training data for validation, sampled _____ shuffling.', answer: '20; before (the split happens before any shuffling, so the last 20% of the array is used)', type: 'cloze', bloomLevel: 'remember', tags: ['validation-split', 'keras'], order: 4 },

  // --- Lesson 2.4: Binary Classification -- Movie Reviews (IMDb) ---
  { id: 'rc-2.4-1', lessonId: '2.4', prompt: 'For binary classification (e.g., IMDb sentiment), what activation and loss function should the output layer use?', answer: 'Activation: **sigmoid** (outputs a single probability between 0 and 1). Loss: **binary_crossentropy** (measures the divergence between predicted probability and binary label).', type: 'recall', bloomLevel: 'remember', tags: ['binary-classification', 'sigmoid'], order: 1 },
  { id: 'rc-2.4-2', lessonId: '2.4', prompt: 'In the IMDb example, reviews are encoded as sequences of integers representing word indices. Why might using multi-hot encoding be preferred over feeding raw sequences to Dense layers?', answer: 'Multi-hot encoding creates a fixed-size vector where each dimension indicates word presence. Dense layers require fixed-size input. Raw integer sequences have variable length and their integer values have no ordinal meaning. Multi-hot encoding loses word order but provides a simple fixed-size representation suitable for Dense layers.', type: 'concept', bloomLevel: 'understand', tags: ['text-encoding', 'multi-hot', 'imdb'], order: 2 },
  { id: 'rc-2.4-3', lessonId: '2.4', prompt: 'Your binary classifier achieves 88% validation accuracy. You add more layers and units, and validation accuracy drops to 85%. Explain what happened.', answer: 'Adding capacity without regularization likely caused overfitting. The larger model memorizes training noise rather than learning generalizable patterns. Solutions: add dropout, reduce model size, add L2 regularization, or get more training data.', type: 'application', bloomLevel: 'analyze', tags: ['overfitting', 'model-capacity'], order: 3 },
  { id: 'rc-2.4-4', lessonId: '2.4', prompt: 'For binary classification, the final Dense layer should have _____ unit(s) with _____ activation.', answer: '1; sigmoid', type: 'cloze', bloomLevel: 'remember', tags: ['binary-classification', 'output-layer'], order: 4 },

  // --- Lesson 2.5: Multiclass Classification -- Newswires (Reuters) ---
  { id: 'rc-2.5-1', lessonId: '2.5', prompt: 'For single-label multiclass classification (e.g., Reuters with 46 classes), what activation and loss function are used?', answer: 'Activation: **softmax** (output is a probability distribution over all classes, summing to 1). Loss: **categorical_crossentropy** (if labels are one-hot) or **sparse_categorical_crossentropy** (if labels are integers).', type: 'recall', bloomLevel: 'remember', tags: ['multiclass-classification', 'softmax'], order: 1 },
  { id: 'rc-2.5-2', lessonId: '2.5', prompt: 'Why is it problematic to have an intermediate layer with fewer units than the number of output classes in a multiclass problem?', answer: 'This creates an "information bottleneck." If you have 46 output classes but an intermediate layer with only 32 units, you compress the representation below what is needed to separate 46 classes. Important distinguishing information gets permanently lost. The bottleneck layer should have at least as many units as the output.', type: 'concept', bloomLevel: 'understand', tags: ['information-bottleneck', 'layer-sizing'], order: 2 },
  { id: 'rc-2.5-3', lessonId: '2.5', prompt: 'You train a 46-class Reuters classifier with integer labels. Should you use `categorical_crossentropy` or `sparse_categorical_crossentropy`? Why?', answer: 'Use `sparse_categorical_crossentropy` because the labels are integers (e.g., 3, 17, 42), not one-hot vectors. Sparse categorical crossentropy accepts integer labels directly, avoiding the need to one-hot encode 46 classes and saving memory.', type: 'application', bloomLevel: 'apply', tags: ['sparse-crossentropy', 'reuters'], order: 3 },
  { id: 'rc-2.5-4', lessonId: '2.5', prompt: 'In multiclass classification, the output layer has $N$ units (one per class) with _____ activation, ensuring outputs are non-negative and sum to _____.', answer: 'softmax; 1', type: 'cloze', bloomLevel: 'remember', tags: ['softmax', 'probability-distribution'], order: 4 },

  // --- Lesson 2.6: Regression -- Predicting House Prices ---
  { id: 'rc-2.6-1', lessonId: '2.6', prompt: 'For a regression task, what activation should the final layer use and what loss function is standard?', answer: 'Final layer: **no activation** (linear output) so the network can output any real value. Loss: **mean squared error (MSE)**, which penalizes large errors quadratically. MAE (mean absolute error) is also common.', type: 'recall', bloomLevel: 'remember', tags: ['regression', 'mse', 'linear-output'], order: 1 },
  { id: 'rc-2.6-2', lessonId: '2.6', prompt: 'Why is feature normalization (zero mean, unit variance) especially important for regression on datasets like Boston Housing where features have very different scales?', answer: 'Features with large values (e.g., tax rate in hundreds) would dominate the loss relative to features with small values (e.g., crime rate). Normalization puts all features on the same scale, making gradient descent converge faster and preventing any single feature from dominating learning.', type: 'concept', bloomLevel: 'understand', tags: ['feature-normalization', 'regression'], order: 2 },
  { id: 'rc-2.6-3', lessonId: '2.6', prompt: 'You have only 404 training samples for a regression task. Why should you use K-fold cross-validation instead of a simple train/val split?', answer: 'With so few samples, a single split would produce a validation set too small and variable to give reliable performance estimates. K-fold CV (e.g., K=4) trains K models on different splits and averages results, giving a more robust and reliable performance estimate despite limited data.', type: 'application', bloomLevel: 'apply', tags: ['k-fold', 'small-data', 'cross-validation'], order: 3 },
  { id: 'rc-2.6-4', lessonId: '2.6', prompt: 'For regression, the metric MAE stands for _____ and computes the average _____ difference between predictions and targets.', answer: 'Mean Absolute Error; absolute (unsigned)', type: 'cloze', bloomLevel: 'remember', tags: ['mae', 'regression-metrics'], order: 4 },

  // --- Lesson 2.7: TensorFlow and PyTorch Fundamentals ---
  { id: 'rc-2.7-1', lessonId: '2.7', prompt: 'In TensorFlow, what is a `tf.Variable` and how does it differ from a `tf.constant`?', answer: 'A `tf.Variable` holds a mutable tensor whose value can be updated (e.g., model weights). A `tf.constant` is immutable once created. Variables are tracked by TensorFlow for automatic differentiation; constants are not trainable.', type: 'recall', bloomLevel: 'remember', tags: ['tensorflow', 'variable', 'constant'], order: 1 },
  { id: 'rc-2.7-2', lessonId: '2.7', prompt: 'Compare how TensorFlow and PyTorch handle automatic differentiation.', answer: 'TensorFlow uses `tf.GradientTape()` context manager: operations inside the tape are recorded, then `tape.gradient()` computes gradients. PyTorch uses `tensor.requires_grad=True` and calling `loss.backward()` computes gradients stored in `tensor.grad`. Both use reverse-mode autodiff but with different APIs.', type: 'concept', bloomLevel: 'understand', tags: ['autodiff', 'gradient-tape', 'pytorch'], order: 2 },
  { id: 'rc-2.7-3', lessonId: '2.7', prompt: 'Write pseudocode for one training step in PyTorch: forward pass, loss, backward, and optimizer step.', answer: '```python\noptimizer.zero_grad()      # Clear old gradients\npredictions = model(inputs) # Forward pass\nloss = loss_fn(predictions, targets)  # Compute loss\nloss.backward()            # Backward pass (compute gradients)\noptimizer.step()           # Update weights\n```', type: 'application', bloomLevel: 'apply', tags: ['pytorch', 'training-step'], order: 3 },
  { id: 'rc-2.7-4', lessonId: '2.7', prompt: 'In PyTorch, gradients accumulate by default, so you must call `optimizer._____()` at the start of each training step to reset them.', answer: 'zero_grad', type: 'cloze', bloomLevel: 'remember', tags: ['pytorch', 'zero-grad'], order: 4 },

  // --- Lesson 2.8: Introduction to JAX ---
  { id: 'rc-2.8-1', lessonId: '2.8', prompt: 'What are the three core transformations that make JAX unique?', answer: '1. **jit** (just-in-time compilation via XLA for speed). 2. **grad** (automatic differentiation of Python functions). 3. **vmap** (automatic vectorization/batching of functions). These compose: you can `jit(vmap(grad(f)))`.', type: 'recall', bloomLevel: 'remember', tags: ['jax', 'jit', 'grad', 'vmap'], order: 1 },
  { id: 'rc-2.8-2', lessonId: '2.8', prompt: 'Why are JAX arrays immutable, and how does this affect programming style?', answer: 'JAX arrays are immutable because JAX is built on functional programming principles, which XLA compilation requires for optimization. Instead of modifying arrays in-place, you create new arrays with updates (e.g., `x = x.at[i].set(v)`). This enables safe parallelism and JIT compilation.', type: 'concept', bloomLevel: 'understand', tags: ['jax', 'immutability', 'functional'], order: 2 },
  { id: 'rc-2.8-3', lessonId: '2.8', prompt: 'You have a function that computes loss for a single sample. How would you use JAX to efficiently compute gradients over a batch?', answer: 'Use `jax.vmap(jax.grad(loss_fn))` to first auto-vectorize the gradient function over a batch dimension, then optionally wrap with `jax.jit` for compilation: `batched_grad = jax.jit(jax.vmap(jax.grad(loss_fn)))`. The transformations compose cleanly.', type: 'application', bloomLevel: 'apply', tags: ['jax', 'vmap', 'grad'], order: 3 },
  { id: 'rc-2.8-4', lessonId: '2.8', prompt: 'JAX uses _____ (a compiler) to optimize and compile Python functions for GPU/TPU execution when you apply the `jit` transformation.', answer: 'XLA (Accelerated Linear Algebra)', type: 'cloze', bloomLevel: 'remember', tags: ['jax', 'xla', 'jit'], order: 4 },

  // --- Lesson 2.9: Problem Type Decision Guide ---
  { id: 'rc-2.9-1', lessonId: '2.9', prompt: 'Match each problem type with its output activation and loss: binary classification, multiclass classification, regression.', answer: 'Binary: sigmoid + binary_crossentropy. Multiclass: softmax + categorical_crossentropy. Regression: no activation (linear) + MSE.', type: 'recall', bloomLevel: 'remember', tags: ['problem-types', 'activation-loss'], order: 1 },
  { id: 'rc-2.9-2', lessonId: '2.9', prompt: 'How do you decide between multiclass single-label and multiclass multi-label classification? What changes in the output layer?', answer: 'Single-label: each sample belongs to exactly one class (use softmax, which sums to 1). Multi-label: each sample can have multiple labels simultaneously (use sigmoid on each output unit independently). Loss changes from categorical_crossentropy to binary_crossentropy applied per label.', type: 'concept', bloomLevel: 'understand', tags: ['multi-label', 'single-label', 'classification'], order: 2 },
  { id: 'rc-2.9-3', lessonId: '2.9', prompt: 'A client wants to predict both the price of a house (continuous) and whether it will sell within 30 days (yes/no). How would you structure the model output?', answer: 'Use a model with two output heads: one regression head (1 unit, no activation, MSE loss) for price prediction, and one binary classification head (1 unit, sigmoid activation, binary_crossentropy loss) for the sell prediction. Total loss is a weighted sum of both losses.', type: 'application', bloomLevel: 'apply', tags: ['multi-output', 'regression', 'classification'], order: 3 },
  { id: 'rc-2.9-4', lessonId: '2.9', prompt: 'For multi-label classification where each sample can have multiple tags, each output unit uses _____ activation (not softmax) because the labels are _____.', answer: 'sigmoid; independent (non-mutually-exclusive)', type: 'cloze', bloomLevel: 'remember', tags: ['multi-label', 'sigmoid'], order: 4 },

  // --- Lesson 2.10: End-to-End Workflow Summary ---
  { id: 'rc-2.10-1', lessonId: '2.10', prompt: 'List the five main steps of the end-to-end deep learning workflow.', answer: '1. **Define the problem** and assemble a dataset. 2. **Choose a metric** to measure success. 3. **Prepare the data** (vectorize, normalize, handle missing values). 4. **Develop a model** (choose architecture, train, evaluate, iterate). 5. **Regularize and tune** to maximize generalization.', type: 'recall', bloomLevel: 'remember', tags: ['workflow', 'end-to-end'], order: 1 },
  { id: 'rc-2.10-2', lessonId: '2.10', prompt: 'Why should you establish a simple baseline before building complex models?', answer: 'A simple baseline (e.g., random predictions, logistic regression) establishes a lower bound. If a complex model does not significantly outperform the baseline, the problem may not require deep learning or the data may be insufficient. Baselines also reveal whether your evaluation pipeline is correct.', type: 'concept', bloomLevel: 'understand', tags: ['baseline', 'model-development'], order: 2 },
  { id: 'rc-2.10-3', lessonId: '2.10', prompt: 'You have collected data for a new classification task. Walk through how you would go from raw data to a trained, evaluated model.', answer: '1. Explore and clean data, split into train/val/test. 2. Vectorize text/images, normalize numerical features. 3. Build a simple model (e.g., 2-layer Dense) as baseline. 4. Train with fit(), monitor val metrics. 5. If underfitting, increase capacity. If overfitting, add regularization (dropout, L2, data augmentation). 6. Once satisfied, evaluate on the test set exactly once.', type: 'application', bloomLevel: 'apply', tags: ['workflow', 'model-development'], order: 3 },
  { id: 'rc-2.10-4', lessonId: '2.10', prompt: 'The final test set should only be used _____ during the entire project, after all tuning is complete, to provide an unbiased estimate of _____.', answer: 'once; generalization performance (real-world performance)', type: 'cloze', bloomLevel: 'remember', tags: ['test-set', 'evaluation'], order: 4 },
  // ========== MODULE 3 - ML FUNDAMENTALS ==========

  // --- Lesson 3.1: Generalization -- The Central Challenge ---
  { id: 'rc-3.1-1', lessonId: '3.1', prompt: 'What is the difference between optimization and generalization in machine learning?', answer: 'Optimization is the process of adjusting a model to perform as well as possible on the **training** data. Generalization is the model\'s ability to perform well on **unseen** data. The central challenge is that optimizing too aggressively on training data (overfitting) hurts generalization.', type: 'concept', bloomLevel: 'understand', tags: ['generalization', 'optimization', 'overfitting'], order: 1 },
  { id: 'rc-3.1-2', lessonId: '3.1', prompt: 'Define underfitting and overfitting. When does the transition from one to the other typically occur during training?', answer: 'Underfitting: the model has not learned enough from the data (both training and validation loss are high). Overfitting: the model memorizes training data (training loss decreases but validation loss increases). The transition occurs when validation loss stops decreasing and starts rising while training loss continues to fall.', type: 'recall', bloomLevel: 'remember', tags: ['underfitting', 'overfitting'], order: 2 },
  { id: 'rc-3.1-3', lessonId: '3.1', prompt: 'You train a model for 100 epochs. Training accuracy is 99.8% but validation accuracy is 78%. Diagnose the problem and propose three concrete solutions.', answer: 'Severe overfitting. Solutions: 1. **Early stopping** -- stop training when validation loss stops improving. 2. **Add dropout** (e.g., 0.5 after Dense layers). 3. **Reduce model capacity** -- fewer layers or smaller layers. Also consider: L2 regularization, data augmentation, or collecting more data.', type: 'application', bloomLevel: 'analyze', tags: ['overfitting', 'regularization', 'diagnosis'], order: 3 },
  { id: 'rc-3.1-4', lessonId: '3.1', prompt: 'The gap between training performance and validation performance is called the _____ gap, and minimizing it while keeping training performance high is the core goal.', answer: 'generalization (or overfitting)', type: 'cloze', bloomLevel: 'remember', tags: ['generalization-gap', 'overfitting'], order: 4 },

  // --- Lesson 3.2: The Nature of Generalization in Deep Learning ---
  { id: 'rc-3.2-1', lessonId: '3.2', prompt: 'Deep learning models have far more parameters than training samples, yet they generalize. Why does this contradict classical statistical learning theory?', answer: 'Classical theory (e.g., VC dimension) predicts that overparameterized models should always overfit. But deep networks have an implicit bias toward simple solutions due to gradient descent dynamics, architecture structure, and the manifold hypothesis -- they find low-complexity functions that generalize despite having the capacity to memorize.', type: 'concept', bloomLevel: 'understand', tags: ['generalization-theory', 'overparameterization'], order: 1 },
  { id: 'rc-3.2-2', lessonId: '3.2', prompt: 'What is the "manifold hypothesis" and why does it matter for deep learning?', answer: 'The manifold hypothesis states that real-world high-dimensional data (images, text) lies on low-dimensional manifolds within the high-dimensional space. Deep learning works because networks only need to fit these low-dimensional structures, not the entire space, making generalization possible despite high dimensionality.', type: 'concept', bloomLevel: 'understand', tags: ['manifold-hypothesis', 'generalization'], order: 2 },
  { id: 'rc-3.2-3', lessonId: '3.2', prompt: 'If you randomly shuffle the labels in a training set, a deep network can still achieve near-zero training loss. What does this demonstrate about memorization vs. generalization?', answer: 'It demonstrates that deep networks have enough capacity to memorize arbitrary input-label mappings (pure noise) without any generalizable pattern. The fact that models generalize on real data is because real data has structure (the manifold hypothesis), not because the models are inherently constrained.', type: 'application', bloomLevel: 'analyze', tags: ['memorization', 'random-labels'], order: 3 },
  { id: 'rc-3.2-4', lessonId: '3.2', prompt: 'Gradient descent has an implicit bias toward finding _____ solutions, which partly explains why overparameterized networks generalize.', answer: 'simple (or low-complexity / smooth)', type: 'cloze', bloomLevel: 'remember', tags: ['implicit-bias', 'gradient-descent'], order: 4 },

  // --- Lesson 3.3: Evaluating ML Models ---
  { id: 'rc-3.3-1', lessonId: '3.3', prompt: 'Name three common evaluation splits and their purposes: training set, validation set, test set.', answer: 'Training set: used to fit the model (learn weights). Validation set: used to tune hyperparameters and monitor overfitting during development. Test set: used once at the end for final unbiased performance evaluation. Never tune on the test set.', type: 'recall', bloomLevel: 'remember', tags: ['train-val-test', 'evaluation'], order: 1 },
  { id: 'rc-3.3-2', lessonId: '3.3', prompt: 'Explain why tuning hyperparameters on the validation set can lead to "information leakage" and how the test set guards against it.', answer: 'Every time you check validation performance and adjust hyperparameters, you leak information about the validation set into the model. Over many iterations, the model becomes indirectly optimized for the validation set and may not generalize beyond it. The test set, never used for decisions, provides a truly unbiased final estimate.', type: 'concept', bloomLevel: 'understand', tags: ['information-leakage', 'hyperparameter-tuning'], order: 2 },
  { id: 'rc-3.3-3', lessonId: '3.3', prompt: 'You have a dataset of 500 samples. Should you use a simple holdout split or K-fold cross-validation? Why?', answer: 'K-fold CV (e.g., K=5). With only 500 samples, a single holdout split (e.g., 80/20) gives a validation set of just 100 samples, which produces high-variance performance estimates. K-fold trains K models on different splits and averages metrics, providing a more reliable and stable estimate.', type: 'application', bloomLevel: 'apply', tags: ['k-fold', 'small-data'], order: 3 },
  { id: 'rc-3.3-4', lessonId: '3.3', prompt: 'In K-fold cross-validation, the data is split into K partitions, and the model is trained K times, each time using a different partition as the _____ set.', answer: 'validation', type: 'cloze', bloomLevel: 'remember', tags: ['k-fold', 'cross-validation'], order: 4 },

  // --- Lesson 3.4: Improving Model Fit ---
  { id: 'rc-3.4-1', lessonId: '3.4', prompt: 'Name three common strategies for improving model fit when the model is underfitting.', answer: '1. **Increase model capacity** (more layers, larger layers). 2. **Reduce regularization** (decrease dropout rate, remove L2 penalty). 3. **Train for more epochs** (the model may not have converged). Also: improve feature engineering or use a better architecture for the data type.', type: 'recall', bloomLevel: 'remember', tags: ['underfitting', 'model-capacity'], order: 1 },
  { id: 'rc-3.4-2', lessonId: '3.4', prompt: 'What is the relationship between learning rate and model fit? What happens if the learning rate is too high or too low?', answer: 'Too high: the optimizer overshoots minima, loss oscillates or diverges -- the model cannot fit the data. Too low: training is extremely slow and may get stuck in poor local minima. The learning rate must be tuned to allow stable convergence. Learning rate schedules and adaptive optimizers (Adam) help.', type: 'concept', bloomLevel: 'understand', tags: ['learning-rate', 'convergence'], order: 2 },
  { id: 'rc-3.4-3', lessonId: '3.4', prompt: 'Your model\'s training loss plateaus at a high value despite training for many epochs. Propose a systematic debugging approach.', answer: '1. Verify data pipeline (correct labels, preprocessing). 2. Try a larger model (the model may lack capacity). 3. Try a different optimizer or smaller learning rate. 4. Check that the loss function matches the task. 5. As a sanity check, overfit on a tiny subset (10 samples) to confirm the model can learn at all.', type: 'application', bloomLevel: 'analyze', tags: ['debugging', 'underfitting'], order: 3 },
  { id: 'rc-3.4-4', lessonId: '3.4', prompt: 'A standard diagnostic for training issues is to try to _____ a small subset of training data (e.g., 10 samples). If the model cannot do this, there is a fundamental problem.', answer: 'overfit (or memorize)', type: 'cloze', bloomLevel: 'remember', tags: ['debugging', 'sanity-check'], order: 4 },

  // --- Lesson 3.5: Improving Generalization (Regularization) ---
  { id: 'rc-3.5-1', lessonId: '3.5', prompt: 'Name four regularization techniques commonly used in deep learning.', answer: '1. **Dropout** (randomly zeroing activations during training). 2. **L2 regularization** (weight decay -- penalizing large weights). 3. **Data augmentation** (creating modified training samples). 4. **Early stopping** (halting training when validation loss rises).', type: 'recall', bloomLevel: 'remember', tags: ['regularization', 'dropout', 'l2'], order: 1 },
  { id: 'rc-3.5-2', lessonId: '3.5', prompt: 'Explain intuitively why dropout works as a regularizer.', answer: 'Dropout randomly disables neurons during each training step, forcing the network to not rely on any single neuron. This prevents co-adaptation of features and encourages redundant representations. Effectively, it trains an ensemble of different sub-networks, whose averaging at test time produces more robust predictions.', type: 'concept', bloomLevel: 'understand', tags: ['dropout', 'regularization'], order: 2 },
  { id: 'rc-3.5-3', lessonId: '3.5', prompt: 'Your image classifier overfits despite dropout=0.5. You have only 2,000 training images. Which additional regularization strategy would be most impactful and why?', answer: 'Data augmentation -- it directly addresses the root cause (insufficient training variety) by creating transformed versions of existing images (rotations, flips, zoom, shifts). This effectively increases training set diversity without collecting new data, and is often the most impactful technique for small image datasets.', type: 'application', bloomLevel: 'apply', tags: ['data-augmentation', 'small-data'], order: 3 },
  { id: 'rc-3.5-4', lessonId: '3.5', prompt: 'L2 regularization adds a penalty of $\\text{\\_\\_\\_\\_\\_} \\cdot \\sum w^2$ to the loss, pushing weights toward _____ and favoring simpler models.', answer: '$\\lambda$ (a small coefficient); zero (smaller values)', type: 'cloze', bloomLevel: 'remember', tags: ['l2-regularization', 'weight-decay'], order: 4 },

  // --- Lesson 3.6: Defining the Task (Universal Workflow Part 1) ---
  { id: 'rc-3.6-1', lessonId: '3.6', prompt: 'What are the key questions to answer before writing any code in the "Define the Task" phase?', answer: '1. What is the input data and what are you trying to predict? 2. What type of problem is this (binary classification, multiclass, regression, etc.)? 3. What metric will measure success? 4. Is the data available and properly labeled? 5. Are there any ethical considerations?', type: 'recall', bloomLevel: 'remember', tags: ['task-definition', 'workflow'], order: 1 },
  { id: 'rc-3.6-2', lessonId: '3.6', prompt: 'Why is choosing the right success metric more important than choosing the right model architecture?', answer: 'The metric defines what "success" means and what the model will optimize toward. A wrong metric can lead to a model that performs well by the metric but fails in practice (e.g., optimizing accuracy on imbalanced data where always predicting the majority class scores 95%). The metric must align with the actual business or research goal.', type: 'concept', bloomLevel: 'understand', tags: ['success-metric', 'task-definition'], order: 2 },
  { id: 'rc-3.6-3', lessonId: '3.6', prompt: 'A medical dataset has 98% negative and 2% positive cases. Why is accuracy a poor metric here? What should you use instead?', answer: 'A model predicting "negative" for everything achieves 98% accuracy but is useless. Better metrics: precision/recall, F1 score, or area under the ROC curve (AUC-ROC). These metrics account for the model\'s ability to identify the rare positive class.', type: 'application', bloomLevel: 'apply', tags: ['class-imbalance', 'metrics'], order: 3 },
  { id: 'rc-3.6-4', lessonId: '3.6', prompt: 'For imbalanced classification problems, _____ and _____ are better metrics than accuracy because they measure performance on the minority class.', answer: 'precision; recall (or F1 score, AUC-ROC)', type: 'cloze', bloomLevel: 'remember', tags: ['precision', 'recall', 'imbalanced-data'], order: 4 },

  // --- Lesson 3.7: Developing a Model (Universal Workflow Part 2) ---
  { id: 'rc-3.7-1', lessonId: '3.7', prompt: 'What is the recommended order for model development? (Three phases)', answer: '1. **Beat a baseline**: Build the simplest reasonable model that outperforms a naive baseline. 2. **Scale up**: Increase model capacity until you overfit the training data. 3. **Regularize and tune**: Add regularization and tune hyperparameters to bridge the gap between training and validation performance.', type: 'recall', bloomLevel: 'remember', tags: ['model-development', 'workflow'], order: 1 },
  { id: 'rc-3.7-2', lessonId: '3.7', prompt: 'Why should you deliberately overfit before regularizing? What does this strategy reveal?', answer: 'If you cannot overfit the training data, there is a fundamental problem (bugs, insufficient capacity, data issues). Overfitting proves the model has enough capacity to learn the patterns. Only then does regularization make sense -- you know you are reducing overfitting, not fighting underfitting.', type: 'concept', bloomLevel: 'understand', tags: ['overfit-first', 'model-development'], order: 2 },
  { id: 'rc-3.7-3', lessonId: '3.7', prompt: 'You start with a 2-layer model (baseline: 72% val accuracy). You scale to 8 layers (train: 98%, val: 75%). What should you do next?', answer: 'The model is overfitting (large train/val gap). Next: add regularization -- dropout, L2, data augmentation, early stopping. Also try reducing the model back to 4-6 layers. Goal: close the gap while keeping val accuracy well above the 72% baseline.', type: 'application', bloomLevel: 'apply', tags: ['regularization', 'model-development'], order: 3 },
  { id: 'rc-3.7-4', lessonId: '3.7', prompt: 'The three-phase model development strategy is: beat a _____, then _____ to overfit, then regularize and _____.', answer: 'baseline; scale up; tune', type: 'cloze', bloomLevel: 'remember', tags: ['model-development', 'workflow'], order: 4 },

  // --- Lesson 3.8: Deploying Your Model (Universal Workflow Part 3) ---
  { id: 'rc-3.8-1', lessonId: '3.8', prompt: 'Name three key considerations when deploying a deep learning model to production.', answer: '1. **Inference speed**: optimize model for latency constraints (quantization, pruning, TFLite). 2. **Monitoring**: track prediction quality, data drift, and failure modes in production. 3. **Maintenance**: plan for retraining as data distribution shifts over time.', type: 'recall', bloomLevel: 'remember', tags: ['deployment', 'production'], order: 1 },
  { id: 'rc-3.8-2', lessonId: '3.8', prompt: 'What is "data drift" (or concept drift) and why is it dangerous for deployed models?', answer: 'Data drift occurs when the real-world data distribution changes over time compared to the training data. The model was optimized for the old distribution and may perform poorly on the new one. Without monitoring, performance degrades silently, potentially causing harmful decisions before anyone notices.', type: 'concept', bloomLevel: 'understand', tags: ['data-drift', 'monitoring'], order: 2 },
  { id: 'rc-3.8-3', lessonId: '3.8', prompt: 'Your deployed model\'s accuracy drops from 95% to 88% over three months. What should you investigate?', answer: '1. Check for data drift: has the input distribution changed? 2. Look for new categories or edge cases not in training data. 3. Check the data pipeline for bugs or upstream changes. 4. If drift is confirmed, collect new labeled data and retrain. 5. Set up automated monitoring to catch future degradation earlier.', type: 'application', bloomLevel: 'analyze', tags: ['model-monitoring', 'retraining'], order: 3 },
  { id: 'rc-3.8-4', lessonId: '3.8', prompt: 'To reduce model size and latency for edge deployment, two common techniques are model _____ (reducing precision) and model _____ (removing unimportant weights).', answer: 'quantization; pruning', type: 'cloze', bloomLevel: 'remember', tags: ['quantization', 'pruning', 'deployment'], order: 4 },

  // --- Lesson 3.9: Regularization Techniques Deep Dive ---
  { id: 'rc-3.9-1', lessonId: '3.9', prompt: 'How does Dropout(0.5) work during training vs. inference?', answer: 'Training: randomly sets 50% of activations to zero at each step, forcing redundant representations. Inference: dropout is disabled, all units are active, and outputs are used as-is (scaling is handled during training by the inverted dropout approach, which divides by (1-rate) during training).', type: 'recall', bloomLevel: 'remember', tags: ['dropout', 'training-vs-inference'], order: 1 },
  { id: 'rc-3.9-2', lessonId: '3.9', prompt: 'Compare L1 and L2 regularization: what effect does each have on the weight distribution?', answer: 'L1 (lasso) adds $|w|$ penalty, driving many weights to exactly zero -- producing sparse models (feature selection). L2 (ridge) adds $w^2$ penalty, pushing weights toward small but non-zero values -- producing smoother models with small, distributed weights. L2 is more common in deep learning.', type: 'concept', bloomLevel: 'understand', tags: ['l1-regularization', 'l2-regularization'], order: 2 },
  { id: 'rc-3.9-3', lessonId: '3.9', prompt: 'You have three regularization options: dropout, L2, and data augmentation. Your image dataset has 500 samples per class. Rank them by expected impact and justify.', answer: '1. **Data augmentation** (highest impact -- directly creates more training diversity from limited images). 2. **Dropout** (prevents co-adaptation of features, works well with augmentation). 3. **L2** (helpful but less impactful than the other two for image tasks with small data). Best results come from combining all three.', type: 'application', bloomLevel: 'analyze', tags: ['regularization-comparison', 'small-data'], order: 3 },
  { id: 'rc-3.9-4', lessonId: '3.9', prompt: 'Dropout acts as an implicit _____ by training many different sub-networks, whose predictions are averaged at test time.', answer: 'ensemble', type: 'cloze', bloomLevel: 'remember', tags: ['dropout', 'ensemble'], order: 4 },

  // --- Lesson 3.10: Module 3 Integration -- The Complete ML Practitioner's Checklist ---
  { id: 'rc-3.10-1', lessonId: '3.10', prompt: 'What is the single most important chart to examine during model development, and what does it reveal?', answer: 'Training loss vs. validation loss over epochs. It reveals: whether the model is learning (both decrease), when overfitting begins (val loss starts rising while train loss drops), the optimal epoch count, and whether the model has converged.', type: 'recall', bloomLevel: 'remember', tags: ['training-curves', 'diagnosis'], order: 1 },
  { id: 'rc-3.10-2', lessonId: '3.10', prompt: 'Summarize the "fit/generalization" tradeoff that every ML practitioner must balance.', answer: 'A model must fit the training data well enough to learn real patterns (avoid underfitting) but not so well that it memorizes noise (avoid overfitting). The sweet spot is where validation performance peaks. Every technique in ML -- architecture choice, regularization, data augmentation, early stopping -- serves this balance.', type: 'concept', bloomLevel: 'understand', tags: ['fit-generalization', 'core-principle'], order: 2 },
  { id: 'rc-3.10-3', lessonId: '3.10', prompt: 'Create a 5-step checklist for systematically going from a new dataset to a deployed model.', answer: '1. Define the problem, choose metrics, explore and clean data. 2. Establish a simple baseline model. 3. Scale up model capacity until overfitting. 4. Regularize and tune hyperparameters to maximize validation performance. 5. Evaluate on test set, deploy, and set up monitoring.', type: 'application', bloomLevel: 'apply', tags: ['workflow', 'checklist'], order: 3 },
  { id: 'rc-3.10-4', lessonId: '3.10', prompt: 'The universal workflow follows: define task -> develop model -> _____ -> deploy and _____.', answer: 'regularize and tune; monitor (maintain)', type: 'cloze', bloomLevel: 'remember', tags: ['universal-workflow', 'deployment'], order: 4 },

  // ========== MODULE 4 - DEEP DIVE INTO PRACTICE ==========

  // --- Lesson 4.1: Keras Model-Building APIs -- Sequential and Functional ---
  { id: 'rc-4.1-1', lessonId: '4.1', prompt: 'When should you use the Sequential API vs. the Functional API in Keras?', answer: 'Sequential: simple linear stacks of layers (single input, single output, no branching). Functional: anything more complex -- multiple inputs/outputs, shared layers, residual connections, or any non-linear topology.', type: 'recall', bloomLevel: 'remember', tags: ['keras', 'sequential', 'functional-api'], order: 1 },
  { id: 'rc-4.1-2', lessonId: '4.1', prompt: 'In the Functional API, what does `x = layers.Dense(64, activation="relu")(x)` mean? What does the `(x)` do?', answer: 'It creates a Dense layer and immediately calls it on tensor x. The Functional API treats layers as functions that transform tensors. Keras traces these tensor-to-tensor calls to build a computation graph. The resulting model knows the full topology from inputs to outputs.', type: 'concept', bloomLevel: 'understand', tags: ['functional-api', 'tensor-graph'], order: 2 },
  { id: 'rc-4.1-3', lessonId: '4.1', prompt: 'You need a model that takes both an image and a text description as input and outputs a classification. Which API should you use and why?', answer: 'The Functional API. Sequential only handles a single input/output. With Functional, define two Input layers (one for image, one for text), process each through appropriate layers (Conv2D for image, Embedding+Dense for text), merge them (e.g., Concatenate), then add classification layers.', type: 'application', bloomLevel: 'apply', tags: ['functional-api', 'multi-input'], order: 3 },
  { id: 'rc-4.1-4', lessonId: '4.1', prompt: 'In the Functional API, a model is created by specifying its _____ and _____ tensors: `Model(inputs=..., outputs=...)`.', answer: 'inputs; outputs', type: 'cloze', bloomLevel: 'remember', tags: ['functional-api', 'model-creation'], order: 4 },

  // --- Lesson 4.2: Subclassing and Mixing Approaches ---
  { id: 'rc-4.2-1', lessonId: '4.2', prompt: 'When subclassing keras.Model, which two methods must you implement?', answer: '`__init__()`: create and store all layers as attributes. `call(inputs)`: define the forward pass computation. The `call()` method is executed when the model is called on data.', type: 'recall', bloomLevel: 'remember', tags: ['model-subclassing', 'keras'], order: 1 },
  { id: 'rc-4.2-2', lessonId: '4.2', prompt: 'A Functional model can display model.summary() with shapes. A subclassed model often cannot. Why?', answer: 'The Functional API builds an explicit static graph that can be inspected before any data flows. Subclassed models define the forward pass as arbitrary Python code in call() (with conditionals, loops), which cannot be statically analyzed without actually running data through it.', type: 'concept', bloomLevel: 'understand', tags: ['subclassing', 'inspectability'], order: 2 },
  { id: 'rc-4.2-3', lessonId: '4.2', prompt: 'When should you choose subclassing over the Functional API?', answer: 'When you need dynamic Python control flow in the forward pass (if/else branching, variable-length loops, recursive structures). This is common in research with novel architectures. For most production applications, the Functional API is preferred because it offers better debugging and serialization.', type: 'application', bloomLevel: 'apply', tags: ['subclassing', 'functional-api'], order: 3 },
  { id: 'rc-4.2-4', lessonId: '4.2', prompt: 'You can mix approaches: use a subclassed layer inside a _____ model, or use Functional sub-models inside a subclassed model.', answer: 'Functional', type: 'cloze', bloomLevel: 'remember', tags: ['mixed-approaches', 'keras'], order: 4 },

  // --- Lesson 4.3: Callbacks and TensorBoard ---
  { id: 'rc-4.3-1', lessonId: '4.3', prompt: 'Name three commonly used Keras callbacks and what each does.', answer: '1. **ModelCheckpoint**: saves model weights (optionally only when validation metric improves). 2. **EarlyStopping**: stops training when a monitored metric stops improving for a specified number of epochs (patience). 3. **ReduceLROnPlateau**: reduces learning rate when a metric plateaus.', type: 'recall', bloomLevel: 'remember', tags: ['callbacks', 'model-checkpoint', 'early-stopping'], order: 1 },
  { id: 'rc-4.3-2', lessonId: '4.3', prompt: 'How does TensorBoard help during model development? What kind of information does it display?', answer: 'TensorBoard is a web-based visualization tool that displays training/validation metrics over time (loss, accuracy curves), model graph architecture, histograms of weights and gradients, embedding projections, and profiling information. It helps diagnose training issues, compare experiments, and understand model behavior.', type: 'concept', bloomLevel: 'understand', tags: ['tensorboard', 'visualization'], order: 2 },
  { id: 'rc-4.3-3', lessonId: '4.3', prompt: 'Configure EarlyStopping and ModelCheckpoint callbacks to save the best model and stop when overfitting begins. Explain each parameter choice.', answer: '```python\ncallbacks = [\n    EarlyStopping(monitor="val_loss", patience=5, restore_best_weights=True),\n    ModelCheckpoint("best_model.keras", monitor="val_loss", save_best_only=True)\n]\n```\nmonitor="val_loss": track validation loss. patience=5: tolerate 5 epochs without improvement. save_best_only: only save when val_loss improves. restore_best_weights: revert to best epoch after stopping.', type: 'application', bloomLevel: 'apply', tags: ['early-stopping', 'model-checkpoint'], order: 3 },
  { id: 'rc-4.3-4', lessonId: '4.3', prompt: 'EarlyStopping with patience=$N$ stops training if the monitored metric does not improve for _____ consecutive epochs.', answer: '$N$', type: 'cloze', bloomLevel: 'remember', tags: ['early-stopping', 'patience'], order: 4 },

  // --- Lesson 4.4: Custom Training Loops ---
  { id: 'rc-4.4-1', lessonId: '4.4', prompt: 'Name three scenarios where you would need a custom training loop instead of model.fit().', answer: '1. **GANs** (two models trained alternately with different losses). 2. **Custom gradient manipulation** (gradient clipping, accumulation, or penalty). 3. **Multiple loss functions with complex weighting** or RL-style training. Also: when you need fine-grained control over each training step.', type: 'recall', bloomLevel: 'remember', tags: ['custom-training', 'advanced-training'], order: 1 },
  { id: 'rc-4.4-2', lessonId: '4.4', prompt: 'What is the role of GradientTape in a custom training loop? How does it relate to the standard fit() method?', answer: 'GradientTape records operations during the forward pass, then computes gradients via automatic differentiation. In fit(), this happens internally. In a custom loop, you explicitly: open a GradientTape, run the forward pass, compute loss, call tape.gradient() for gradients, then optimizer.apply_gradients() to update weights.', type: 'concept', bloomLevel: 'understand', tags: ['gradient-tape', 'custom-training'], order: 2 },
  { id: 'rc-4.4-3', lessonId: '4.4', prompt: 'Instead of writing a full custom loop, you can override the `train_step()` method. What advantage does this offer?', answer: 'You get custom gradient/loss logic while retaining fit()\'s built-in functionality: callbacks, progress bars, epoch management, validation handling, and distributed training support. It is the best of both worlds for most custom training needs.', type: 'application', bloomLevel: 'apply', tags: ['train-step', 'custom-training'], order: 3 },
  { id: 'rc-4.4-4', lessonId: '4.4', prompt: 'In a custom training loop, after computing gradients with tape.gradient(), you apply them to weights using `optimizer._____()`.', answer: 'apply_gradients (called as optimizer.apply_gradients(zip(gradients, model.trainable_weights)))', type: 'cloze', bloomLevel: 'remember', tags: ['optimizer', 'custom-training'], order: 4 },

  // --- Lesson 4.5: Introduction to ConvNets ---
  { id: 'rc-4.5-1', lessonId: '4.5', prompt: 'What two key properties make ConvNets better than Dense layers for image data?', answer: '1. **Translation invariance**: patterns learned at one location can be recognized anywhere in the image. 2. **Spatial hierarchy**: ConvNets learn local patterns first, then combine them into progressively larger and more complex patterns. Dense layers learn global patterns with no spatial structure.', type: 'concept', bloomLevel: 'understand', tags: ['convnets', 'translation-invariance'], order: 1 },
  { id: 'rc-4.5-2', lessonId: '4.5', prompt: 'Conv2D(64, 3) applied to a $28 \\times 28 \\times 1$ input produces output shape $26 \\times 26 \\times 64$. Explain each dimension.', answer: '$26 \\times 26$: spatial dimensions shrink by (kernel_size - 1) with "valid" padding ($28 - 3 + 1 = 26$). 64: the number of output filters/channels. Each filter learns a different $3 \\times 3$ pattern. The 64 is a hyperparameter controlling the layer\'s representational capacity.', type: 'recall', bloomLevel: 'remember', tags: ['conv2d', 'output-shape'], order: 2 },
  { id: 'rc-4.5-3', lessonId: '4.5', prompt: 'Conv2D(32, 3) on input shape (batch, 28, 28, 3). How many trainable parameters does this layer have (including bias)?', answer: '$3 \\times 3 \\times 3 \\times 32 + 32 = 896$. Each filter is $3 \\times 3$ spatial $\\times$ 3 input channels = 27 weights. With 32 filters: $27 \\times 32 = 864$ weights + 32 biases = 896 total parameters.', type: 'application', bloomLevel: 'apply', tags: ['conv2d', 'parameter-count'], order: 3 },
  { id: 'rc-4.5-4', lessonId: '4.5', prompt: 'A convolution layer slides small windows called _____ (or filters) over the input, computing a dot product at each position to produce a _____ map.', answer: 'kernels; feature (or activation)', type: 'cloze', bloomLevel: 'remember', tags: ['convolution', 'kernels'], order: 4 },

  // --- Lesson 4.6: Max Pooling and ConvNet Architecture ---
  { id: 'rc-4.6-1', lessonId: '4.6', prompt: 'What does MaxPooling2D(pool_size=2) do? Given input $26 \\times 26 \\times 64$, what is the output shape?', answer: 'It takes the maximum value in each $2 \\times 2$ window, halving each spatial dimension. Output: $13 \\times 13 \\times 64$. The depth (channels) is unchanged. It provides translation invariance and reduces computation.', type: 'recall', bloomLevel: 'remember', tags: ['max-pooling', 'downsampling'], order: 1 },
  { id: 'rc-4.6-2', lessonId: '4.6', prompt: 'Why is downsampling (via pooling or strided convolutions) essential in ConvNet architectures?', answer: 'Without downsampling, later conv layers still only see small local patches ($3 \\times 3$ of the original). Downsampling increases the effective receptive field so later layers can detect large-scale patterns. It also reduces feature map size (fewer parameters, less memory, faster computation).', type: 'concept', bloomLevel: 'understand', tags: ['downsampling', 'receptive-field'], order: 2 },
  { id: 'rc-4.6-3', lessonId: '4.6', prompt: 'Design a simple ConvNet for $32 \\times 32$ color images with 10 classes. Specify the layer progression.', answer: 'Conv2D(32, 3, relu) -> MaxPool(2) -> Conv2D(64, 3, relu) -> MaxPool(2) -> Conv2D(128, 3, relu) -> Flatten -> Dense(64, relu) -> Dense(10, softmax). Pattern: increasing filters (32->64->128) with pooling to reduce spatial size, then flatten and classify.', type: 'application', bloomLevel: 'apply', tags: ['convnet-architecture', 'design'], order: 3 },
  { id: 'rc-4.6-4', lessonId: '4.6', prompt: 'The standard ConvNet pattern alternates _____ layers (for feature extraction) with _____ layers (for spatial downsampling), ending with Dense layers for classification.', answer: 'Conv2D; MaxPooling2D (or pooling)', type: 'cloze', bloomLevel: 'remember', tags: ['convnet-pattern', 'architecture'], order: 4 },

  // --- Lesson 4.7: Training a ConvNet from Scratch on Small Data ---
  { id: 'rc-4.7-1', lessonId: '4.7', prompt: 'With only 2,000 training images per class and no data augmentation, why does a ConvNet overfit quickly?', answer: 'With limited training variety, the model sees the same images every epoch and memorizes specific pixel patterns and noise unique to the training set. It learns features specific to training images rather than generalizable visual patterns, leading to a large gap between training and validation performance.', type: 'concept', bloomLevel: 'understand', tags: ['small-data', 'overfitting'], order: 1 },
  { id: 'rc-4.7-2', lessonId: '4.7', prompt: 'Name four common image data augmentation transformations and what each does.', answer: '1. **Random horizontal flip**: mirrors the image left-right. 2. **Random rotation**: rotates by a small angle. 3. **Random zoom**: zooms in or out slightly. 4. **Random translation/shift**: moves the image horizontally or vertically. Others: brightness/contrast changes, random crop.', type: 'recall', bloomLevel: 'remember', tags: ['data-augmentation', 'image-transforms'], order: 2 },
  { id: 'rc-4.7-3', lessonId: '4.7', prompt: 'Data augmentation is applied during training but NOT during validation or testing. Why?', answer: 'Augmentation is a regularization technique to increase training diversity and prevent overfitting. During validation/testing, you need unmodified data for honest, reproducible performance measurement. Augmenting test data would add noise to your evaluation.', type: 'application', bloomLevel: 'apply', tags: ['data-augmentation', 'evaluation'], order: 3 },
  { id: 'rc-4.7-4', lessonId: '4.7', prompt: 'Combining data augmentation (at the input level) with _____ (at the representation level) addresses overfitting from two complementary angles.', answer: 'dropout', type: 'cloze', bloomLevel: 'remember', tags: ['data-augmentation', 'dropout'], order: 4 },

  // --- Lesson 4.8: Transfer Learning -- Feature Extraction ---
  { id: 'rc-4.8-1', lessonId: '4.8', prompt: 'What is transfer learning via feature extraction, and why is it effective with small datasets?', answer: 'Feature extraction uses a pretrained ConvNet (e.g., trained on ImageNet) as a fixed feature extractor. You freeze its convolutional layers and only train a new classifier on top. It works because early/middle layers learn general visual features (edges, textures, shapes) that transfer to new tasks, requiring much less data.', type: 'concept', bloomLevel: 'understand', tags: ['transfer-learning', 'feature-extraction'], order: 1 },
  { id: 'rc-4.8-2', lessonId: '4.8', prompt: 'What does "freezing" a layer mean in transfer learning, and how is it done in Keras?', answer: 'Freezing means setting `layer.trainable = False` so its weights are NOT updated during training. The frozen layer acts as a fixed function. To freeze the entire base: `conv_base.trainable = False`. Only the new classifier layers on top will be trained.', type: 'recall', bloomLevel: 'remember', tags: ['freezing', 'keras'], order: 2 },
  { id: 'rc-4.8-3', lessonId: '4.8', prompt: 'You are classifying medical X-rays using a VGG16 model pretrained on ImageNet (natural photos). Which layers would transfer well and which might not?', answer: 'Early layers (edges, textures) transfer well since both domains share low-level features. Late layers (trained to detect cats, cars, etc.) are specific to natural images and may not transfer to medical imaging. You might need to unfreeze and fine-tune the later layers on your medical data.', type: 'application', bloomLevel: 'analyze', tags: ['transfer-learning', 'domain-adaptation'], order: 3 },
  { id: 'rc-4.8-4', lessonId: '4.8', prompt: 'In feature extraction, _____ layers are most transferable because they learn generic visual features, while _____ layers become increasingly task-specific.', answer: 'early (lower); later (higher/deeper)', type: 'cloze', bloomLevel: 'remember', tags: ['feature-hierarchy', 'transfer-learning'], order: 4 },

  // --- Lesson 4.9: Transfer Learning -- Fine-Tuning ---
  { id: 'rc-4.9-1', lessonId: '4.9', prompt: 'What is the two-phase approach to fine-tuning a pretrained model, and why is each phase necessary?', answer: 'Phase 1: Freeze the base, train only the new classifier head until it converges. Phase 2: Unfreeze some top layers of the base and train at a very low learning rate. Phase 1 is needed because the random classifier produces random gradients that would destroy pretrained weights. Phase 2 adapts the base to the new task.', type: 'recall', bloomLevel: 'remember', tags: ['fine-tuning', 'two-phase'], order: 1 },
  { id: 'rc-4.9-2', lessonId: '4.9', prompt: 'Why must you use a very small learning rate (e.g., $10^{-5}$) during fine-tuning?', answer: 'The pretrained weights are already good representations learned from millions of images. A large learning rate would make large, destructive updates that overwrite these useful features. A tiny learning rate makes small, careful adjustments that adapt the features to the new task without losing the pretrained knowledge.', type: 'concept', bloomLevel: 'understand', tags: ['fine-tuning', 'learning-rate'], order: 2 },
  { id: 'rc-4.9-3', lessonId: '4.9', prompt: 'You fine-tune the last 4 convolutional blocks of VGG16 but keep the first 3 frozen. Why not unfreeze everything?', answer: 'Early layers learn generic features (edges, textures) that work for any visual task and do not need adaptation. Unfreezing them risks: (1) overfitting due to too many trainable parameters for limited data, (2) destroying the generic features that make transfer learning effective.', type: 'application', bloomLevel: 'apply', tags: ['selective-unfreezing', 'fine-tuning'], order: 3 },
  { id: 'rc-4.9-4', lessonId: '4.9', prompt: 'Fine-tuning is always done as a _____ step after the new classifier head has already been trained on _____ base features.', answer: 'second (Phase 2); frozen', type: 'cloze', bloomLevel: 'remember', tags: ['fine-tuning', 'workflow'], order: 4 },

  // --- Lesson 4.10: Residual Connections ---
  { id: 'rc-4.10-1', lessonId: '4.10', prompt: 'What is a residual connection and how does it help train very deep networks?', answer: 'A residual connection adds the input of a block to its output: $\\text{output} = f(x) + x$. This creates a "gradient highway" because the gradient of $f(x) + x$ with respect to $x$ includes a $+1$ term, ensuring gradient signal flows even if the block\'s own gradient is vanishingly small. This solves the vanishing gradient problem for deep networks.', type: 'concept', bloomLevel: 'understand', tags: ['residual-connections', 'vanishing-gradients'], order: 1 },
  { id: 'rc-4.10-2', lessonId: '4.10', prompt: 'In a residual block, the input has 32 channels but the block output has 64 channels. How do you handle the shape mismatch for the addition?', answer: 'Use a $1 \\times 1$ Conv2D projection on the skip path: `residual = Conv2D(64, 1)(x)`. This linearly projects 32 channels to 64 channels so the shapes match for element-wise addition. This is called a "projection shortcut."', type: 'application', bloomLevel: 'apply', tags: ['projection-shortcut', 'residual'], order: 2 },
  { id: 'rc-4.10-3', lessonId: '4.10', prompt: 'If the block in a residual connection learns to output all zeros, what does the output become? Why is this significant?', answer: '$\\text{output} = 0 + x = x$ (identity function). This means a residual layer can always "do nothing" if it has nothing useful to add. As a result, adding more layers never hurts -- a deeper network is at least as good as a shallower one. This is why ResNets can successfully train with 100+ layers.', type: 'concept', bloomLevel: 'understand', tags: ['identity-mapping', 'resnet'], order: 3 },
  { id: 'rc-4.10-4', lessonId: '4.10', prompt: 'The ResNet architecture, which introduced residual connections, won the 2015 ImageNet competition with _____ layers, far deeper than any previous network.', answer: '152', type: 'cloze', bloomLevel: 'remember', tags: ['resnet', 'imagenet'], order: 4 },

  // --- Lesson 4.11: Batch Normalization and Separable Convolutions ---
  { id: 'rc-4.11-1', lessonId: '4.11', prompt: 'What does batch normalization do, and where is it typically placed in a layer stack?', answer: 'Batch normalization normalizes each channel to have zero mean and unit variance within each mini-batch, then applies learned scale ($\\gamma$) and shift ($\\beta$) parameters. It is typically placed after a Conv2D or Dense layer and before the activation function. It stabilizes training and allows higher learning rates.', type: 'recall', bloomLevel: 'remember', tags: ['batch-normalization', 'training-stability'], order: 1 },
  { id: 'rc-4.11-2', lessonId: '4.11', prompt: 'Batch normalization normalizes to mean=0, std=1, then learns scale and shift. Why learn to potentially undo the normalization?', answer: 'The optimal activation distribution for each layer may not be exactly mean=0, std=1. The learned $\\gamma$ (scale) and $\\beta$ (shift) parameters allow the network to recover whatever distribution is most useful. Normalization provides a stable starting point; the learned parameters fine-tune from there.', type: 'concept', bloomLevel: 'understand', tags: ['batch-normalization', 'gamma-beta'], order: 2 },
  { id: 'rc-4.11-3', lessonId: '4.11', prompt: 'Standard Conv2D(64, 3) on 32-channel input has 18,432 params. How many does SeparableConv2D(64, 3) have? Why is it more efficient?', answer: 'Depthwise: $3 \\times 3 \\times 32 = 288$. Pointwise: $1 \\times 1 \\times 32 \\times 64 = 2{,}048$. Total: $2{,}336$ (about $8\\times$ fewer). Separable convolutions factorize: depthwise conv handles spatial patterns per channel, pointwise ($1 \\times 1$) conv handles cross-channel mixing. This decoupling is much more parameter-efficient.', type: 'application', bloomLevel: 'apply', tags: ['separable-convolution', 'efficiency'], order: 3 },
  { id: 'rc-4.11-4', lessonId: '4.11', prompt: 'Depthwise separable convolutions factorize a standard convolution into two operations: a _____ convolution (spatial filtering per channel) and a _____ convolution (cross-channel mixing).', answer: 'depthwise; pointwise ($1 \\times 1$)', type: 'cloze', bloomLevel: 'remember', tags: ['separable-convolution', 'factorization'], order: 4 },

  // --- Lesson 4.12: Putting It Together -- Building a Mini Xception Model ---
  { id: 'rc-4.12-1', lessonId: '4.12', prompt: 'Describe the key components of a mini-Xception block and what each contributes.', answer: 'A mini-Xception block combines: SeparableConv2D layers (efficient feature extraction), BatchNormalization (training stability), residual connections (gradient flow for depth), and MaxPooling (spatial downsampling). Together they build a deep, efficient, well-training architecture.', type: 'recall', bloomLevel: 'remember', tags: ['xception', 'architecture'], order: 1 },
  { id: 'rc-4.12-2', lessonId: '4.12', prompt: 'How does a Vision Transformer (ViT) process images differently from a ConvNet?', answer: 'ViT splits images into fixed-size patches (e.g., $16 \\times 16$), flattens each patch into a vector, adds positional embeddings, and processes the sequence of patch embeddings with a Transformer encoder using self-attention. Unlike ConvNets, ViT has no built-in spatial inductive bias (locality, translation invariance) but learns these from data.', type: 'concept', bloomLevel: 'understand', tags: ['vision-transformer', 'vit'], order: 2 },
  { id: 'rc-4.12-3', lessonId: '4.12', prompt: 'You want to determine which components of your model contribute to its accuracy. What technique should you use?', answer: 'An ablation study: systematically remove or replace individual components (e.g., remove residual connections, replace separable convolutions with standard ones, remove batch norm) and measure the impact on validation performance. This is the most reliable way to understand the causal contribution of each component.', type: 'application', bloomLevel: 'analyze', tags: ['ablation-study', 'methodology'], order: 3 },
  { id: 'rc-4.12-4', lessonId: '4.12', prompt: 'The Xception architecture replaces all standard convolutions with _____ convolutions, achieving similar accuracy with far fewer parameters.', answer: 'depthwise separable', type: 'cloze', bloomLevel: 'remember', tags: ['xception', 'separable-convolution'], order: 4 },

  // ========== MODULE 5 - COMPUTER VISION & SEQUENCES ==========

  // --- Lesson 5.1: Visualizing Intermediate Activations ---
  { id: 'rc-5.1-1', lessonId: '5.1', prompt: 'What do intermediate activations reveal when you feed an image through a trained ConvNet?', answer: 'Each channel of each layer\'s activation shows what visual pattern that filter responds to. Early layers show edges and textures. Deeper layers show increasingly abstract features (object parts, compositional patterns). Some channels may be blank (zeros) for a given image, meaning that filter detects a pattern not present.', type: 'recall', bloomLevel: 'remember', tags: ['activations', 'feature-maps'], order: 1 },
  { id: 'rc-5.1-2', lessonId: '5.1', prompt: 'Why do deeper layers produce activations that are harder for humans to interpret visually?', answer: 'Deeper layers encode increasingly abstract, high-level concepts (combinations of lower-level features). These abstract representations are meaningful to the classifier but do not correspond to recognizable visual patterns. They encode "what" rather than "where" -- semantic content rather than spatial detail.', type: 'concept', bloomLevel: 'understand', tags: ['deep-features', 'abstraction'], order: 2 },
  { id: 'rc-5.1-3', lessonId: '5.1', prompt: 'You visualize activations and find that all channels in layer 5 are zero for every input image. What does this indicate?', answer: 'The layer is "dead" -- it produces no useful output. Possible causes: weights collapsed to zero due to dying ReLU (too many negative pre-activations), poor initialization, or the layer was inadvertently disconnected. Debug by checking gradients, using LeakyReLU, or reducing the learning rate.', type: 'application', bloomLevel: 'analyze', tags: ['dead-neurons', 'debugging'], order: 3 },
  { id: 'rc-5.1-4', lessonId: '5.1', prompt: 'To extract intermediate activations in Keras, you create a new Model with the same _____ but with the target layer\'s output as the new output.', answer: 'inputs (input layer)', type: 'cloze', bloomLevel: 'remember', tags: ['keras', 'activation-extraction'], order: 4 },

  // --- Lesson 5.2: Visualizing ConvNet Filters and Grad-CAM ---
  { id: 'rc-5.2-1', lessonId: '5.2', prompt: 'What does Grad-CAM produce and how is it computed?', answer: 'Grad-CAM produces a heatmap showing which spatial regions of the input image most influenced the prediction for a specific class. It computes the gradient of the class score with respect to the final convolutional feature map, averages gradients per channel to get importance weights, then computes a weighted combination of the feature maps.', type: 'recall', bloomLevel: 'remember', tags: ['grad-cam', 'explainability'], order: 1 },
  { id: 'rc-5.2-2', lessonId: '5.2', prompt: 'A Grad-CAM heatmap for "elephant" highlights a waterhole in the background instead of the elephant. What does this reveal?', answer: 'The model learned a spurious correlation: elephants in the training data often appear near waterholes. It uses the background context rather than the actual animal features for prediction. It would likely fail on elephants in unusual settings. This indicates the need for more diverse training data or data augmentation.', type: 'application', bloomLevel: 'analyze', tags: ['spurious-correlation', 'grad-cam'], order: 2 },
  { id: 'rc-5.2-3', lessonId: '5.2', prompt: 'Beyond curiosity, why is model explainability (via techniques like Grad-CAM) practically important?', answer: 'Trust/safety: verify the model looks at relevant features (e.g., tumor, not patient ID). Debugging: understand why the model fails on certain inputs. Compliance: regulated industries (healthcare, finance) may require explainable decisions. Improvement: identifying what the model attends to guides data collection and architecture refinement.', type: 'concept', bloomLevel: 'understand', tags: ['explainability', 'safety'], order: 3 },
  { id: 'rc-5.2-4', lessonId: '5.2', prompt: 'Filter visualization generates a synthetic input image that maximally activates a specific _____, revealing the visual pattern that filter has learned to detect.', answer: 'filter (or channel/neuron)', type: 'cloze', bloomLevel: 'remember', tags: ['filter-visualization', 'feature-visualization'], order: 4 },

  // --- Lesson 5.3: Image Segmentation Fundamentals ---
  { id: 'rc-5.3-1', lessonId: '5.3', prompt: 'What is the difference between semantic segmentation and instance segmentation?', answer: 'Semantic segmentation assigns a class label to every pixel (e.g., "cat," "dog," "background") but does not distinguish between different instances of the same class. Instance segmentation distinguishes individual objects -- two overlapping cats get separate masks. Instance segmentation is a harder task.', type: 'recall', bloomLevel: 'remember', tags: ['semantic-segmentation', 'instance-segmentation'], order: 1 },
  { id: 'rc-5.3-2', lessonId: '5.3', prompt: 'A classification ConvNet reduces $224 \\times 224$ to $7 \\times 7$ feature maps. A segmentation model must output $224 \\times 224$. How is this achieved?', answer: 'Using an encoder-decoder architecture. The encoder (like a classification ConvNet) progressively reduces spatial resolution. The decoder progressively upsamples (via transposed convolutions or upsampling + conv) back to full resolution. Skip connections from encoder to decoder preserve fine spatial details lost during encoding.', type: 'concept', bloomLevel: 'understand', tags: ['encoder-decoder', 'upsampling'], order: 2 },
  { id: 'rc-5.3-3', lessonId: '5.3', prompt: 'Why are skip connections between encoder and decoder critical in segmentation architectures like U-Net?', answer: 'During encoding, fine spatial details (precise edges, boundaries) are progressively lost. Skip connections pass high-resolution feature maps from encoder stages directly to corresponding decoder stages, allowing the decoder to recover precise spatial boundaries. Without them, segmentation masks would be coarse and blurry.', type: 'application', bloomLevel: 'apply', tags: ['skip-connections', 'u-net'], order: 3 },
  { id: 'rc-5.3-4', lessonId: '5.3', prompt: 'In segmentation, the output has the same spatial dimensions as the input, with each pixel assigned to one of N classes. The output shape is $(H, W, \\text{\\_\\_\\_\\_\\_})$.', answer: '$N$ (number of classes)', type: 'cloze', bloomLevel: 'remember', tags: ['segmentation', 'output-shape'], order: 4 },

  // --- Lesson 5.4: Segment Anything Model (SAM) ---
  { id: 'rc-5.4-1', lessonId: '5.4', prompt: 'What makes SAM a "foundation model" for segmentation?', answer: 'SAM was trained on over 1 billion masks from 11 million images, giving it extremely broad visual understanding. It can segment virtually any object in any image without task-specific training, prompted by points, bounding boxes, or text. It generalizes to novel object types and domains zero-shot.', type: 'recall', bloomLevel: 'remember', tags: ['sam', 'foundation-model'], order: 1 },
  { id: 'rc-5.4-2', lessonId: '5.4', prompt: 'Describe the three components of SAM\'s architecture and their roles.', answer: '1. **Image encoder**: A ViT that produces image embeddings (runs once per image). 2. **Prompt encoder**: Encodes user prompts (points, boxes, text) into embeddings. 3. **Mask decoder**: A lightweight Transformer that combines image and prompt embeddings to predict segmentation masks. The heavy computation is in the image encoder; the mask decoder is fast.', type: 'concept', bloomLevel: 'understand', tags: ['sam', 'architecture'], order: 2 },
  { id: 'rc-5.4-3', lessonId: '5.4', prompt: 'You need to segment cells in microscope images, a domain SAM was not specifically trained on. How would you approach this?', answer: 'First try SAM zero-shot with point or box prompts on your microscope images -- it often generalizes surprisingly well. If accuracy is insufficient, fine-tune SAM\'s mask decoder on a small labeled microscope dataset while keeping the image encoder frozen. This adapts SAM to the new domain with minimal data.', type: 'application', bloomLevel: 'apply', tags: ['sam', 'domain-adaptation'], order: 3 },
  { id: 'rc-5.4-4', lessonId: '5.4', prompt: 'SAM uses a _____ prompt interface, accepting points, boxes, or text to specify what to segment, making it interactive and flexible.', answer: 'promptable (or multi-modal prompt)', type: 'cloze', bloomLevel: 'remember', tags: ['sam', 'prompting'], order: 4 },

  // --- Lesson 5.5: Object Detection -- Single-Stage vs. Two-Stage ---
  { id: 'rc-5.5-1', lessonId: '5.5', prompt: 'How does object detection differ from image classification and image segmentation?', answer: 'Classification: one label per image. Segmentation: per-pixel class labels. Detection: locates multiple objects by predicting bounding boxes, each with a class label and confidence score. Detection answers both "what" and "where" at the object level.', type: 'recall', bloomLevel: 'remember', tags: ['object-detection', 'task-comparison'], order: 1 },
  { id: 'rc-5.5-2', lessonId: '5.5', prompt: 'Compare single-stage and two-stage object detectors in terms of speed and accuracy.', answer: 'Two-stage (e.g., Faster R-CNN): first proposes candidate regions, then classifies each. Higher accuracy but slower. Single-stage (e.g., YOLO, SSD): predicts boxes and classes in one pass directly from the feature map. Faster (real-time) but historically slightly less accurate, though modern single-stage detectors have largely closed the gap.', type: 'concept', bloomLevel: 'understand', tags: ['single-stage', 'two-stage', 'yolo'], order: 2 },
  { id: 'rc-5.5-3', lessonId: '5.5', prompt: 'You are building a real-time object detector for a self-driving car. Should you use YOLO or Faster R-CNN? Justify your choice.', answer: 'YOLO (single-stage) because real-time performance is critical for safety. YOLO processes frames in milliseconds, enabling real-time decision-making. Faster R-CNN is more accurate but too slow for real-time automotive applications. Modern YOLO versions achieve sufficient accuracy for practical deployment.', type: 'application', bloomLevel: 'apply', tags: ['yolo', 'real-time-detection'], order: 3 },
  { id: 'rc-5.5-4', lessonId: '5.5', prompt: 'Non-Maximum Suppression (NMS) is a post-processing step in object detection that removes _____ bounding boxes for the same object, keeping only the highest-confidence one.', answer: 'overlapping (duplicate/redundant)', type: 'cloze', bloomLevel: 'remember', tags: ['nms', 'post-processing'], order: 4 },

  // --- Lesson 5.6: Training a YOLO Model ---
  { id: 'rc-5.6-1', lessonId: '5.6', prompt: 'How does YOLO divide an image for detection? What does each grid cell predict?', answer: 'YOLO divides the image into an $S \\times S$ grid. Each cell predicts $B$ bounding boxes (each with $x, y$, width, height, confidence) and $C$ class probabilities. A cell is responsible for detecting objects whose center falls within it. This spatial division enables single-pass prediction.', type: 'recall', bloomLevel: 'remember', tags: ['yolo', 'grid-detection'], order: 1 },
  { id: 'rc-5.6-2', lessonId: '5.6', prompt: 'What are "anchor boxes" in YOLO, and why are they used?', answer: 'Anchor boxes are predefined box shapes (aspect ratios and sizes) that serve as reference templates. Instead of predicting boxes from scratch, the model predicts offsets from these anchors. This is easier to learn because most objects match common shapes. Anchors are typically determined by clustering the training data bounding boxes.', type: 'concept', bloomLevel: 'understand', tags: ['anchor-boxes', 'yolo'], order: 2 },
  { id: 'rc-5.6-3', lessonId: '5.6', prompt: 'You fine-tune a pretrained YOLO model on a custom dataset of 500 labeled images. What steps are needed?', answer: '1. Format annotations (bounding boxes + class labels) in YOLO format. 2. Configure the model for your number of classes. 3. Freeze the backbone initially, train the detection head. 4. Optionally unfreeze and fine-tune at a lower learning rate. 5. Apply data augmentation (mosaic, random flip, scale). 6. Evaluate using mAP (mean Average Precision).', type: 'application', bloomLevel: 'apply', tags: ['yolo', 'fine-tuning'], order: 3 },
  { id: 'rc-5.6-4', lessonId: '5.6', prompt: 'The standard metric for object detection accuracy is _____, which averages precision across different IoU (Intersection over Union) thresholds and classes.', answer: 'mAP (mean Average Precision)', type: 'cloze', bloomLevel: 'remember', tags: ['map', 'detection-metrics'], order: 4 },

  // --- Lesson 5.7: Timeseries Forecasting Fundamentals ---
  { id: 'rc-5.7-1', lessonId: '5.7', prompt: 'Why must timeseries data be split chronologically rather than randomly?', answer: 'Random splitting allows the model to see future data during training (temporal leakage). The model could learn to "cheat" by using future information that would not be available in real-time prediction. Always split chronologically: train on past data, validate/test on future data.', type: 'recall', bloomLevel: 'remember', tags: ['timeseries', 'data-leakage'], order: 1 },
  { id: 'rc-5.7-2', lessonId: '5.7', prompt: 'What is a "naive baseline" for timeseries forecasting, and why is it important to establish?', answer: 'A naive baseline predicts that the next value equals the current value (persistence forecast). It is important because timeseries often have high autocorrelation, making this surprisingly competitive. Any useful model must significantly outperform this baseline. If it does not, the task may not benefit from ML.', type: 'concept', bloomLevel: 'understand', tags: ['baseline', 'timeseries'], order: 2 },
  { id: 'rc-5.7-3', lessonId: '5.7', prompt: 'You have hourly temperature data for 5 years and want to predict the next 24 hours. How would you structure the input/output for supervised learning?', answer: 'Use a sliding window approach: each input is a sequence of past observations (e.g., the last 168 hours / 7 days), and the target is the next 24 hours. Slide the window across the training period to generate many (input, target) pairs. Normalize the data. Split chronologically into train/val/test.', type: 'application', bloomLevel: 'apply', tags: ['sliding-window', 'timeseries'], order: 3 },
  { id: 'rc-5.7-4', lessonId: '5.7', prompt: 'In timeseries forecasting, the length of past observations fed as input is called the _____ window, and the number of future steps to predict is the _____ horizon.', answer: 'lookback (or context/history); forecast (or prediction)', type: 'cloze', bloomLevel: 'remember', tags: ['lookback-window', 'forecast-horizon'], order: 4 },

  // --- Lesson 5.8: 1D Convolutions for Timeseries ---
  { id: 'rc-5.8-1', lessonId: '5.8', prompt: 'How does Conv1D process timeseries data? What dimension does it operate on?', answer: 'Conv1D slides a 1D kernel along the time dimension. Input shape: (batch, timesteps, features). The kernel detects local temporal patterns (e.g., a sharp rise followed by a dip). It is analogous to Conv2D detecting local spatial patterns in images, but operates on one dimension (time) instead of two (height, width).', type: 'recall', bloomLevel: 'remember', tags: ['conv1d', 'timeseries'], order: 1 },
  { id: 'rc-5.8-2', lessonId: '5.8', prompt: 'What are the advantages and limitations of Conv1D compared to RNNs for timeseries?', answer: 'Advantages: Conv1D is faster to train (parallelizable), simpler, and good at detecting local patterns. Limitations: Conv1D has a fixed receptive field and struggles with long-range dependencies unless stacked deeply or combined with dilated convolutions. RNNs theoretically handle arbitrary-length dependencies.', type: 'concept', bloomLevel: 'understand', tags: ['conv1d', 'rnn', 'comparison'], order: 2 },
  { id: 'rc-5.8-3', lessonId: '5.8', prompt: 'Design a 1D CNN architecture for classifying timeseries with 200 timesteps and 5 features into 3 classes.', answer: 'Input: (batch, 200, 5). Conv1D(32, 7, relu) -> MaxPool1D(2) -> Conv1D(64, 5, relu) -> GlobalMaxPool1D -> Dense(32, relu) -> Dense(3, softmax). Pattern: extract local temporal features with increasing filters, pool to reduce length, global pool to fixed vector, then classify.', type: 'application', bloomLevel: 'apply', tags: ['conv1d', 'architecture-design'], order: 3 },
  { id: 'rc-5.8-4', lessonId: '5.8', prompt: '_____ convolutions use kernels with gaps between elements, exponentially increasing the receptive field without increasing parameters.', answer: 'Dilated (or atrous)', type: 'cloze', bloomLevel: 'remember', tags: ['dilated-convolution', 'receptive-field'], order: 4 },

  // --- Lesson 5.9: Recurrent Neural Networks (RNNs) ---
  { id: 'rc-5.9-1', lessonId: '5.9', prompt: 'What is the fundamental idea of an RNN, and how does it differ from a feedforward network?', answer: 'An RNN processes sequences one timestep at a time, maintaining a hidden state that carries information from previous timesteps. Unlike feedforward networks that process fixed-size inputs independently, RNNs have a recurrent connection: the hidden state at time t depends on the input at time t AND the hidden state at time t-1.', type: 'recall', bloomLevel: 'remember', tags: ['rnn', 'hidden-state'], order: 1 },
  { id: 'rc-5.9-2', lessonId: '5.9', prompt: 'Why does SimpleRNN struggle with long sequences while LSTM handles them better?', answer: 'SimpleRNN suffers from vanishing gradients: as gradients are backpropagated through many timesteps, they shrink exponentially, making it impossible to learn long-range dependencies. LSTM uses gating mechanisms (forget gate, input gate, output gate) and a cell state that acts as a gradient highway, allowing information to flow across many timesteps.', type: 'concept', bloomLevel: 'understand', tags: ['lstm', 'vanishing-gradients'], order: 2 },
  { id: 'rc-5.9-3', lessonId: '5.9', prompt: 'You need to stack two LSTM layers. The first LSTM must use return_sequences=True. Why?', answer: 'The second LSTM expects a full sequence as input (a 3D tensor with timesteps). If the first LSTM uses return_sequences=False (default), it outputs only the last hidden state (a 2D tensor), which cannot be fed to another recurrent layer. return_sequences=True outputs the hidden state at every timestep.', type: 'application', bloomLevel: 'apply', tags: ['lstm', 'return-sequences'], order: 3 },
  { id: 'rc-5.9-4', lessonId: '5.9', prompt: 'LSTM stands for _____ and uses three gates -- forget, input, and output -- to control information flow through the cell _____.', answer: 'Long Short-Term Memory; state', type: 'cloze', bloomLevel: 'remember', tags: ['lstm', 'gates'], order: 4 },

  // --- Lesson 5.10: Advanced RNN Techniques ---
  { id: 'rc-5.10-1', lessonId: '5.10', prompt: 'What is a bidirectional RNN, and when should you NOT use it?', answer: 'A bidirectional RNN processes the sequence both forward and backward, concatenating the hidden states. This captures context from both past and future. Do NOT use it for timeseries forecasting, because during prediction the future is unavailable. Only use when the complete sequence is available (e.g., text classification, speech recognition).', type: 'recall', bloomLevel: 'remember', tags: ['bidirectional', 'rnn'], order: 1 },
  { id: 'rc-5.10-2', lessonId: '5.10', prompt: 'What is recurrent dropout and how does it differ from standard dropout applied to RNN outputs?', answer: 'Recurrent dropout applies the same dropout mask at every timestep to the recurrent connections (hidden-to-hidden weights), not just the input-to-hidden connections. Standard dropout on outputs would use different masks per timestep. Recurrent dropout prevents overfitting of temporal patterns without destroying the temporal signal.', type: 'concept', bloomLevel: 'understand', tags: ['recurrent-dropout', 'regularization'], order: 2 },
  { id: 'rc-5.10-3', lessonId: '5.10', prompt: 'For a sentiment analysis task on full movie reviews, would you use a unidirectional or bidirectional LSTM? Why?', answer: 'Bidirectional LSTM. The entire review text is available at inference time, so we can leverage both past and future context. A bidirectional model captures that "not bad" (where "not" comes before "bad") means positive, regardless of reading direction. This typically improves text classification accuracy.', type: 'application', bloomLevel: 'apply', tags: ['bidirectional', 'sentiment-analysis'], order: 3 },
  { id: 'rc-5.10-4', lessonId: '5.10', prompt: 'The GRU (Gated Recurrent Unit) is a simplified alternative to LSTM with _____ gates instead of three, offering similar performance with fewer parameters.', answer: 'two (reset gate and update gate)', type: 'cloze', bloomLevel: 'remember', tags: ['gru', 'gates'], order: 4 },

  // --- Lesson 5.11: Timeseries Best Practices and Going Further ---
  { id: 'rc-5.11-1', lessonId: '5.11', prompt: 'Why is attention beneficial for timeseries models compared to RNNs with fixed-size hidden states?', answer: 'RNNs compress all past information into a single fixed-size hidden state vector (information bottleneck). Attention mechanisms allow the model to directly access any past timestep with learned importance weights, recalling distant events with full fidelity instead of relying on the compressed hidden state.', type: 'concept', bloomLevel: 'understand', tags: ['attention', 'timeseries'], order: 1 },
  { id: 'rc-5.11-2', lessonId: '5.11', prompt: 'Name three best practices for building timeseries forecasting models.', answer: '1. Always compare against a naive baseline (persistence forecast). 2. Split data chronologically (never randomly). 3. Normalize features carefully, fitting statistics only on the training split. Also: use multiple metrics (MAE, RMSE), try both CNN and RNN approaches, and consider seasonality/trend decomposition.', type: 'recall', bloomLevel: 'remember', tags: ['timeseries', 'best-practices'], order: 2 },
  { id: 'rc-5.11-3', lessonId: '5.11', prompt: 'Your timeseries model performs well on the validation set (last 6 months of 2023) but poorly when deployed in January 2024. What might explain this?', answer: 'Distribution shift or non-stationarity: the data pattern changed between validation and deployment periods. Possible causes: seasonality not captured, external event changed the pattern, or the validation period was too similar to training. Solutions: use longer validation windows, include seasonal features, and set up monitoring for drift.', type: 'application', bloomLevel: 'analyze', tags: ['distribution-shift', 'deployment'], order: 3 },
  { id: 'rc-5.11-4', lessonId: '5.11', prompt: 'Modern Transformer-based timeseries models have largely replaced RNNs because self-attention processes all timesteps in _____ rather than sequentially, enabling better parallelism and long-range modeling.', answer: 'parallel', type: 'cloze', bloomLevel: 'remember', tags: ['transformer', 'timeseries'], order: 4 },

  // --- Lesson 5.12: Module 5 Integration -- Computer Vision and Sequences Recap ---
  { id: 'rc-5.12-1', lessonId: '5.12', prompt: 'Match each computer vision task to its output format: classification, segmentation, detection.', answer: 'Classification: single class label (or probability vector) per image. Segmentation: class label for every pixel (output same spatial size as input). Detection: list of bounding boxes, each with class label and confidence score.', type: 'recall', bloomLevel: 'remember', tags: ['vision-tasks', 'taxonomy'], order: 1 },
  { id: 'rc-5.12-2', lessonId: '5.12', prompt: 'Compare the strengths of Conv1D, RNNs, and Transformers for sequence modeling.', answer: 'Conv1D: fast, good at local patterns, limited receptive field. RNNs (LSTM/GRU): handle variable-length sequences, capture temporal order, but slow (sequential) and struggle with very long dependencies. Transformers: parallel processing, excellent long-range modeling via self-attention, but quadratic memory in sequence length.', type: 'concept', bloomLevel: 'understand', tags: ['sequence-models', 'comparison'], order: 2 },
  { id: 'rc-5.12-3', lessonId: '5.12', prompt: 'You need to choose a model for each task: (a) classify satellite images, (b) detect cars in dashcam video, (c) forecast next-day stock prices. What architecture for each?', answer: '(a) ConvNet (or ViT) with transfer learning for image classification. (b) YOLO or similar single-stage detector for real-time bounding box detection. (c) 1D CNN, LSTM, or Transformer for timeseries forecasting. Each task matches a specific problem type with appropriate architectures.', type: 'application', bloomLevel: 'apply', tags: ['architecture-selection', 'problem-types'], order: 3 },
  { id: 'rc-5.12-4', lessonId: '5.12', prompt: 'The key architectural pattern in both image segmentation (U-Net) and sequence-to-sequence models is the _____ structure with skip connections.', answer: 'encoder-decoder', type: 'cloze', bloomLevel: 'remember', tags: ['encoder-decoder', 'architecture-pattern'], order: 4 },


  // --- Module 1 extra cards (5th card per lesson) ---
  { id: 'rc-1.1-5', lessonId: '1.1', prompt: 'What is the fundamental paradigm shift that distinguishes ML from classical programming?', answer: 'In classical programming, humans write explicit rules that transform data into answers. In ML, the paradigm is reversed: the system receives data AND answers, then learns the rules. The programmer\'s role shifts from rule-writer to data curator and architecture designer.', type: 'concept', bloomLevel: 'analyze', tags: ["paradigm-shift","ml-vs-programming"], order: 5 },
  { id: 'rc-1.2-5', lessonId: '1.2', prompt: 'If deep learning models learn representations automatically, does that mean feature engineering is obsolete?', answer: 'Not entirely. Deep learning reduces the need for manual feature engineering, but domain knowledge still helps: choosing the right input representation, data preprocessing, and architecture priors (e.g., ConvNets for images) are all forms of soft feature engineering. For tabular data especially, hand-crafted features often still outperform end-to-end deep learning.', type: 'concept', bloomLevel: 'analyze', tags: ["feature-engineering","representations"], order: 5 },
  { id: 'rc-1.3-5', lessonId: '1.3', prompt: 'How does backpropagation use the loss function\'s gradient to improve the model?', answer: 'Backpropagation computes the gradient of the loss with respect to every weight in the network via the chain rule. The gradient tells the optimizer the direction of steepest increase in loss. By updating weights in the opposite direction (gradient descent), each weight is adjusted to reduce the loss. The learning rate controls the step size.', type: 'concept', bloomLevel: 'understand', tags: ["backpropagation","gradient-descent"], order: 5 },
  { id: 'rc-1.4-5', lessonId: '1.4', prompt: 'Why does Chollet argue that scaling up current deep learning approaches will NOT lead to AGI?', answer: 'Current DL performs local generalization (interpolation between training examples) but lacks extreme generalization (adapting to fundamentally novel situations). Intelligence requires on-the-fly abstraction and reasoning, not just pattern matching on massive data. Increasing model size and data improves local generalization but does not bridge the gap to genuine understanding or adaptability.', type: 'concept', bloomLevel: 'analyze', tags: ["agi","limitations","scaling"], order: 5 },
  { id: 'rc-1.5-5', lessonId: '1.5', prompt: 'In the MNIST example, the model uses categorical crossentropy as the loss function. Why is this appropriate instead of mean squared error?', answer: 'Categorical crossentropy is designed for classification: it heavily penalizes confident wrong predictions and rewards the correct class probability approaching 1. MSE treats output probabilities as continuous values and penalizes all deviations equally, which does not match the structure of a probability distribution over classes. Crossentropy leads to faster, more stable training for classification.', type: 'concept', bloomLevel: 'understand', tags: ["crossentropy","loss-function","classification"], order: 5 },
  { id: 'rc-1.6-5', lessonId: '1.6', prompt: 'A video dataset has shape (1000, 30, 224, 224, 3). What is the rank and what does each axis represent?', answer: 'Rank: 5 (five axes). Axis 0 (1000): number of video clips. Axis 1 (30): frames per clip (timesteps). Axis 2 (224): frame height. Axis 3 (224): frame width. Axis 4 (3): color channels (RGB). Video is one of the highest-rank data types commonly encountered in deep learning.', type: 'application', bloomLevel: 'apply', tags: ["tensor","shape","video"], order: 5 },
  { id: 'rc-1.7-5', lessonId: '1.7', prompt: 'Why is the dot product (tensor product) considered the most important tensor operation in neural networks?', answer: 'The dot product is how Dense layers combine their inputs: $\\text{output} = \\text{dot}(\\text{input}, W) + b$. It is the fundamental operation that lets each output neuron compute a weighted sum of all inputs. In matrix form, it maps an input vector to a new vector in a different space. Without dot products, layers could not mix information across features.', type: 'concept', bloomLevel: 'understand', tags: ["dot-product","dense-layer"], order: 5 },
  { id: 'rc-1.8-5', lessonId: '1.8', prompt: 'Explain the difference between batch gradient descent, stochastic gradient descent, and mini-batch gradient descent.', answer: 'Batch GD computes gradients on the entire dataset per update (slow, exact). Stochastic GD uses a single sample per update (fast, very noisy). Mini-batch GD uses a small batch (e.g., 32 samples) per update -- the practical standard. Mini-batch balances noise (which helps escape local minima) with stability (averaging over multiple samples).', type: 'recall', bloomLevel: 'remember', tags: ["sgd","mini-batch","batch-gd"], order: 5 },
  { id: 'rc-1.9-5', lessonId: '1.9', prompt: 'If a neural network has 10 layers, how many times is the chain rule applied during backpropagation?', answer: 'The chain rule is applied at every layer boundary, so approximately 10 times (once per layer). Each application multiplies the incoming gradient by the local derivative of that layer\'s operation. This chaining is what allows the gradient signal to flow from the loss all the way back to the first layer\'s weights.', type: 'application', bloomLevel: 'apply', tags: ["chain-rule","backpropagation"], order: 5 },
  { id: 'rc-1.10-5', lessonId: '1.10', prompt: 'After reimplementing MNIST from scratch, what is the single most important thing that differentiates using a framework like Keras from manual implementation?', answer: 'Automatic differentiation. Manually deriving and implementing gradients for each layer is tedious, error-prone, and must be redone for every architecture change. Keras/TF/PyTorch compute gradients automatically for any differentiable computation, letting you focus on model design rather than calculus. This is what makes rapid experimentation practical.', type: 'concept', bloomLevel: 'analyze', tags: ["autodiff","frameworks","abstraction"], order: 5 },

  // --- Module 2 extra cards (5th card per lesson) ---
  { id: 'rc-2.1-5', lessonId: '2.1', prompt: 'What does it mean that Keras is \'backend-agnostic\', and why is this valuable?', answer: 'Keras code can run on TensorFlow, PyTorch, or JAX without modification. This is valuable because you can write model code once and deploy it on whichever backend suits your needs: TF for production serving, JAX for research with JIT compilation, PyTorch for ecosystem compatibility. It also future-proofs your code against framework shifts.', type: 'concept', bloomLevel: 'understand', tags: ["keras","backend-agnostic"], order: 5 },
  { id: 'rc-2.2-5', lessonId: '2.2', prompt: 'A Dense layer with 128 units applied to input shape (batch, 784) creates how many trainable parameters?', answer: '$784 \\times 128 + 128 = 100{,}480$ parameters. The weight matrix has shape $(784, 128) = 100{,}352$ values, plus 128 bias values (one per output unit). This illustrates why Dense layers on large inputs create many parameters -- and why ConvNets with weight sharing are preferred for images.', type: 'application', bloomLevel: 'apply', tags: ["dense-layer","parameter-count"], order: 5 },
  { id: 'rc-2.3-5', lessonId: '2.3', prompt: 'What is the difference between the validation loss used during training and the test loss used for final evaluation?', answer: 'Validation loss is monitored during training to make decisions (early stopping, hyperparameter tuning) -- this means it indirectly influences the model. Test loss is computed once at the end on data never used for any decision-making, giving a truly unbiased estimate of generalization. If you tune based on test loss, it becomes another validation set.', type: 'concept', bloomLevel: 'understand', tags: ["validation","test-set","evaluation"], order: 5 },
  { id: 'rc-2.4-5', lessonId: '2.4', prompt: 'For binary classification, what threshold is typically applied to the sigmoid output, and when might you change it?', answer: 'The default threshold is 0.5: output >= 0.5 predicts positive, < 0.5 predicts negative. You might lower the threshold (e.g., 0.3) when false negatives are costly (medical diagnosis: better to flag more patients for review). You might raise it when false positives are costly (spam filter: don\'t want to block legitimate email).', type: 'application', bloomLevel: 'apply', tags: ["sigmoid","threshold","binary-classification"], order: 5 },
  { id: 'rc-2.5-5', lessonId: '2.5', prompt: 'What is the practical difference between using `categorical_crossentropy` with one-hot labels vs `sparse_categorical_crossentropy` with integer labels?', answer: 'They compute the same loss value. The difference is label format: categorical_crossentropy expects one-hot vectors (e.g., [0,0,1,0,...]), sparse_categorical_crossentropy expects integer labels (e.g., 2). Sparse is more memory-efficient for many classes (one int vs. N floats per sample). Use whichever matches your data format.', type: 'recall', bloomLevel: 'remember', tags: ["crossentropy","label-encoding"], order: 5 },
  { id: 'rc-2.6-5', lessonId: '2.6', prompt: 'In $K$-fold cross-validation with $K=4$ on 400 samples, how many models are trained and how large is each validation fold?', answer: '4 models are trained, one for each fold. Each validation fold has $400/4 = 100$ samples, and each model trains on the other 300 samples. The final performance estimate is the average of all 4 validation scores, giving a more robust estimate than a single split.', type: 'application', bloomLevel: 'apply', tags: ["k-fold","cross-validation"], order: 5 },
  { id: 'rc-2.7-5', lessonId: '2.7', prompt: 'In PyTorch, you call `loss.backward()` then `optimizer.step()`. What happens in each call?', answer: '`loss.backward()` computes gradients of the loss with respect to all parameters with `requires_grad=True` by running backpropagation through the computation graph. `optimizer.step()` updates the parameters using those gradients according to the optimizer\'s algorithm (e.g., SGD, Adam). You also need `optimizer.zero_grad()` before backward() to clear gradients from the previous step.', type: 'recall', bloomLevel: 'remember', tags: ["pytorch","backward","optimizer"], order: 5 },
  { id: 'rc-2.8-5', lessonId: '2.8', prompt: 'Why can\'t standard NumPy random functions be used inside a `jax.jit`-compiled function?', answer: 'NumPy random functions use hidden global state (a global PRNG seed), which is a side effect. `jax.jit` requires pure functions with no side effects to compile correctly. JAX\'s explicit PRNG key system makes randomness a function argument rather than global state, ensuring reproducibility and compatibility with JIT compilation.', type: 'concept', bloomLevel: 'understand', tags: ["jax","jit","random-state"], order: 5 },
  { id: 'rc-2.9-5', lessonId: '2.9', prompt: 'A dataset has images labeled with multiple tags (e.g., \'beach\', \'sunset\', \'people\'). What output configuration is needed?', answer: 'This is multi-label classification. Use $N$ sigmoid units (one per possible tag) with binary_crossentropy loss. Each sigmoid independently predicts whether that tag applies. Do NOT use softmax -- it assumes exactly one class is correct. The model can predict any combination of tags.', type: 'application', bloomLevel: 'apply', tags: ["multi-label","sigmoid","classification"], order: 5 },
  { id: 'rc-2.10-5', lessonId: '2.10', prompt: 'Why is ML development described as an iterative process rather than a linear pipeline?', answer: 'Because you rarely get a good model on the first try. Each step reveals new information: data exploration reveals quality issues, initial training reveals underfitting/overfitting, evaluation reveals specific failure modes. Each finding sends you back to an earlier step (collect more data, change preprocessing, adjust architecture, add regularization). The workflow is a loop, not a line.', type: 'concept', bloomLevel: 'analyze', tags: ["iterative-development","workflow"], order: 5 },

  // --- Module 3 extra cards (5th card per lesson) ---
  { id: 'rc-3.1-5', lessonId: '3.1', prompt: 'A dataset has 10,000 features but only 500 samples. Why is this model especially prone to overfitting?', answer: 'With far more features than samples, the model can find many spurious correlations -- patterns that are coincidental in the training data but don\'t generalize. In high dimensions, random noise creates apparent structure. This is the \'curse of dimensionality.\' Solutions: reduce features (PCA, feature selection), get more data, or apply strong regularization.', type: 'application', bloomLevel: 'analyze', tags: ["curse-of-dimensionality","overfitting"], order: 5 },
  { id: 'rc-3.2-5', lessonId: '3.2', prompt: 'How does the manifold hypothesis explain why a ConvNet can recognize a cat it has never seen before?', answer: 'The manifold hypothesis says all cat images lie on a low-dimensional manifold in pixel space. During training, the ConvNet learns the structure of this manifold. A new cat image lies on (or near) the same manifold, so the model can recognize it by interpolating between training examples -- not by memorizing specific images, but by learning what the \'cat manifold\' looks like.', type: 'concept', bloomLevel: 'understand', tags: ["manifold-hypothesis","generalization"], order: 5 },
  { id: 'rc-3.3-5', lessonId: '3.3', prompt: 'You repeatedly evaluate on the test set and use the results to adjust your model. Why does your reported test accuracy become unreliable?', answer: 'Each time you use test results to make model decisions, you leak test information into your development process. The test set effectively becomes another validation set. Your reported accuracy is now optimistically biased -- it overestimates real-world performance because the model has been indirectly tuned to that specific test set. This is why test evaluation should be done only once.', type: 'concept', bloomLevel: 'analyze', tags: ["data-leakage","test-set"], order: 5 },
  { id: 'rc-3.4-5', lessonId: '3.4', prompt: 'Your model\'s training loss decreases very slowly despite 50 epochs. Should you add more layers or adjust the learning rate first? Why?', answer: 'Adjust the learning rate first -- it is the single most impactful hyperparameter and the cheapest to change. A too-low learning rate directly causes slow convergence. If increasing the learning rate helps but then diverges, try an adaptive optimizer like Adam. Only add more layers if the model genuinely cannot represent the patterns (training loss plateaus, not just converges slowly).', type: 'application', bloomLevel: 'apply', tags: ["learning-rate","debugging"], order: 5 },
  { id: 'rc-3.5-5', lessonId: '3.5', prompt: 'Dropout rate of 0.5 means half the neurons are randomly turned off during training. Why doesn\'t this destroy the learned features?', answer: 'Because dropout forces redundancy: the network must distribute each concept across multiple neurons rather than relying on a single one. When all neurons are active at test time (with scaled outputs), the distributed representation is more robust and generalizable. Think of it as training an ensemble of many sub-networks that share weights.', type: 'concept', bloomLevel: 'understand', tags: ["dropout","ensemble-effect"], order: 5 },
  { id: 'rc-3.6-5', lessonId: '3.6', prompt: 'You want to build an ML model to predict whether a patient has a rare disease (prevalence 0.1%). What metric should you optimize and why?', answer: 'Optimize recall (sensitivity) -- the percentage of actual disease cases correctly identified. With 0.1% prevalence, accuracy is misleading (99.9% by always predicting \'healthy\'). High recall ensures most sick patients are flagged. Also track precision to manage false alarm rate. AUC-PR (area under precision-recall curve) gives a single balanced metric for imbalanced problems.', type: 'application', bloomLevel: 'apply', tags: ["class-imbalance","recall","precision"], order: 5 },
  { id: 'rc-3.7-5', lessonId: '3.7', prompt: 'What does \'beating a baseline\' prove about your data and model pipeline?', answer: 'It proves three things: (1) the data contains a learnable signal (not just noise), (2) your model architecture can capture at least some of that signal, and (3) your data pipeline (loading, preprocessing, label encoding) is correct. If you can\'t beat a baseline, something fundamental is wrong -- do not proceed to complex models until you diagnose the issue.', type: 'concept', bloomLevel: 'understand', tags: ["baseline","validation"], order: 5 },
  { id: 'rc-3.8-5', lessonId: '3.8', prompt: 'A deployed model\'s accuracy drops from 92% to 78% over six months. The code hasn\'t changed. What is the most likely cause?', answer: 'Distribution shift -- the real-world data has changed since training. Examples: user behavior evolved, new products were introduced, seasonal patterns shifted, or an external event changed the domain. The model was trained on old data that no longer represents current reality. Fix: retrain on recent data, set up monitoring to detect future drift earlier.', type: 'application', bloomLevel: 'analyze', tags: ["distribution-shift","model-degradation"], order: 5 },
  { id: 'rc-3.9-5', lessonId: '3.9', prompt: 'When would you combine L2 regularization with dropout rather than using just one?', answer: 'Combine them when overfitting is severe and a single technique is insufficient. L2 regularization shrinks all weights (preventing any single weight from dominating), while dropout prevents co-adaptation of neurons (forcing distributed representations). They address overfitting through different mechanisms and are complementary. Start with one, add the other if the train-val gap remains large.', type: 'concept', bloomLevel: 'understand', tags: ["l2","dropout","combining-regularization"], order: 5 },
  { id: 'rc-3.10-5', lessonId: '3.10', prompt: 'Summarize the diagnostic mindset: given training and validation curves, how do you decide your next action?', answer: 'Both losses high and flat: check learning rate and data pipeline. Training loss decreasing, validation loss flat or increasing: overfitting -- add regularization, get more data, or reduce capacity. Training loss not decreasing: model too small, learning rate too low, or data issue. Both losses low and close: good fit -- evaluate on test set. Always let the curves guide your next experiment.', type: 'recall', bloomLevel: 'remember', tags: ["diagnostic-mindset","training-curves"], order: 5 },

  // ========== MODULE 6 - NLP & GENERATION ==========


  // --- Lesson 6.1 ---
  { id: 'rc-6.1-1', lessonId: '6.1', prompt: 'What are the three main tokenization strategies for text, and which is the modern standard?', answer: 'Word-level (split on spaces -- struggles with rare/new words), character-level (individual characters -- very long sequences), and subword tokenization (BPE, WordPiece -- splits rare words into common subword units). Subword is the modern standard because it balances vocabulary size with the ability to handle any word, including unseen ones.', type: 'recall', bloomLevel: 'remember', tags: ["tokenization","bpe","wordpiece"], order: 1 },
  { id: 'rc-6.1-2', lessonId: '6.1', prompt: 'Why does subword tokenization handle rare and unseen words better than word-level tokenization?', answer: 'Subword tokenization decomposes rare words into common subword units. For example, "unhappiness" might become ["un", "happiness"]. Even if the full word was never seen during training, the subword pieces carry meaning the model has learned. Word-level tokenization would map the entire unseen word to an UNK token, losing all semantic information.', type: 'concept', bloomLevel: 'understand', tags: ["subword","oov-words"], order: 2 },
  { id: 'rc-6.1-3', lessonId: '6.1', prompt: 'You are building a text classifier for a language with no spaces between words (e.g., Chinese). Which tokenization approach would you use?', answer: 'Subword tokenization (BPE or SentencePiece) because it is language-agnostic -- it learns token boundaries from data rather than relying on whitespace. Character-level tokenization also works but produces very long sequences. Word-level tokenization requires a language-specific segmenter and is fragile.', type: 'application', bloomLevel: 'apply', tags: ["tokenization","multilingual"], order: 3 },
  { id: 'rc-6.1-4', lessonId: '6.1', prompt: 'The text preprocessing pipeline converts raw text to model input in three steps: _____, mapping tokens to _____, and optionally padding/truncating to fixed _____.', answer: 'tokenization (splitting text into tokens); integer indices (via a vocabulary lookup table); length (sequence length)', type: 'cloze', bloomLevel: 'remember', tags: ["text-pipeline","preprocessing"], order: 4 },

  // --- Lesson 6.2 ---
  { id: 'rc-6.2-1', lessonId: '6.2', prompt: 'What is a word embedding, and how does it differ from one-hot encoding?', answer: 'A word embedding maps each token to a dense, low-dimensional vector (e.g., 256 dimensions) where similar words are nearby in vector space. One-hot encoding maps each token to a sparse, high-dimensional vector (vocabulary-sized) where all words are equidistant. Embeddings capture semantic relationships; one-hot encoding does not.', type: 'recall', bloomLevel: 'remember', tags: ["embedding","one-hot"], order: 1 },
  { id: 'rc-6.2-2', lessonId: '6.2', prompt: 'Why might a bag-of-words model outperform a sequence model for certain text classification tasks?', answer: 'Bag-of-words ignores word order but is simpler and faster. For tasks where keyword presence matters more than word order (e.g., topic classification, spam detection), bag-of-words can be sufficient and less prone to overfitting on small datasets. Sequence models shine when word order is semantically important (sentiment, where "not good" differs from "good").', type: 'concept', bloomLevel: 'understand', tags: ["bag-of-words","sequence-models"], order: 2 },
  { id: 'rc-6.2-3', lessonId: '6.2', prompt: 'When would you use pretrained embeddings (Word2Vec, GloVe) instead of training embeddings from scratch?', answer: 'Use pretrained embeddings when your dataset is small -- they provide semantic knowledge learned from billions of words, preventing the embedding layer from overfitting. Train from scratch when you have a large dataset, domain-specific vocabulary (medical, legal), or when the pretrained embeddings do not cover your language or domain well.', type: 'application', bloomLevel: 'apply', tags: ["pretrained-embeddings","transfer-learning"], order: 3 },

  // --- Lesson 6.3 ---
  { id: 'rc-6.3-1', lessonId: '6.3', prompt: 'What does a language model predict, and how is this used for text generation?', answer: 'A language model predicts $p(\\text{next\\_token} | \\text{previous\\_tokens})$ -- the probability distribution over the next token given all preceding tokens. For generation, you sample from this distribution to select the next token, append it, and repeat. This autoregressive process generates text one token at a time.', type: 'recall', bloomLevel: 'remember', tags: ["language-model","autoregressive"], order: 1 },
  { id: 'rc-6.3-2', lessonId: '6.3', prompt: 'Why is next-token prediction considered a form of self-supervised learning?', answer: 'Because the training labels come from the data itself -- no human annotation needed. Given a sentence "The cat sat on the", the target is "mat" -- the actual next word from the corpus. Every position in every text provides a free training example. This enables training on internet-scale data without manual labeling.', type: 'concept', bloomLevel: 'understand', tags: ["self-supervised","language-model"], order: 2 },
  { id: 'rc-6.3-3', lessonId: '6.3', prompt: 'In autoregressive text generation, the model produces one token at a time by repeatedly predicting $p(\\text{next\\_token} | \\text{\\_\\_\\_\\_\\_})$ and sampling from the distribution.', answer: 'all previous tokens (the context / prompt plus all previously generated tokens)', type: 'cloze', bloomLevel: 'remember', tags: ["autoregressive","generation"], order: 3 },

  // --- Lesson 6.4 ---
  { id: 'rc-6.4-1', lessonId: '6.4', prompt: 'Explain the Query-Key-Value mechanism in dot-product attention.', answer: 'Each input token is projected into three vectors: Query (what am I looking for?), Key (what do I contain?), Value (what information do I carry?). Attention scores are computed by dot product of Query with all Keys, scaled and softmaxed to get weights, then used to take a weighted sum of Values. This lets each token selectively attend to relevant tokens.', type: 'recall', bloomLevel: 'remember', tags: ["attention","query-key-value"], order: 1 },
  { id: 'rc-6.4-2', lessonId: '6.4', prompt: 'Why does multi-head attention use multiple attention heads instead of one?', answer: 'Each head can learn to attend to different types of relationships (syntactic, semantic, positional). One head might focus on the previous word, another on the subject of the sentence. Multiple heads in parallel give the model richer, more diverse attention patterns than a single head could capture.', type: 'concept', bloomLevel: 'understand', tags: ["multi-head-attention","transformer"], order: 2 },
  { id: 'rc-6.4-3', lessonId: '6.4', prompt: 'A Transformer encoder block combines four components. Name them in order.', answer: '1. Multi-head self-attention. 2. Residual connection + layer normalization. 3. Feedforward neural network (two Dense layers). 4. Another residual connection + layer normalization. This pattern is repeated $N$ times (typically 6-12) in a Transformer encoder.', type: 'recall', bloomLevel: 'remember', tags: ["transformer-encoder","architecture"], order: 3 },

  // --- Lesson 6.5 ---
  { id: 'rc-6.5-1', lessonId: '6.5', prompt: 'What is causal (masked) attention, and why is it needed in the Transformer decoder?', answer: 'Causal attention masks out future tokens so that position $i$ can only attend to positions $0$ through $i$. This is needed because during generation, future tokens do not exist yet -- the model must predict the next token using only past context. Without masking, the model would "cheat" by looking at the answer during training.', type: 'recall', bloomLevel: 'remember', tags: ["causal-attention","masking"], order: 1 },
  { id: 'rc-6.5-2', lessonId: '6.5', prompt: 'Self-attention is permutation-invariant -- it treats tokens as an unordered set. Why is positional encoding necessary?', answer: 'Without positional encoding, "the cat sat on the mat" and "mat the on sat cat the" would produce identical attention patterns. Positional encoding adds position information to each token embedding so the model knows that "cat" is the 2nd word, not the 5th. This restores order sensitivity to an otherwise order-agnostic mechanism.', type: 'concept', bloomLevel: 'understand', tags: ["positional-encoding","attention"], order: 2 },
  { id: 'rc-6.5-3', lessonId: '6.5', prompt: 'In an encoder-decoder Transformer for translation, what role does cross-attention play?', answer: 'Cross-attention allows each decoder position to attend to all encoder positions. The decoder generates Query vectors from the partially generated translation, while Key and Value vectors come from the encoder output (the source sentence). This enables the decoder to focus on relevant parts of the source when generating each target word.', type: 'concept', bloomLevel: 'understand', tags: ["cross-attention","encoder-decoder"], order: 3 },

  // --- Lesson 6.6 ---
  { id: 'rc-6.6-1', lessonId: '6.6', prompt: 'What is the key difference between BERT-style and GPT-style pretraining?', answer: 'BERT uses masked language modeling (MLM): randomly mask tokens and predict them from surrounding context (bidirectional). GPT uses causal language modeling (CLM): predict the next token from left context only (unidirectional). BERT is suited for understanding tasks (classification, QA), while GPT is suited for generation tasks.', type: 'recall', bloomLevel: 'remember', tags: ["bert","gpt","pretraining"], order: 1 },
  { id: 'rc-6.6-2', lessonId: '6.6', prompt: 'Why does fine-tuning a pretrained Transformer require much less data than training from scratch?', answer: 'The pretrained model has already learned general language representations (syntax, semantics, world knowledge) from massive text data. Fine-tuning only needs to adapt these rich representations to the specific task. This is transfer learning for NLP: the model starts with strong priors rather than random weights, so a few thousand labeled examples often suffice.', type: 'concept', bloomLevel: 'understand', tags: ["fine-tuning","transfer-learning"], order: 2 },
  { id: 'rc-6.6-3', lessonId: '6.6', prompt: 'You need to classify customer reviews into 5 categories. You have 2,000 labeled reviews. Should you train a Transformer from scratch or fine-tune a pretrained one?', answer: 'Fine-tune a pretrained model (e.g., BERT). 2,000 samples is far too few to train a Transformer from scratch (which typically needs millions of examples). A pretrained model already understands language; you only need to train a small classification head on top. This is the standard approach for NLP tasks with limited labeled data.', type: 'application', bloomLevel: 'apply', tags: ["fine-tuning","text-classification"], order: 3 },
  { id: 'rc-6.6-4', lessonId: '6.6', prompt: 'Transformers process all tokens in _____ (unlike RNNs which process sequentially), enabling much faster training on modern hardware.', answer: 'parallel', type: 'cloze', bloomLevel: 'remember', tags: ["transformer","parallelism"], order: 4 },

  // --- Lesson 6.7 ---
  { id: 'rc-6.7-1', lessonId: '6.7', prompt: 'What is the difference between greedy decoding and temperature-based sampling for text generation?', answer: 'Greedy decoding always picks the highest-probability token -- producing deterministic, often repetitive text. Temperature-based sampling scales the logits before softmax: temperature $< 1$ sharpens the distribution (more confident, less diverse), temperature $> 1$ flattens it (more random, more creative). Temperature $= 1$ uses the raw learned probabilities.', type: 'recall', bloomLevel: 'remember', tags: ["greedy-decoding","temperature","sampling"], order: 1 },
  { id: 'rc-6.7-2', lessonId: '6.7', prompt: 'What is top-k sampling, and why is it preferred over pure random sampling?', answer: 'Top-$k$ sampling restricts the next-token choice to the $k$ most likely tokens, then samples among them. Pure random sampling occasionally picks very unlikely tokens, producing incoherent text. Top-$k$ prevents this by eliminating the long tail of improbable tokens while still allowing diversity among the plausible candidates.', type: 'concept', bloomLevel: 'understand', tags: ["top-k","sampling"], order: 2 },
  { id: 'rc-6.7-3', lessonId: '6.7', prompt: 'You generate text with temperature=0.1 and get repetitive output. You then try temperature=2.0 and get incoherent gibberish. What temperature range should you explore?', answer: 'Try values between 0.5 and 1.2. Temperature=0.1 is too low (essentially greedy, favoring the single most likely token repeatedly). Temperature=2.0 is too high (nearly uniform distribution, random token selection). Values around 0.7-1.0 typically balance coherence with creativity. Also consider combining temperature with top-k or top-p for better control.', type: 'application', bloomLevel: 'apply', tags: ["temperature","generation-quality"], order: 3 },

  // --- Lesson 6.8 ---
  { id: 'rc-6.8-1', lessonId: '6.8', prompt: 'What is instruction fine-tuning, and why is it necessary for LLMs?', answer: 'A base LLM trained on next-token prediction can continue text but does not follow instructions. Instruction fine-tuning trains the model on (instruction, desired_response) pairs, teaching it to interpret and execute user instructions. Without it, asking "Summarize this article" might just produce more article-like text rather than a summary.', type: 'recall', bloomLevel: 'remember', tags: ["instruction-tuning","llm"], order: 1 },
  { id: 'rc-6.8-2', lessonId: '6.8', prompt: 'How does LoRA achieve parameter-efficient fine-tuning?', answer: 'LoRA freezes the original weight matrices and adds small trainable low-rank decomposition matrices ($A$ and $B$) such that the weight update is $W + AB$. For a $4096 \\times 4096$ weight matrix with rank 16, LoRA trains only $2 \\times (4096 \\times 16) = 131\\text{K}$ params instead of 16.7M. This is $\\sim 100\\times$ fewer parameters while achieving comparable performance to full fine-tuning.', type: 'concept', bloomLevel: 'understand', tags: ["lora","parameter-efficient"], order: 2 },
  { id: 'rc-6.8-3', lessonId: '6.8', prompt: 'RLHF (Reinforcement Learning from Human Feedback) trains a _____ model on human preference data, then uses it as a reward signal to fine-tune the LLM.', answer: 'reward', type: 'cloze', bloomLevel: 'remember', tags: ["rlhf","reward-model"], order: 3 },

  // --- Lesson 6.9 ---
  { id: 'rc-6.9-1', lessonId: '6.9', prompt: 'What is RAG (Retrieval-Augmented Generation) and what problem does it solve?', answer: 'RAG augments LLM generation with relevant documents retrieved from an external knowledge base. It solves the hallucination and knowledge cutoff problems: instead of relying solely on parametric memory (which may be outdated or wrong), the model generates responses grounded in retrieved, verifiable documents.', type: 'recall', bloomLevel: 'remember', tags: ["rag","retrieval"], order: 1 },
  { id: 'rc-6.9-2', lessonId: '6.9', prompt: 'Why do LLMs hallucinate, and why is this fundamentally hard to fix?', answer: 'LLMs generate text by predicting statistically likely continuations, with no mechanism to distinguish factual memory from plausible fiction. Hallucination is hard to fix because the model has no internal concept of truth -- it produces text that matches training patterns, and confident-sounding false statements are common in its training data. RAG and fact-checking help but do not eliminate the problem.', type: 'concept', bloomLevel: 'understand', tags: ["hallucination","llm-limitations"], order: 2 },
  { id: 'rc-6.9-3', lessonId: '6.9', prompt: 'A multimodal LLM can process both text and images by encoding images into the same _____ space as text tokens, enabling unified reasoning across modalities.', answer: 'embedding (or representation/token)', type: 'cloze', bloomLevel: 'remember', tags: ["multimodal","embedding-space"], order: 3 },

  // --- Lesson 6.10 ---
  { id: 'rc-6.10-1', lessonId: '6.10', prompt: 'What is a latent space in the context of generative models?', answer: 'A latent space is a lower-dimensional, continuous space where each point maps to a generated output (e.g., an image). Similar points produce similar outputs. The encoder compresses input to this space; the decoder reconstructs from it. A well-structured latent space enables interpolation (blending between images) and sampling (generating new images).', type: 'recall', bloomLevel: 'remember', tags: ["latent-space","generative-models"], order: 1 },
  { id: 'rc-6.10-2', lessonId: '6.10', prompt: 'A VAE uses two loss terms. What are they and what does each encourage?', answer: 'Reconstruction loss (e.g., MSE between input and output) ensures the encoder-decoder can faithfully reproduce the input. KL divergence loss penalizes the encoded distribution for deviating from a standard normal distribution, ensuring the latent space is continuous and well-structured for sampling. The balance between them controls generation quality vs. latent space regularity.', type: 'concept', bloomLevel: 'understand', tags: ["vae","loss-function"], order: 2 },
  { id: 'rc-6.10-3', lessonId: '6.10', prompt: 'The reparameterization trick rewrites the sampling step $z = \\mu + \\sigma \\cdot \\epsilon$ (where $\\epsilon \\sim \\mathcal{N}(0,1)$) to enable _____ through the sampling operation.', answer: 'backpropagation (gradient flow)', type: 'cloze', bloomLevel: 'remember', tags: ["reparameterization","vae"], order: 3 },

  // --- Lesson 6.11 ---
  { id: 'rc-6.11-1', lessonId: '6.11', prompt: 'Describe the two processes in a diffusion model: forward and reverse.', answer: 'Forward process: gradually add Gaussian noise to an image over $T$ steps until it becomes pure noise (a fixed, non-learned process). Reverse process: a neural network (typically U-Net) learns to remove one step of noise at a time. Generation starts from pure noise and iteratively denoises to produce a clean image.', type: 'recall', bloomLevel: 'remember', tags: ["diffusion","forward-reverse"], order: 1 },
  { id: 'rc-6.11-2', lessonId: '6.11', prompt: 'Why do diffusion models produce higher quality images than VAEs?', answer: 'Diffusion models decompose generation into many small denoising steps, each of which is a simple task. VAEs must generate the entire image in one pass through the decoder. The iterative refinement allows diffusion models to correct errors at each step, producing sharper, more detailed images. The tradeoff is much slower generation (hundreds of denoising steps).', type: 'concept', bloomLevel: 'understand', tags: ["diffusion","vae","quality"], order: 2 },
  { id: 'rc-6.11-3', lessonId: '6.11', prompt: 'During training, a diffusion model receives a noisy image at step $t$ and learns to predict the _____ that was added, rather than the clean image directly.', answer: 'noise ($\\epsilon$)', type: 'cloze', bloomLevel: 'remember', tags: ["diffusion","noise-prediction"], order: 3 },

  // --- Lesson 6.12 ---
  { id: 'rc-6.12-1', lessonId: '6.12', prompt: 'How does a text prompt guide image generation in a text-to-image diffusion model?', answer: 'A text encoder (like CLIP) converts the prompt into an embedding vector. This vector is injected into the denoising U-Net via cross-attention at each denoising step. The U-Net attends to the text embedding when deciding how to remove noise, steering the generation toward an image that matches the text description.', type: 'recall', bloomLevel: 'remember', tags: ["text-to-image","cross-attention","clip"], order: 1 },
  { id: 'rc-6.12-2', lessonId: '6.12', prompt: 'What is latent diffusion, and why is it faster than pixel-space diffusion?', answer: 'Latent diffusion performs the diffusion process in a compressed latent space (e.g., $64 \\times 64 \\times 4$ instead of $512 \\times 512 \\times 3$). A pretrained encoder compresses images to latent representations; a decoder reconstructs them. Working in the smaller latent space is dramatically faster ($\\sim 50\\times$ fewer values) while maintaining quality, since the latent space captures the essential structure.', type: 'concept', bloomLevel: 'understand', tags: ["latent-diffusion","efficiency"], order: 2 },
  { id: 'rc-6.12-3', lessonId: '6.12', prompt: 'Classifier-free guidance amplifies the text conditioning during generation by interpolating between _____ and _____ predictions, controlled by a guidance scale parameter.', answer: 'unconditional (no text prompt) and conditional (with text prompt)', type: 'cloze', bloomLevel: 'remember', tags: ["classifier-free-guidance","text-to-image"], order: 3 },

  // ========== MODULE 7 - MASTERY ==========

  // --- Lesson 7.1 ---
  { id: 'rc-7.1-1', lessonId: '7.1', prompt: 'What are hyperparameters, and why can\'t they be learned by gradient descent?', answer: 'Hyperparameters are settings that control the training process itself: learning rate, number of layers, units per layer, dropout rate, batch size. They cannot be learned by gradient descent because they define the structure of the optimization problem -- changing the number of layers changes which gradients exist. They are tuned by evaluating models with different settings on a validation set.', type: 'recall', bloomLevel: 'remember', tags: ["hyperparameters","optimization"], order: 1 },
  { id: 'rc-7.1-2', lessonId: '7.1', prompt: 'Why is random search more efficient than grid search for hyperparameter optimization?', answer: 'Not all hyperparameters matter equally. Grid search allocates equal resolution to each dimension, wasting evaluations on unimportant parameters. Random search explores more unique values of the important parameters by chance. Empirically, random search finds good configurations in far fewer evaluations, especially in high-dimensional search spaces.', type: 'concept', bloomLevel: 'understand', tags: ["random-search","grid-search"], order: 2 },
  { id: 'rc-7.1-3', lessonId: '7.1', prompt: 'Why does model ensembling improve predictions even when individual models have similar accuracy?', answer: 'Different models make different errors because they explore different parts of the hypothesis space. Averaging or voting smooths out individual mistakes. Ensembling is most effective when models are diverse (different architectures, initializations, or hyperparameters). Even a simple average of 3-5 models typically improves accuracy by 1-3%.', type: 'concept', bloomLevel: 'understand', tags: ["ensembling","model-diversity"], order: 3 },
  { id: 'rc-7.1-4', lessonId: '7.1', prompt: 'Bayesian optimization uses a _____ model of the objective function to choose the next hyperparameters to evaluate, balancing exploration and exploitation.', answer: 'surrogate (or probabilistic)', type: 'cloze', bloomLevel: 'remember', tags: ["bayesian-optimization","surrogate-model"], order: 4 },

  // --- Lesson 7.2 ---
  { id: 'rc-7.2-1', lessonId: '7.2', prompt: 'In data-parallel training across 4 GPUs with batch_size=64 per GPU, what is the effective batch size and how are gradients combined?', answer: 'Effective batch size $= 4 \\times 64 = 256$. Each GPU processes its local batch, computes local gradients, then all GPUs synchronize by averaging their gradients (all-reduce). One weight update uses gradient information from all 256 samples, producing a more stable estimate than any single GPU\'s gradient.', type: 'application', bloomLevel: 'apply', tags: ["data-parallelism","multi-gpu"], order: 1 },
  { id: 'rc-7.2-2', lessonId: '7.2', prompt: 'When scaling from 1 to N GPUs, why should you typically increase the learning rate?', answer: 'With $N$ GPUs, the effective batch size is $N$ times larger, producing more reliable gradient estimates with less noise. A more reliable gradient supports a larger step size. The common rule of thumb is linear scaling: multiply the learning rate by $N$. Without this adjustment, each individual weight update makes too small a step relative to the larger batch.', type: 'concept', bloomLevel: 'understand', tags: ["learning-rate-scaling","multi-gpu"], order: 2 },
  { id: 'rc-7.2-3', lessonId: '7.2', prompt: 'What is the main bottleneck in multi-GPU training, and how does it affect scalability?', answer: 'Communication overhead: synchronizing gradients across GPUs (all-reduce) takes time proportional to model size. As you add more GPUs, the computation per GPU decreases but communication stays constant or grows. At some point, GPUs spend more time waiting for gradient synchronization than computing. This is why scaling efficiency decreases with more GPUs.', type: 'concept', bloomLevel: 'understand', tags: ["communication-overhead","scalability"], order: 3 },

  // --- Lesson 7.3 ---
  { id: 'rc-7.3-1', lessonId: '7.3', prompt: 'What is mixed-precision training and what is its primary benefit?', answer: 'Mixed-precision training uses float16 for forward/backward computation (fast, memory-efficient) while maintaining float32 master weights (accurate accumulation of small updates). Primary benefit: roughly $2\\times$ speedup and $2\\times$ memory reduction with minimal accuracy loss. Modern GPUs have dedicated float16 hardware (Tensor Cores) that make this especially effective.', type: 'recall', bloomLevel: 'remember', tags: ["mixed-precision","float16"], order: 1 },
  { id: 'rc-7.3-2', lessonId: '7.3', prompt: 'A 7B parameter model in float32 requires 28GB of memory. How much memory does int8 quantization save?', answer: 'INT8 uses 1 byte per parameter instead of 4 bytes (float32). So $7\\text{B} \\times 1\\text{ byte} = 7\\text{GB}$, saving 21GB (75% reduction). This enables running large models on consumer GPUs (e.g., 8GB VRAM). The tradeoff is small accuracy loss from reduced precision, but modern quantization techniques (GPTQ, AWQ) minimize this impact.', type: 'application', bloomLevel: 'apply', tags: ["quantization","memory"], order: 2 },
  { id: 'rc-7.3-3', lessonId: '7.3', prompt: 'Loss scaling in mixed-precision training prevents small gradient values from _____ to zero in float16, which would halt learning.', answer: 'underflowing (rounding down)', type: 'cloze', bloomLevel: 'remember', tags: ["loss-scaling","float16"], order: 3 },

  // --- Lesson 7.4 ---
  { id: 'rc-7.4-1', lessonId: '7.4', prompt: 'What is the difference between local generalization and extreme generalization?', answer: 'Local generalization: handling new inputs that are similar to training data (interpolation on the data manifold). Current DL excels at this. Extreme generalization: adapting to fundamentally novel situations never encountered during training (extrapolation, abstract reasoning). Current DL fails at this. The gap between local and extreme generalization is the fundamental limitation of deep learning.', type: 'concept', bloomLevel: 'understand', tags: ["local-generalization","extreme-generalization"], order: 1 },
  { id: 'rc-7.4-2', lessonId: '7.4', prompt: 'An LLM correctly answers a math problem phrased one way but fails when the same problem is rephrased. What does this reveal about how the model works?', answer: 'It reveals the model relies on pattern matching against training data rather than genuine mathematical reasoning. Different phrasings activate different learned patterns. True understanding would produce consistent answers regardless of surface-level phrasing. This sensitivity to phrasing is evidence that LLMs perform local generalization (matching known patterns) not abstract reasoning.', type: 'concept', bloomLevel: 'analyze', tags: ["phrasing-sensitivity","reasoning"], order: 2 },
  { id: 'rc-7.4-3', lessonId: '7.4', prompt: 'Why is anthropomorphizing AI models (attributing human-like understanding to them) dangerous?', answer: 'It leads to incorrect expectations about capabilities and failure modes. If you think a model \'understands,\' you expect it to handle edge cases, detect its own errors, and reason about novel situations -- which it cannot do. Treating models as sophisticated pattern matchers leads to better engineering: adding guardrails, monitoring for failures, and not trusting outputs blindly.', type: 'concept', bloomLevel: 'understand', tags: ["anthropomorphism","ai-safety"], order: 3 },
  { id: 'rc-7.4-4', lessonId: '7.4', prompt: 'Deep learning models perform _____ generalization (interpolating between training examples) but struggle with _____ generalization (handling fundamentally novel situations).', answer: 'local; extreme', type: 'cloze', bloomLevel: 'remember', tags: ["generalization","limitations"], order: 4 },

  // --- Lesson 7.5 ---
  { id: 'rc-7.5-1', lessonId: '7.5', prompt: 'How does Chollet define intelligence, and how does this differ from what current DL achieves?', answer: 'Intelligence = the ability to efficiently acquire new abstractions and recombine them to solve novel problems. Current DL excels at extracting statistical patterns from data (acquisition) but is poor at flexibly recombining abstractions for novel tasks (recombination). True intelligence adapts to the unknown on the fly; DL interpolates between the known.', type: 'concept', bloomLevel: 'understand', tags: ["intelligence","abstraction"], order: 1 },
  { id: 'rc-7.5-2', lessonId: '7.5', prompt: 'What is the ARC benchmark and why can\'t current AI models solve it?', answer: 'ARC (Abstraction and Reasoning Corpus) presents novel visual puzzles requiring on-the-fly reasoning from a few examples. Each puzzle uses unique rules never seen in training. Current models fail because they rely on pattern matching against training data -- ARC deliberately uses novel rule combinations that cannot be memorized. It tests extreme generalization, not local generalization.', type: 'concept', bloomLevel: 'understand', tags: ["arc","reasoning","benchmark"], order: 2 },
  { id: 'rc-7.5-3', lessonId: '7.5', prompt: 'Program synthesis combines deep learning with discrete _____ to solve tasks that require systematic reasoning rather than pattern matching.', answer: 'search (or program generation)', type: 'cloze', bloomLevel: 'remember', tags: ["program-synthesis","hybrid-ai"], order: 3 },

  // --- Lesson 7.6 ---
  { id: 'rc-7.6-1', lessonId: '7.6', prompt: 'Match each architecture to its primary domain: Dense, ConvNets, RNNs, Transformers.', answer: 'Dense: tabular/structured data. ConvNets: images and spatial data. RNNs (LSTM/GRU): short sequences (though increasingly replaced). Transformers: NLP, long sequences, and increasingly vision (ViT) and multimodal tasks. Transformers are becoming the dominant architecture across domains.', type: 'recall', bloomLevel: 'remember', tags: ["architecture-guide","domain-matching"], order: 1 },
  { id: 'rc-7.6-2', lessonId: '7.6', prompt: 'What is the single most important principle to internalize as an ML practitioner?', answer: 'The tension between fitting training data (optimization) and performing well on unseen data (generalization) is the central challenge of all ML. Every technique -- regularization, evaluation, architecture choice, data augmentation -- serves the goal of achieving good generalization. Understanding this tension guides all practical decisions.', type: 'concept', bloomLevel: 'understand', tags: ["generalization","core-principle"], order: 2 },
  { id: 'rc-7.6-3', lessonId: '7.6', prompt: 'Name three ways to stay current in the fast-evolving field of deep learning.', answer: '1. Follow key conferences (NeurIPS, ICML, ICLR) and arXiv for new research. 2. Practice on Kaggle competitions to build hands-on skills with real data. 3. Engage with the Keras and open-source ML community (GitHub, forums, tutorials). Also: read blog posts from leading researchers, reproduce papers, and continuously experiment with new architectures.', type: 'recall', bloomLevel: 'remember', tags: ["staying-current","continuous-learning"], order: 3 },
  { id: 'rc-7.6-4', lessonId: '7.6', prompt: 'The universal ML workflow can be summarized as: define the _____, develop the _____, deploy and _____.', answer: 'task; model; monitor (maintain)', type: 'cloze', bloomLevel: 'remember', tags: ["workflow","summary"], order: 4 },
];

/** Index review cards by lesson ID for O(1) lookup. */
export const reviewCardsByLesson: Record<string, ReviewCard[]> = {};
for (const card of reviewCards) {
  if (!reviewCardsByLesson[card.lessonId]) {
    reviewCardsByLesson[card.lessonId] = [];
  }
  reviewCardsByLesson[card.lessonId].push(card);
}

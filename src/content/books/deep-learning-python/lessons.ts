/**
 * Lesson definitions for the Deep Learning Learning Platform.
 *
 * 72 lessons across 7 modules, each targeting ~15 minutes of focused learning.
 * Content sourced from "Deep Learning with Python, Third Edition" (Chollet & Watson).
 */

import type { Lesson } from '@/lib/db/schema';

export const lessons: Lesson[] = [
  // ---------------------------------------------------------------------------
  // Module 1: Foundations (Chapters 1-2)
  // ---------------------------------------------------------------------------
  {
    id: '1.1',
    moduleId: 'mod-1',
    title: 'What is AI, ML, and Deep Learning?',
    description: 'Distinguish between AI, ML, and deep learning as nested fields and understand the ML paradigm shift.',
    order: 1,
    sourceSections: ['1.1', '1.2', '1.3'],
    prerequisites: [],
    learningObjectives: [
      'Distinguish between artificial intelligence, machine learning, and deep learning as nested fields',
      'Explain why machine learning replaced symbolic AI for complex, fuzzy problems',
      'Describe the fundamental paradigm shift: ML systems are trained on data rather than explicitly programmed',
    ],
    keyConcepts: [
      'Artificial intelligence: the effort to automate intellectual tasks normally performed by humans',
      'Symbolic AI: handcrafted rules and explicit knowledge databases (dominant 1950s-1980s)',
      'Machine learning: systems that are trained on examples rather than programmed with rules',
      'The ML paradigm: input data + expected output + feedback signal = learned rules',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/1.1.mdx',
  },
  {
    id: '1.2',
    moduleId: 'mod-1',
    title: 'Learning Representations from Data',
    description: 'Understand what representations are, how deep learning learns hierarchical representations, and why depth matters.',
    order: 2,
    sourceSections: ['1.4', '1.5'],
    prerequisites: ['1.1'],
    learningObjectives: [
      'Explain what a "representation" of data is and why finding the right representation matters',
      'Understand that deep learning learns successive layers of increasingly meaningful representations',
      'Grasp why the word "deep" refers to the number of layers, not the depth of understanding',
    ],
    keyConcepts: [
      'Representation: a different way to look at or encode data (e.g., RGB vs. HSV for color images)',
      'Hypothesis space: the predefined set of possible transformations the algorithm searches through',
      'Deep learning: learning successive layers of increasingly meaningful representations',
      'Information distillation: each layer refines the data, making it more useful for the task',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/1.2.mdx',
  },
  {
    id: '1.3',
    moduleId: 'mod-1',
    title: 'How Deep Learning Works (The Big Picture)',
    description: 'Understand the training loop: weights, loss function, optimizer, backpropagation, and self-supervised learning.',
    order: 3,
    sourceSections: ['1.6', '1.7', '1.8'],
    prerequisites: ['1.2'],
    learningObjectives: [
      'Describe the training loop: weights, loss function, optimizer, backpropagation',
      'Explain what makes deep learning uniquely powerful (simplicity, scalability, versatility)',
      'Understand self-supervised learning and how it powers generative AI',
    ],
    keyConcepts: [
      'Weights (parameters): numbers that define what each layer does to its input',
      'Loss function: measures how far the network\'s output is from the expected output',
      'Optimizer / Backpropagation: adjusts weights to reduce the loss',
      'Training loop: repeated cycle of forward pass, loss computation, backward pass, weight update',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/1.3.mdx',
  },
  {
    id: '1.4',
    moduleId: 'mod-1',
    title: 'The Hype Cycle and Promise of AI',
    description: 'Identify concrete achievements of deep learning, understand AI winters, and distinguish hype from reality.',
    order: 4,
    sourceSections: ['1.9', '1.10', '1.11', '1.12'],
    prerequisites: ['1.3'],
    learningObjectives: [
      'Identify concrete achievements of deep learning (and distinguish them from hype)',
      'Understand why current AI is better described as "cognitive automation" than "intelligence"',
      'Appreciate the cyclical history of AI winters and why skepticism is healthy',
    ],
    keyConcepts: [
      'Cognitive automation vs. intelligence: AI encodes human skills, it does not replicate minds',
      'AI winter: periods when funding and interest in AI dramatically decline after hype fails to deliver',
      'Adaptability as the hallmark of intelligence: AI handles trained scenarios, intelligence handles the unexpected',
      'Overhyped claims: near-term AGI, mass unemployment, superintelligence',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/1.4.mdx',
  },
  {
    id: '1.5',
    moduleId: 'mod-1',
    title: 'Your First Neural Network (MNIST)',
    description: 'Walk through an end-to-end neural network: load data, build model, compile, train, evaluate.',
    order: 5,
    sourceSections: ['2.1'],
    prerequisites: ['1.3'],
    learningObjectives: [
      'Walk through an end-to-end neural network workflow: load data, build model, compile, train, evaluate',
      'Understand the roles of Dense layers, activation functions, loss, optimizer, and metrics',
      'Interpret training output: loss, accuracy, train vs. test performance',
    ],
    keyConcepts: [
      'MNIST: 60,000 training + 10,000 test images of handwritten digits (28x28 pixels, grayscale)',
      'Dense (fully connected) layer: every input connects to every output',
      'Softmax activation: outputs a probability distribution over classes (sums to 1)',
      'Compilation: specifying the loss function, optimizer, and metrics',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/1.5.mdx',
  },
  {
    id: '1.6',
    moduleId: 'mod-1',
    title: 'Tensors -- The Data Structures of Deep Learning',
    description: 'Define tensors and their key attributes: rank, shape, and dtype.',
    order: 6,
    sourceSections: ['2.2.1', '2.2.2', '2.2.3', '2.2.4', '2.2.5'],
    prerequisites: ['1.5'],
    learningObjectives: [
      'Define tensors and identify their rank (number of axes/dimensions)',
      'Distinguish between scalars, vectors, matrices, and higher-rank tensors with examples',
      'Identify the three key attributes of a tensor: ndim, shape, and dtype',
    ],
    keyConcepts: [
      'Tensor: a container for numerical data; a generalization of matrices to arbitrary dimensions',
      'Rank (ndim): the number of axes -- scalar=0, vector=1, matrix=2, 3D tensor=3, etc.',
      'Shape: a tuple describing the size along each axis (e.g., (60000, 28, 28))',
      'dtype: the data type of values stored in the tensor (e.g., float32, uint8, int64)',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/1.6.mdx',
  },
  {
    id: '1.7',
    moduleId: 'mod-1',
    title: 'Tensor Operations and Batches',
    description: 'Understand element-wise operations, broadcasting, dot products, and why data is processed in batches.',
    order: 7,
    sourceSections: ['2.2.6', '2.2.7', '2.2.8', '2.3.1', '2.3.2', '2.3.3', '2.3.4'],
    prerequisites: ['1.6'],
    learningObjectives: [
      'Understand element-wise operations, broadcasting, and the tensor (dot) product',
      'Explain what a "batch" is and why deep learning processes data in batches',
      'Perform basic tensor manipulations: slicing, reshaping',
    ],
    keyConcepts: [
      'Element-wise operations: applied independently to each element (e.g., relu, addition)',
      'Broadcasting: how operations between tensors of different shapes are handled',
      'Tensor product (dot product): the fundamental operation connecting layers in a neural network',
      'Batch: a small subset of training data processed together (first axis is the "batch axis")',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/1.7.mdx',
  },
  {
    id: '1.8',
    moduleId: 'mod-1',
    title: 'Gradient-Based Optimization',
    description: 'Understand derivatives, gradients, and stochastic gradient descent step by step.',
    order: 8,
    sourceSections: ['2.4.1', '2.4.2', '2.4.3'],
    prerequisites: ['1.7'],
    learningObjectives: [
      'Understand what a derivative and gradient are (intuitively, not just mathematically)',
      'Explain how stochastic gradient descent (SGD) works step by step',
      'Understand why "mini-batch" SGD is the practical standard',
    ],
    keyConcepts: [
      'Derivative: rate of change of a function; tells you how output changes when you nudge the input',
      'Gradient: the derivative of a tensor operation; a vector pointing in the direction of steepest increase',
      'Gradient descent: iteratively moving weights in the opposite direction of the gradient',
      'Learning rate: the magnitude of each weight update step',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/1.8.mdx',
  },
  {
    id: '1.9',
    moduleId: 'mod-1',
    title: 'Backpropagation and the Chain Rule',
    description: 'Explain backpropagation as the chain rule applied to compute gradients through layers.',
    order: 9,
    sourceSections: ['2.4.4'],
    prerequisites: ['1.8'],
    learningObjectives: [
      'Explain backpropagation as the application of the chain rule to compute gradients through layers',
      'Understand why backpropagation is called "back" propagation (gradients flow from output to input)',
      'Appreciate that automatic differentiation in frameworks handles this math for you',
    ],
    keyConcepts: [
      'Chain rule: the derivative of composed functions f(g(x)) is f\'(g(x)) * g\'(x)',
      'Backpropagation: applying the chain rule to compute the gradient of the loss with respect to all weights',
      'Forward pass: computing the output by passing data through layers sequentially',
      'Backward pass: computing gradients by propagating the loss signal back through the layers',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/1.9.mdx',
  },
  {
    id: '1.10',
    moduleId: 'mod-1',
    title: 'Reimplementing the First Example from Scratch',
    description: 'Trace through the complete training process and connect abstract concepts to the concrete MNIST example.',
    order: 10,
    sourceSections: ['2.5.1', '2.5.2', '2.5.3', '2.5.4', '2.6'],
    prerequisites: ['1.9'],
    learningObjectives: [
      'Trace through the complete training process: forward pass, loss, gradient computation, weight update',
      'Connect the abstract concepts (tensors, gradients, SGD) to the concrete MNIST example',
      'Solidify understanding of the full training loop as preparation for working with frameworks',
    ],
    keyConcepts: [
      'Weight initialization: random starting values for the weight matrices and zero biases',
      'Forward pass: output = relu(dot(input, W) + b) for hidden layer, softmax for output layer',
      'Training step: forward pass -> compute loss -> compute gradients -> update weights',
      'Epoch: one complete pass through the entire training dataset',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/1.10.mdx',
  },

  // ---------------------------------------------------------------------------
  // Module 2: Getting Started (Chapters 3-4)
  // ---------------------------------------------------------------------------
  {
    id: '2.1',
    moduleId: 'mod-2',
    title: 'The Deep Learning Framework Landscape',
    description: 'Understand TensorFlow, PyTorch, JAX, and Keras and how they relate to each other.',
    order: 1,
    sourceSections: ['3.1', '3.2'],
    prerequisites: ['1.10'],
    learningObjectives: [
      'Understand the historical evolution of deep learning frameworks',
      'Explain how TensorFlow, PyTorch, JAX, and Keras relate to each other',
      'Know why Keras serves as a high-level API on top of multiple backends',
    ],
    keyConcepts: [
      'Backend framework: the low-level engine that handles tensor computation and autodiff',
      'Keras: a high-level API that works on top of TF, PyTorch, or JAX',
      'Eager vs. graph execution: running operations immediately vs. compiling a computation graph first',
      'Autodifferentiation: all three backends provide automatic gradient computation',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/2.1.mdx',
  },
  {
    id: '2.2',
    moduleId: 'mod-2',
    title: 'Introduction to Keras -- Layers, Models, and Compilation',
    description: 'Learn that layers are building blocks with weights, and the three-step workflow: build, compile, fit.',
    order: 2,
    sourceSections: ['3.6.1', '3.6.2', '3.6.3', '3.6.4', '3.6.5'],
    prerequisites: ['2.1'],
    learningObjectives: [
      'Understand that layers are the building blocks and they store learned weights',
      'Know the three steps: build model (layers) -> compile (loss, optimizer, metrics) -> fit (train)',
      'Choose appropriate loss functions for different problem types',
    ],
    keyConcepts: [
      'Layer: a data-processing module that stores state (weights) and performs a transformation',
      'Model: a directed acyclic graph of layers (or a sequence of layers)',
      'Compile: specifying loss, optimizer, and metrics before training',
      'Loss function selection: binary_crossentropy, categorical_crossentropy, mse',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/2.2.mdx',
  },
  {
    id: '2.3',
    moduleId: 'mod-2',
    title: 'Training and Validation with fit()',
    description: 'Understand how fit() works, why validation data is essential, and how to use trained models for inference.',
    order: 3,
    sourceSections: ['3.6.6', '3.6.7', '3.6.8'],
    prerequisites: ['2.2'],
    learningObjectives: [
      'Understand how fit() works internally (batching, epoch loop, weight updates)',
      'Know why validation data is essential and how to use it during training',
      'Use a trained model for inference (predictions)',
    ],
    keyConcepts: [
      'fit() internals: for each epoch, iterate over batches, compute loss, compute gradients, update weights',
      'Validation split: holding out a portion of training data to monitor generalization',
      'Validation loss: the key signal for overfitting',
      'model.predict(): using the trained model to make predictions on new data',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/2.3.mdx',
  },
  {
    id: '2.4',
    moduleId: 'mod-2',
    title: 'Binary Classification -- Movie Reviews (IMDb)',
    description: 'Set up a binary classification problem end-to-end and interpret training curves.',
    order: 4,
    sourceSections: ['4.1.1', '4.1.2', '4.1.3', '4.1.4', '4.1.5'],
    prerequisites: ['2.3'],
    learningObjectives: [
      'Set up a binary classification problem end-to-end (data prep through evaluation)',
      'Understand multi-hot encoding for variable-length text data',
      'Interpret training curves to diagnose overfitting',
    ],
    keyConcepts: [
      'IMDb dataset: 25,000 training + 25,000 test movie reviews labeled positive/negative',
      'Multi-hot encoding: representing a review as a vector of 0s and 1s indicating which words appear',
      'Binary crossentropy loss: the standard loss for two-class classification',
      'Sigmoid activation: squashes output to [0, 1] range, interpreted as probability',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/2.4.mdx',
  },
  {
    id: '2.5',
    moduleId: 'mod-2',
    title: 'Multiclass Classification -- Newswires (Reuters)',
    description: 'Extend binary classification to multiclass with softmax and categorical crossentropy.',
    order: 5,
    sourceSections: ['4.2.1', '4.2.2', '4.2.3', '4.2.4', '4.2.5', '4.2.6', '4.2.7', '4.2.8', '4.2.9'],
    prerequisites: ['2.4'],
    learningObjectives: [
      'Extend binary classification to multiclass (46 classes)',
      'Understand softmax activation and categorical crossentropy for multiclass problems',
      'Know why intermediate layers must be large enough to not create an information bottleneck',
    ],
    keyConcepts: [
      'Multiclass classification: assigning one of N (N > 2) possible classes to each input',
      'Softmax: outputs N probabilities that sum to 1',
      'Categorical crossentropy: the loss function for multiclass classification with softmax',
      'Information bottleneck: intermediate layers too small to represent the full class space',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/2.5.mdx',
  },
  {
    id: '2.6',
    moduleId: 'mod-2',
    title: 'Regression -- Predicting House Prices',
    description: 'Set up a regression problem with continuous output, feature normalization, and K-fold cross-validation.',
    order: 6,
    sourceSections: ['4.3.1', '4.3.2', '4.3.3', '4.3.4', '4.3.5', '4.3.6'],
    prerequisites: ['2.5'],
    learningObjectives: [
      'Set up a regression problem where the output is a continuous value',
      'Understand feature normalization and why it matters for neural networks',
      'Use K-fold cross-validation for small datasets',
    ],
    keyConcepts: [
      'Regression: predicting a continuous value rather than a discrete class',
      'Feature normalization: subtracting the mean and dividing by the standard deviation',
      'K-fold cross-validation: splitting data into K folds, training on K-1, validating on the remaining',
      'Mean absolute error (MAE): metric measuring average absolute prediction error',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/2.6.mdx',
  },
  {
    id: '2.7',
    moduleId: 'mod-2',
    title: 'TensorFlow and PyTorch Fundamentals',
    description: 'Write basic tensor operations and compute gradients in TensorFlow and PyTorch.',
    order: 7,
    sourceSections: ['3.3.1', '3.3.2', '3.4.1', '3.4.2'],
    prerequisites: ['2.1'],
    learningObjectives: [
      'Write basic tensor operations in TensorFlow and PyTorch',
      'Understand GradientTape (TF) and autograd (PyTorch) for computing gradients',
      'Compare the two frameworks\' approaches to the same linear classifier task',
    ],
    keyConcepts: [
      'tf.Variable / torch.nn.Parameter: tensors that can be updated during training',
      'tf.GradientTape: TF context manager that records operations for autodiff',
      'torch.autograd: PyTorch\'s automatic differentiation engine',
      'Linear classifier: W*x + b with gradient descent -- the simplest possible model',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/2.7.mdx',
  },
  {
    id: '2.8',
    moduleId: 'mod-2',
    title: 'Introduction to JAX',
    description: 'Understand JAX\'s functional programming approach, key transforms (jit, grad, vmap), and explicit random state.',
    order: 8,
    sourceSections: ['3.5.1', '3.5.2', '3.5.3', '3.5.4', '3.5.5'],
    prerequisites: ['2.7'],
    learningObjectives: [
      'Understand JAX\'s functional programming approach (pure functions, immutable arrays)',
      'Know the key JAX transformations: jit, grad, vmap',
      'Understand JAX\'s unique approach to random number generation',
    ],
    keyConcepts: [
      'Functional paradigm: JAX arrays are immutable; operations return new arrays',
      'jit (just-in-time compilation): compiles Python functions into optimized XLA code',
      'grad: transforms a function into one that computes its gradient',
      'Explicit random state: JAX requires you to pass and split PRNG keys manually',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/2.8.mdx',
  },
  {
    id: '2.9',
    moduleId: 'mod-2',
    title: 'Problem Type Decision Guide',
    description: 'Given a new problem, determine the type and select appropriate activation and loss functions.',
    order: 9,
    sourceSections: ['4.1.7', '4.2.9', '4.3.6'],
    prerequisites: ['2.4', '2.5', '2.6'],
    learningObjectives: [
      'Given a new problem, determine whether it is binary, multiclass, or regression',
      'Select the appropriate last-layer activation and loss function',
      'Know common pitfalls and experiments to try for each problem type',
    ],
    keyConcepts: [
      'Problem type determination: binary, multiclass, multi-label, regression',
      'Activation-loss pairing: sigmoid + binary_crossentropy, softmax + categorical_crossentropy, none + mse',
      'Common mistakes: wrong loss function, information bottleneck, no feature normalization',
      'Experiments: vary number of layers, units per layer, activation functions, epochs',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/2.9.mdx',
  },
  {
    id: '2.10',
    moduleId: 'mod-2',
    title: 'End-to-End Workflow Summary',
    description: 'Execute the full workflow from raw data to trained model to predictions.',
    order: 10,
    sourceSections: ['3.7', '4.4'],
    prerequisites: ['2.9'],
    learningObjectives: [
      'Execute the full workflow from raw data to trained model to predictions',
      'Know the complete checklist: load, preprocess, build, compile, train, evaluate, predict',
      'Identify common patterns across all three problem types studied',
    ],
    keyConcepts: [
      'Universal workflow: load -> preprocess -> build -> compile -> fit -> evaluate -> predict',
      'Data preprocessing: normalize features, encode labels, reshape to match model expectations',
      'Model evaluation: always use held-out test data for final evaluation',
      'Iteration: ML development is iterative -- train, evaluate, adjust, repeat',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/2.10.mdx',
  },

  // ---------------------------------------------------------------------------
  // Module 3: ML Fundamentals (Chapters 5-6)
  // ---------------------------------------------------------------------------
  {
    id: '3.1',
    moduleId: 'mod-3',
    title: 'Generalization -- The Central Challenge',
    description: 'Define the tension between optimization and generalization, and identify sources of overfitting.',
    order: 1,
    sourceSections: ['5.1', '5.1.1'],
    prerequisites: ['2.10'],
    learningObjectives: [
      'Define the tension between optimization and generalization as the core challenge of ML',
      'Identify the three sources of overfitting: noisy data, ambiguous features, rare features/spurious correlations',
      'Distinguish underfitting from overfitting using training and validation curves',
    ],
    keyConcepts: [
      'Optimization vs. generalization: fitting training data vs. performing well on unseen data',
      'Overfitting: model learns training-specific patterns that don\'t generalize',
      'Underfitting: model hasn\'t yet captured all relevant patterns in the data',
      'Spurious correlations: rare features that coincidentally correlate with the target',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/3.1.mdx',
  },
  {
    id: '3.2',
    moduleId: 'mod-3',
    title: 'The Nature of Generalization in Deep Learning',
    description: 'Understand the manifold hypothesis and why deep learning generalizes despite many parameters.',
    order: 2,
    sourceSections: ['5.1.2'],
    prerequisites: ['3.1'],
    learningObjectives: [
      'Understand the manifold hypothesis and why deep learning generalizes despite having many parameters',
      'Grasp the role of the loss landscape and gradient descent as implicit regularizers',
      'Appreciate that deep learning models generalize through learning smooth, structured representations',
    ],
    keyConcepts: [
      'Manifold hypothesis: real-world data lies on low-dimensional manifolds within high-dimensional space',
      'Interpolation vs. extrapolation: models generalize by interpolating between training examples',
      'Implicit regularization: gradient descent and architecture choices act as regularizers',
      'Loss landscape: the surface defined by loss values across all possible weight configurations',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/3.2.mdx',
  },
  {
    id: '3.3',
    moduleId: 'mod-3',
    title: 'Evaluating ML Models',
    description: 'Design proper evaluation strategies using train/validation/test splits and beating baselines.',
    order: 3,
    sourceSections: ['5.2.1', '5.2.2', '5.2.3'],
    prerequisites: ['3.1'],
    learningObjectives: [
      'Design proper evaluation strategies using train/validation/test splits',
      'Understand when to use hold-out validation, K-fold, and iterated K-fold',
      'Know the importance of beating a common-sense baseline before claiming success',
    ],
    keyConcepts: [
      'Three-way split: training (learn), validation (tune), test (final evaluation)',
      'Hold-out validation: simple split, suitable for large datasets',
      'K-fold cross-validation: for small datasets, rotate which fold is validation',
      'Common-sense baseline: a simple non-ML prediction to establish a minimum bar',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/3.3.mdx',
  },
  {
    id: '3.4',
    moduleId: 'mod-3',
    title: 'Improving Model Fit',
    description: 'Diagnose and fix underfitting; learn the strategy of "first overfit, then regularize."',
    order: 4,
    sourceSections: ['5.3.1', '5.3.2', '5.3.3'],
    prerequisites: ['3.3'],
    learningObjectives: [
      'Diagnose and fix underfitting by tuning learning rate, architecture, and model capacity',
      'Know the strategy: first overfit, then regularize',
      'Understand the role of architecture priors (e.g., convolutions for images)',
    ],
    keyConcepts: [
      'Tuning the learning rate: too high = divergence, too low = slow convergence',
      'Architecture priors: using the right layer types encodes useful assumptions',
      'Model capacity: more layers and units = more patterns the model can represent',
      'Strategy: first achieve overfitting, then regularize to close the generalization gap',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/3.4.mdx',
  },
  {
    id: '3.5',
    moduleId: 'mod-3',
    title: 'Improving Generalization (Regularization)',
    description: 'Apply four key regularization techniques: dataset curation, feature engineering, early stopping, explicit regularization.',
    order: 5,
    sourceSections: ['5.4.1', '5.4.2', '5.4.3', '5.4.4'],
    prerequisites: ['3.4'],
    learningObjectives: [
      'Apply four key regularization techniques: dataset curation, feature engineering, early stopping, explicit regularization',
      'Understand dropout, L1/L2 penalties, and how they constrain the model',
      'Use early stopping to find the optimal training duration',
    ],
    keyConcepts: [
      'Dataset curation: improving data quality and quantity to reduce overfitting',
      'Early stopping: halt training when validation loss stops improving',
      'Dropout: randomly setting a fraction of layer outputs to zero during training',
      'L1/L2 regularization: adding a penalty on large weight values to the loss function',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/3.5.mdx',
  },
  {
    id: '3.6',
    moduleId: 'mod-3',
    title: 'Defining the Task (Universal Workflow Part 1)',
    description: 'Frame a real-world problem as an ML task: inputs, outputs, problem type, and success metric.',
    order: 6,
    sourceSections: ['6.1.1', '6.1.2', '6.1.3', '6.1.4'],
    prerequisites: ['3.5'],
    learningObjectives: [
      'Frame a real-world problem as an ML task: inputs, outputs, problem type',
      'Know how to collect and understand a dataset before building any model',
      'Choose an appropriate measure of success for the specific problem',
    ],
    keyConcepts: [
      'Problem framing: what are the inputs? What are the desired outputs? What type of problem?',
      'Dataset collection: enough data, representative data, correctly labeled data',
      'Measure of success: accuracy, precision, recall, AUC, MAE -- depends on the problem',
      'Class imbalance: when some classes have far more examples than others',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/3.6.mdx',
  },
  {
    id: '3.7',
    moduleId: 'mod-3',
    title: 'Developing a Model (Universal Workflow Part 2)',
    description: 'Follow the development workflow: prepare data, beat baseline, scale up, regularize.',
    order: 7,
    sourceSections: ['6.2.1', '6.2.2', '6.2.3', '6.2.4', '6.2.5'],
    prerequisites: ['3.6'],
    learningObjectives: [
      'Follow the develop-a-model workflow: prepare data, choose evaluation protocol, beat baseline, scale up, regularize',
      'Understand why beating a baseline first is critical',
      'Know the difference between overfitting (good sign initially) and the need to regularize',
    ],
    keyConcepts: [
      'Prepare data: convert raw data to tensors, normalize, encode labels',
      'Beat a baseline: prove your model does better than a trivial prediction',
      'Scale up to overfit: increase model capacity until training loss approaches zero',
      'Regularize and tune: add regularization, tune hyperparameters',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/3.7.mdx',
  },
  {
    id: '3.8',
    moduleId: 'mod-3',
    title: 'Deploying Your Model (Universal Workflow Part 3)',
    description: 'Take a trained model to production: export, serve, monitor, and maintain.',
    order: 8,
    sourceSections: ['6.3.1', '6.3.2', '6.3.3', '6.3.4'],
    prerequisites: ['3.7'],
    learningObjectives: [
      'Know the steps to take a trained model to production: export, serve, monitor, maintain',
      'Understand deployment pitfalls: distribution shift, feedback loops, stale models',
      'Communicate model limitations and expectations to stakeholders',
    ],
    keyConcepts: [
      'Inference model: the exported, optimized version of the trained model',
      'Distribution shift: when real-world data differs from training data over time',
      'Monitoring: tracking model performance in production to detect degradation',
      'Model maintenance: retraining or updating as data distributions change',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/3.8.mdx',
  },
  {
    id: '3.9',
    moduleId: 'mod-3',
    title: 'Regularization Techniques Deep Dive',
    description: 'Compare L1 vs. L2 regularization, data augmentation, and building intuition for regularization strength.',
    order: 9,
    sourceSections: ['5.4.4', '5.3'],
    prerequisites: ['3.5'],
    learningObjectives: [
      'Compare L1 vs. L2 regularization and when to use each',
      'Understand data augmentation as a regularization technique',
      'Build intuition for how much regularization to apply',
    ],
    keyConcepts: [
      'L1 regularization: penalty on absolute values; drives some weights to exactly zero (sparsity)',
      'L2 regularization (weight decay): penalty on squared values; shrinks all weights evenly',
      'Data augmentation: generating modified versions of training samples',
      'Combining techniques: dropout + L2 + early stopping + data augmentation',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/3.9.mdx',
  },
  {
    id: '3.10',
    moduleId: 'mod-3',
    title: 'Module 3 Integration -- The Complete ML Practitioner\'s Checklist',
    description: 'Integrate all concepts from Modules 1-3 into a practical workflow checklist.',
    order: 10,
    sourceSections: ['5.5', '6.4'],
    prerequisites: ['3.9'],
    learningObjectives: [
      'Integrate all concepts from Modules 1-3 into a practical workflow checklist',
      'Self-assess readiness for hands-on deep learning projects',
      'Connect theory to practice: every concept maps to a concrete action',
    ],
    keyConcepts: [
      'The complete workflow: define task -> collect data -> build baseline -> develop model -> regularize -> evaluate -> deploy -> monitor',
      'The diagnostic mindset: use training curves to diagnose and fix problems',
      'The experimentation loop: hypothesis -> experiment -> observe -> adjust',
      'Readiness signal: build, train, evaluate, and diagnose simple classifiers and regressors',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/3.10.mdx',
  },

  // ---------------------------------------------------------------------------
  // Module 4: Deep Dive into Practice (Chapters 7-9)
  // ---------------------------------------------------------------------------
  {
    id: '4.1',
    moduleId: 'mod-4',
    title: 'Keras Model-Building APIs -- Sequential and Functional',
    description: 'Know when to use Sequential vs. Functional API and build multi-input/multi-output models.',
    order: 1,
    sourceSections: ['7.1', '7.2.1', '7.2.2'],
    prerequisites: ['3.10'],
    learningObjectives: [
      'Know when to use Sequential vs. Functional API based on model complexity',
      'Build multi-input and multi-output models with the Functional API',
      'Understand that the Functional API creates a DAG of layers',
    ],
    keyConcepts: [
      'Sequential model: a linear stack of layers; simplest API',
      'Functional API: defines models as a graph of layers; supports any topology',
      'Multi-input models: models that take multiple different inputs',
      'Shared layers: the same layer can be used multiple times',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/4.1.mdx',
  },
  {
    id: '4.2',
    moduleId: 'mod-4',
    title: 'Subclassing and Mixing Approaches',
    description: 'Build custom models by subclassing keras.Model and know the tradeoffs.',
    order: 2,
    sourceSections: ['7.2.3', '7.2.4', '7.2.5'],
    prerequisites: ['4.1'],
    learningObjectives: [
      'Build fully custom models by subclassing keras.Model',
      'Know the tradeoffs: flexibility vs. inspectability',
      'Mix Sequential, Functional, and Subclassed components in a single model',
    ],
    keyConcepts: [
      'Model subclassing: define __init__ (create layers) and call() (define forward pass)',
      'Tradeoff: subclassed models are flexible but lose graph inspection',
      'Functional model advantages: inspectable, serializable, can visualize the graph',
      'Right tool for the job: use the simplest API that meets your needs',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/4.2.mdx',
  },
  {
    id: '4.3',
    moduleId: 'mod-4',
    title: 'Callbacks and TensorBoard',
    description: 'Use ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, and TensorBoard.',
    order: 3,
    sourceSections: ['7.3.1', '7.3.2', '7.3.3', '7.3.4'],
    prerequisites: ['4.1'],
    learningObjectives: [
      'Use built-in callbacks: ModelCheckpoint, EarlyStopping, ReduceLROnPlateau',
      'Write custom callbacks for specialized training behavior',
      'Visualize training with TensorBoard',
    ],
    keyConcepts: [
      'Callback: an object that performs actions at various points during training',
      'ModelCheckpoint: saves model weights when validation improves',
      'EarlyStopping: stops training when a metric stops improving',
      'TensorBoard: visualization tool for monitoring training metrics',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/4.3.mdx',
  },
  {
    id: '4.4',
    moduleId: 'mod-4',
    title: 'Custom Training Loops',
    description: 'Write custom training loops for non-standard procedures like GANs or RL.',
    order: 4,
    sourceSections: ['7.4.1', '7.4.2', '7.4.3', '7.4.4', '7.4.5'],
    prerequisites: ['4.3'],
    learningObjectives: [
      'Understand when and why you\'d write a custom training loop instead of using fit()',
      'Know the difference between training mode and inference mode',
      'Write a custom train_step that can be plugged into fit()',
    ],
    keyConcepts: [
      'Custom training loop: full control over gradient computation and weight updates',
      'Training vs. inference mode: dropout and batch norm behave differently',
      'train_step override: customize steps while still using fit()',
      'Low-level metric tracking: manually calling metric.update_state() and metric.result()',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/4.4.mdx',
  },
  {
    id: '4.5',
    moduleId: 'mod-4',
    title: 'Introduction to ConvNets',
    description: 'Understand why ConvNets outperform Dense networks for images: translation invariance and spatial hierarchies.',
    order: 5,
    sourceSections: ['8.1', '8.1.1'],
    prerequisites: ['4.1'],
    learningObjectives: [
      'Understand why ConvNets outperform Dense networks for image tasks',
      'Explain the two key properties: translation invariance and spatial hierarchies',
      'Understand how the convolution operation works (kernels, feature maps, filters)',
    ],
    keyConcepts: [
      'Convolution layer: learns local patterns using small sliding windows (kernels)',
      'Translation invariance: a pattern learned in one location can be recognized anywhere',
      'Spatial hierarchy: early layers learn edges, middle layers textures, deep layers objects',
      'Feature map: the output of a convolution layer',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/4.5.mdx',
  },
  {
    id: '4.6',
    moduleId: 'mod-4',
    title: 'Max Pooling and ConvNet Architecture',
    description: 'Understand max pooling, padding, strides, and the standard Conv -> Pool -> Dense pattern.',
    order: 6,
    sourceSections: ['8.1.2'],
    prerequisites: ['4.5'],
    learningObjectives: [
      'Understand how max pooling downsamples feature maps and why it\'s needed',
      'Explain padding ("valid" vs. "same") and strides',
      'Recognize the standard ConvNet pattern: Conv -> Pool -> Conv -> Pool -> Flatten -> Dense',
    ],
    keyConcepts: [
      'Max pooling: takes the maximum value from each spatial window',
      'Purpose of pooling: reduces spatial dimensions, increases receptive field',
      'Padding: "valid" = no padding, "same" = maintain spatial dimensions',
      'Standard ConvNet pattern: alternating Conv2D and MaxPooling2D, ending with Dense',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/4.6.mdx',
  },
  {
    id: '4.7',
    moduleId: 'mod-4',
    title: 'Training a ConvNet from Scratch on Small Data',
    description: 'Build and train a ConvNet for dogs vs. cats with data augmentation.',
    order: 7,
    sourceSections: ['8.2.1', '8.2.2', '8.2.3', '8.2.4', '8.2.5'],
    prerequisites: ['4.6'],
    learningObjectives: [
      'Build and train a ConvNet for a real image classification problem',
      'Understand why data augmentation is critical for small datasets',
      'Apply data augmentation transformations to prevent overfitting',
    ],
    keyConcepts: [
      'Small data challenge: deep learning typically needs thousands of samples per class',
      'Data augmentation: applying random transformations to training images',
      'Augmentation as regularization: increases effective dataset size',
      'Dropout + augmentation: combining techniques for best generalization',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/4.7.mdx',
  },
  {
    id: '4.8',
    moduleId: 'mod-4',
    title: 'Transfer Learning -- Feature Extraction',
    description: 'Use a pretrained ConvNet as a fixed feature extractor for your own classification task.',
    order: 8,
    sourceSections: ['8.3.1'],
    prerequisites: ['4.7'],
    learningObjectives: [
      'Understand transfer learning and why pretrained models are valuable',
      'Use a pretrained ConvNet as a fixed feature extractor',
      'Know what a "convolutional base" is and how to attach your own classifier',
    ],
    keyConcepts: [
      'Transfer learning: reusing learned representations from one task for another',
      'Pretrained model: a model trained on a large dataset (e.g., ImageNet)',
      'Feature extraction: freezing the convolutional base and training only a new classifier',
      'ImageNet: a massive image dataset; models pretrained on it learn general visual features',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/4.8.mdx',
  },
  {
    id: '4.9',
    moduleId: 'mod-4',
    title: 'Transfer Learning -- Fine-Tuning',
    description: 'Fine-tune a pretrained model by unfreezing top layers with a low learning rate.',
    order: 9,
    sourceSections: ['8.3.2'],
    prerequisites: ['4.8'],
    learningObjectives: [
      'Distinguish fine-tuning from feature extraction',
      'Know the correct fine-tuning procedure: train classifier first, then unfreeze some layers',
      'Understand why you must use a very low learning rate when fine-tuning',
    ],
    keyConcepts: [
      'Fine-tuning: unfreezing some top layers and training them alongside the new classifier',
      'Two-phase training: (1) train classifier with frozen base, (2) unfreeze and retrain',
      'Low learning rate: essential to avoid destroying pretrained representations',
      'Fine-tuning typically yields better accuracy than feature extraction alone',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/4.9.mdx',
  },
  {
    id: '4.10',
    moduleId: 'mod-4',
    title: 'Residual Connections',
    description: 'Understand vanishing gradients and how residual connections solve them.',
    order: 10,
    sourceSections: ['9.1', '9.2'],
    prerequisites: ['4.6'],
    learningObjectives: [
      'Understand the vanishing gradient problem in very deep networks',
      'Explain how residual connections solve it by adding the input back to the output',
      'Implement a residual block and know when to use a projection shortcut',
    ],
    keyConcepts: [
      'Vanishing gradients: gradient signals degrade as they propagate through many layers',
      'Residual connection: output = block(x) + x',
      'Information shortcut: the residual connection preserves information from earlier layers',
      'Projection shortcut: when shapes differ, use a 1x1 Conv to match dimensions',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/4.10.mdx',
  },
  {
    id: '4.11',
    moduleId: 'mod-4',
    title: 'Batch Normalization and Separable Convolutions',
    description: 'Understand batch normalization for stable training and depthwise separable convolutions for efficiency.',
    order: 11,
    sourceSections: ['9.3', '9.4'],
    prerequisites: ['4.10'],
    learningObjectives: [
      'Explain what batch normalization does and why it accelerates training',
      'Understand depthwise separable convolutions and their efficiency advantage',
      'Know where to place batch normalization in a network',
    ],
    keyConcepts: [
      'Batch normalization: normalizes each layer\'s activations to mean=0, std=1 across the batch',
      'BatchNorm benefits: stabilizes training, allows higher learning rates',
      'Depthwise separable convolution: factorizes a standard convolution into depthwise + pointwise',
      'SeparableConv2D: the Keras layer implementing depthwise separable convolutions',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/4.11.mdx',
  },
  {
    id: '4.12',
    moduleId: 'mod-4',
    title: 'Putting It Together -- Building a Mini Xception Model',
    description: 'Combine residual connections, batch normalization, and separable convolutions into a complete architecture.',
    order: 12,
    sourceSections: ['9.5', '9.6'],
    prerequisites: ['4.11'],
    learningObjectives: [
      'Combine residual connections, batch normalization, and separable convolutions',
      'Understand the Xception architecture pattern as a practical example',
      'Know about Vision Transformers (ViT) as the emerging alternative to ConvNets',
    ],
    keyConcepts: [
      'Xception-like architecture: repeated blocks of SeparableConv -> SeparableConv -> MaxPooling with residual connections',
      'Pyramid structure: increasing filters with decreasing spatial dimensions',
      'Ablation studies: testing which components actually contribute to performance',
      'Vision Transformers (ViT): applying the Transformer architecture to image patches',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/4.12.mdx',
  },

  // ---------------------------------------------------------------------------
  // Module 5: Computer Vision & Sequences (Chapters 10-13)
  // ---------------------------------------------------------------------------
  {
    id: '5.1',
    moduleId: 'mod-5',
    title: 'Visualizing Intermediate Activations',
    description: 'Extract and visualize feature maps at each layer to understand what ConvNets learn.',
    order: 1,
    sourceSections: ['10.1'],
    prerequisites: ['4.12'],
    learningObjectives: [
      'Extract and visualize the feature maps at each layer of a trained ConvNet',
      'Interpret what different layers learn at different depths',
      'Understand why early layers show edges and deep layers show abstract patterns',
    ],
    keyConcepts: [
      'Intermediate activations: the output tensors of each layer',
      'Feature map visualization: displaying individual channels as grayscale images',
      'Abstraction gradient: edges -> textures -> parts -> objects',
      'Dead filters: some filters may activate on nothing for a given input',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/5.1.mdx',
  },
  {
    id: '5.2',
    moduleId: 'mod-5',
    title: 'Visualizing ConvNet Filters and Grad-CAM',
    description: 'Use gradient ascent to visualize filters and Grad-CAM to see which regions drive predictions.',
    order: 2,
    sourceSections: ['10.2', '10.3', '10.4'],
    prerequisites: ['5.1'],
    learningObjectives: [
      'Use gradient ascent to visualize what patterns maximally activate each filter',
      'Generate Grad-CAM heatmaps showing which image regions drive a classification decision',
      'Understand latent space visualization for trained ConvNets',
    ],
    keyConcepts: [
      'Filter visualization via gradient ascent: optimize input to maximize filter activation',
      'Grad-CAM: gradient-based class activation mapping',
      'Class activation heatmap: shows which spatial regions most influenced prediction',
      'Model explainability: understanding WHY a model makes specific predictions',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/5.2.mdx',
  },
  {
    id: '5.3',
    moduleId: 'mod-5',
    title: 'Image Segmentation Fundamentals',
    description: 'Distinguish semantic, instance, and panoptic segmentation; understand encoder-decoder architecture.',
    order: 3,
    sourceSections: ['11.1', '11.2'],
    prerequisites: ['4.12'],
    learningObjectives: [
      'Distinguish semantic segmentation, instance segmentation, and panoptic segmentation',
      'Understand the encoder-decoder architecture for segmentation',
      'Know how upsampling restores spatial resolution after encoding',
    ],
    keyConcepts: [
      'Semantic segmentation: classify every pixel into a category',
      'Instance segmentation: distinguish individual instances',
      'Encoder-decoder architecture: encoder reduces dimensions, decoder restores them',
      'Upsampling / transposed convolution: operations that increase spatial dimensions',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/5.3.mdx',
  },
  {
    id: '5.4',
    moduleId: 'mod-5',
    title: 'Segment Anything Model (SAM)',
    description: 'Understand SAM as a promptable segmentation foundation model.',
    order: 4,
    sourceSections: ['11.3'],
    prerequisites: ['5.3'],
    learningObjectives: [
      'Understand how SAM works as a promptable segmentation model',
      'Know the different prompt types: points, boxes, text',
      'Appreciate SAM as an example of a foundation model for vision',
    ],
    keyConcepts: [
      'SAM: a foundation model for image segmentation that responds to prompts',
      'Prompt types: point prompts, box prompts, text prompts',
      'Zero-shot transfer: SAM segments objects it was never specifically trained on',
      'Interactive segmentation: user guides the model with prompts',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/5.4.mdx',
  },
  {
    id: '5.5',
    moduleId: 'mod-5',
    title: 'Object Detection -- Single-Stage vs. Two-Stage',
    description: 'Understand object detection as localization + classification and compare architectures.',
    order: 5,
    sourceSections: ['12.1'],
    prerequisites: ['5.3'],
    learningObjectives: [
      'Understand object detection as localization + classification',
      'Compare two-stage detectors (R-CNN) with single-stage detectors (YOLO, RetinaNet)',
      'Know the tradeoffs between speed and accuracy',
    ],
    keyConcepts: [
      'Object detection: predict bounding boxes AND class labels for all objects',
      'Two-stage detectors: first propose regions, then classify; higher accuracy, slower',
      'Single-stage detectors: predict boxes and classes in one pass; faster',
      'Anchor boxes: predefined boxes that the model refines',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/5.5.mdx',
  },
  {
    id: '5.6',
    moduleId: 'mod-5',
    title: 'Training a YOLO Model',
    description: 'Understand YOLO\'s grid-based detection, multi-component loss, and inference pipeline.',
    order: 6,
    sourceSections: ['12.2', '12.3'],
    prerequisites: ['5.5'],
    learningObjectives: [
      'Understand YOLO\'s grid-based detection approach',
      'Know the components of the YOLO loss: box regression + objectness + classification',
      'Use a pretrained detector for inference on new images',
    ],
    keyConcepts: [
      'YOLO grid: divides the image into SxS grid; each cell predicts bounding boxes',
      'Multi-component loss: box regression, objectness score, and class probability',
      'COCO dataset: the standard object detection benchmark (80 categories)',
      'Inference pipeline: preprocess -> forward pass -> NMS -> output boxes',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/5.6.mdx',
  },
  {
    id: '5.7',
    moduleId: 'mod-5',
    title: 'Timeseries Forecasting Fundamentals',
    description: 'Set up a timeseries problem: windowing, targets, baselines, and temporal splits.',
    order: 7,
    sourceSections: ['13.1', '13.2.1', '13.2.2', '13.2.3'],
    prerequisites: ['3.10'],
    learningObjectives: [
      'Understand timeseries forecasting as predicting future values from past observations',
      'Set up a timeseries problem: windowing, targets, baselines',
      'Know why a common-sense baseline is especially important for timeseries',
    ],
    keyConcepts: [
      'Timeseries forecasting: predicting future values based on historical patterns',
      'Sliding window: using the last N timesteps as input to predict the next value(s)',
      'Common-sense baseline: e.g., predict tomorrow = today',
      'Temporal train/val/test split: must be chronological (no future data leakage)',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/5.7.mdx',
  },
  {
    id: '5.8',
    moduleId: 'mod-5',
    title: '1D Convolutions for Timeseries',
    description: 'Apply Conv1D to timeseries data and understand how it detects local temporal patterns.',
    order: 8,
    sourceSections: ['13.2.4'],
    prerequisites: ['5.7'],
    learningObjectives: [
      'Apply 1D convolutions (Conv1D) to timeseries data',
      'Understand how Conv1D is analogous to Conv2D but operates along the time axis',
      'Compare dense baseline vs. 1D ConvNet for timeseries',
    ],
    keyConcepts: [
      'Conv1D: 1D convolution layer that slides a window along the time axis',
      'Temporal patterns: Conv1D detects local temporal patterns',
      'Conv1D + pooling: similar pattern to image ConvNets but in 1D',
      'Limitations: Conv1D has a limited receptive field for long-range dependencies',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/5.8.mdx',
  },
  {
    id: '5.9',
    moduleId: 'mod-5',
    title: 'Recurrent Neural Networks (RNNs)',
    description: 'Learn how RNNs process sequences with hidden state and compare SimpleRNN, LSTM, and GRU.',
    order: 9,
    sourceSections: ['13.3.1', '13.3.2'],
    prerequisites: ['5.7'],
    learningObjectives: [
      'Understand how RNNs process sequences by maintaining hidden state across timesteps',
      'Distinguish SimpleRNN, LSTM, and GRU and know when to use each',
      'Set up an RNN in Keras with return_sequences for stacking',
    ],
    keyConcepts: [
      'Recurrent loop: output of each timestep feeds into the next',
      'Hidden state: a vector that carries information across timesteps',
      'LSTM: Long Short-Term Memory; uses gates to handle long dependencies',
      'return_sequences: if True, output the full sequence; if False, only the last timestep',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/5.9.mdx',
  },
  {
    id: '5.10',
    moduleId: 'mod-5',
    title: 'Advanced RNN Techniques',
    description: 'Apply recurrent dropout, stack RNN layers, and use bidirectional RNNs.',
    order: 10,
    sourceSections: ['13.3.3', '13.3.4', '13.3.5', '13.3.6'],
    prerequisites: ['5.9'],
    learningObjectives: [
      'Apply recurrent dropout to fight overfitting in RNNs',
      'Stack multiple RNN layers for deeper sequence modeling',
      'Use bidirectional RNNs when the entire sequence is available',
    ],
    keyConcepts: [
      'Recurrent dropout: consistent dropout mask across timesteps',
      'Stacked RNNs: multiple recurrent layers for deeper feature extraction',
      'Bidirectional RNN: processing in both forward and backward directions',
      'When to use bidirectional: when the entire input sequence is available',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/5.10.mdx',
  },
  {
    id: '5.11',
    moduleId: 'mod-5',
    title: 'Timeseries Best Practices and Going Further',
    description: 'Compare Dense, Conv1D, and RNN approaches and preview attention mechanisms.',
    order: 11,
    sourceSections: ['13.4'],
    prerequisites: ['5.10'],
    learningObjectives: [
      'Compare Dense, Conv1D, and RNN approaches for timeseries',
      'Understand the architecture landscape: Transformers and attention for sequences',
      'Know practical tips for production timeseries systems',
    ],
    keyConcepts: [
      'Model comparison: Dense (simple), Conv1D (local patterns), RNN (long-range)',
      'Ensemble approaches: combining models often outperforms any single model',
      'Attention mechanism preview: enables selective focus on relevant timesteps',
      'Feature engineering for timeseries: lags, rolling statistics, seasonal encodings',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/5.11.mdx',
  },
  {
    id: '5.12',
    moduleId: 'mod-5',
    title: 'Module 5 Integration -- Computer Vision and Sequences Recap',
    description: 'Map vision tasks to architectures, map sequence tasks to architectures, and assess readiness.',
    order: 12,
    sourceSections: ['10.5', '13.5'],
    prerequisites: ['5.11'],
    learningObjectives: [
      'Map computer vision tasks to appropriate architectures',
      'Map sequence tasks to appropriate architectures',
      'Self-assess readiness to apply these techniques to new problems',
    ],
    keyConcepts: [
      'Vision task taxonomy: classification, segmentation, detection',
      'Sequence architecture spectrum: Dense, Conv1D, RNN, Transformer',
      'Transfer learning applicability: pretrained models for most vision tasks',
      'Architecture selection: match the model to the data and task',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/5.12.mdx',
  },

  // ---------------------------------------------------------------------------
  // Module 6: NLP & Generation (Chapters 14-17)
  // ---------------------------------------------------------------------------
  {
    id: '6.1',
    moduleId: 'mod-6',
    title: 'Text Preprocessing and Tokenization',
    description: 'Understand the text pipeline: raw text -> tokens -> integers; compare tokenization strategies.',
    order: 1,
    sourceSections: ['14.1', '14.2.1', '14.2.2'],
    prerequisites: ['5.12'],
    learningObjectives: [
      'Understand the text preprocessing pipeline: raw text -> tokens -> integer indices',
      'Compare word-level, character-level, and subword tokenization',
      'Know why subword tokenization (BPE, WordPiece) is the modern standard',
    ],
    keyConcepts: [
      'Tokenization: splitting text into units and mapping each to an integer',
      'Word tokenization: splits on whitespace; struggles with rare/new words',
      'Subword tokenization (BPE): splits rare words into common subword units',
      'TextVectorization layer: Keras layer for tokenization and vocabulary building',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/6.1.mdx',
  },
  {
    id: '6.2',
    moduleId: 'mod-6',
    title: 'Word Embeddings and Sequence Models for Text',
    description: 'Learn about word embeddings, bag-of-words vs. sequence models, and pretrained embeddings.',
    order: 2,
    sourceSections: ['14.3', '14.4', '14.5'],
    prerequisites: ['6.1'],
    learningObjectives: [
      'Understand word embeddings: learned dense vector representations of tokens',
      'Compare bag-of-words models vs. sequence models for text',
      'Know about pretrained embeddings (Word2Vec, GloVe) and when to use them',
    ],
    keyConcepts: [
      'Word embedding: mapping each token to a dense vector where similar words are nearby',
      'Embedding layer: a learnable lookup table for token-to-vector conversion',
      'Bag-of-words: treating text as an unordered set of words',
      'Pretrained embeddings: embeddings trained on huge text corpora',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/6.2.mdx',
  },
  {
    id: '6.3',
    moduleId: 'mod-6',
    title: 'The Language Model Concept',
    description: 'Define language models and understand autoregressive generation.',
    order: 3,
    sourceSections: ['15.1', '15.1.1'],
    prerequisites: ['6.2'],
    learningObjectives: [
      'Define a language model as predicting p(next_token | past_tokens)',
      'Understand autoregressive generation: producing text one token at a time',
      'Build intuition by tracing through a character-level language model',
    ],
    keyConcepts: [
      'Language model: predicts the probability distribution over the next token',
      'Autoregressive generation: repeatedly predicting and appending the next token',
      'Sequence-to-sequence: each input token has a corresponding target (next token)',
      'Softmax output: probability distribution over all possible next tokens',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/6.3.mdx',
  },
  {
    id: '6.4',
    moduleId: 'mod-6',
    title: 'Sequence-to-Sequence Learning and the Transformer Architecture',
    description: 'Understand dot-product attention, multi-head attention, and the Transformer encoder.',
    order: 4,
    sourceSections: ['15.2', '15.3.1', '15.3.2'],
    prerequisites: ['6.3'],
    learningObjectives: [
      'Understand sequence-to-sequence as mapping one sequence to another',
      'Explain dot-product attention: how it enables focusing on relevant parts',
      'Know the structure of a Transformer encoder block',
    ],
    keyConcepts: [
      'Dot-product attention: computing relevance scores via query-key-value mechanism',
      'Query, Key, Value: three projections of input tokens',
      'Multi-head attention: running multiple attention mechanisms in parallel',
      'Transformer encoder: multi-head attention + feedforward + residual + layer norm',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/6.4.mdx',
  },
  {
    id: '6.5',
    moduleId: 'mod-6',
    title: 'Transformer Decoder and Positional Encoding',
    description: 'Understand causal masking, cross-attention, and why positional encoding is necessary.',
    order: 5,
    sourceSections: ['15.3.3', '15.3.4', '15.3.5'],
    prerequisites: ['6.4'],
    learningObjectives: [
      'Understand the Transformer decoder and how it differs from the encoder (causal masking)',
      'Know why positional encoding is necessary and how it works',
      'Trace through a complete encoder-decoder Transformer for translation',
    ],
    keyConcepts: [
      'Causal (masked) attention: each token can only attend to previous tokens',
      'Cross-attention: decoder attends to encoder output',
      'Positional encoding: adding position information to order-agnostic attention',
      'Encoder-decoder architecture: encoder processes source, decoder generates target',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/6.5.mdx',
  },
  {
    id: '6.6',
    moduleId: 'mod-6',
    title: 'Pretrained Transformers and Fine-Tuning for NLP',
    description: 'Understand BERT-style and GPT-style pretraining and how to fine-tune for specific tasks.',
    order: 6,
    sourceSections: ['15.4', '15.5'],
    prerequisites: ['6.5'],
    learningObjectives: [
      'Understand pretraining a Transformer on large text data',
      'Fine-tune a pretrained Transformer for a specific NLP task',
      'Know why Transformers are effective: self-attention, parallel processing',
    ],
    keyConcepts: [
      'BERT-style pretraining: masked language modeling (predict masked tokens)',
      'GPT-style pretraining: causal language modeling (predict next word)',
      'Fine-tuning: adapting a pretrained model to a specific task',
      'Why Transformers work: self-attention captures dependencies at any distance',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/6.6.mdx',
  },
  {
    id: '6.7',
    moduleId: 'mod-6',
    title: 'Training a Mini-GPT -- Text Generation',
    description: 'Build a small GPT-like model and understand sampling strategies: greedy, temperature, top-k, top-p.',
    order: 7,
    sourceSections: ['16.1', '16.2.1', '16.2.2', '16.2.3', '16.2.4'],
    prerequisites: ['6.6'],
    learningObjectives: [
      'Build a small GPT-like model using Transformer decoder blocks',
      'Understand sampling strategies: greedy, temperature, top-k, top-p',
      'Know how temperature controls the randomness of generated text',
    ],
    keyConcepts: [
      'GPT architecture: stack of Transformer decoder blocks with causal masking',
      'Greedy decoding: always pick the highest-probability token',
      'Temperature: scaling logits before softmax to control randomness',
      'Top-k / Top-p sampling: restricting sampling to the most likely tokens',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/6.7.mdx',
  },
  {
    id: '6.8',
    moduleId: 'mod-6',
    title: 'Large Language Models (LLMs) and Fine-Tuning',
    description: 'Understand LLMs, instruction fine-tuning, LoRA, and RLHF.',
    order: 8,
    sourceSections: ['16.3', '16.4'],
    prerequisites: ['6.7'],
    learningObjectives: [
      'Understand how pretrained LLMs are used for text generation',
      'Know what instruction fine-tuning does and why it\'s necessary',
      'Understand LoRA as a parameter-efficient fine-tuning method',
    ],
    keyConcepts: [
      'Pretrained LLM: a large Transformer trained on massive text data',
      'Instruction fine-tuning: training the model to follow instructions',
      'LoRA: fine-tuning only small rank-decomposition matrices',
      'RLHF: using human preferences to improve model behavior',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/6.8.mdx',
  },
  {
    id: '6.9',
    moduleId: 'mod-6',
    title: 'The Frontier of LLMs -- RAG, Reasoning, Multimodal',
    description: 'Understand RAG, multimodal LLMs, reasoning models, and current limitations.',
    order: 9,
    sourceSections: ['16.4.1', '16.4.2', '16.4.3', '16.4.4', '16.5'],
    prerequisites: ['6.8'],
    learningObjectives: [
      'Understand RAG and why LLMs need external knowledge',
      'Know what multimodal LLMs and reasoning models are',
      'Appreciate current limitations and future directions of LLMs',
    ],
    keyConcepts: [
      'RAG: augmenting generation with retrieved external documents',
      'Hallucination: LLMs generating plausible but false information',
      'Multimodal LLMs: models that process both text and images',
      'LLM limitations: knowledge cutoff, hallucination, inability to truly reason',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/6.9.mdx',
  },
  {
    id: '6.10',
    moduleId: 'mod-6',
    title: 'Image Generation -- VAEs',
    description: 'Understand latent spaces, VAE architecture, and the reparameterization trick.',
    order: 10,
    sourceSections: ['17.1.1', '17.1.2', '17.1.3'],
    prerequisites: ['4.12'],
    learningObjectives: [
      'Understand latent spaces for image generation',
      'Know how VAEs work: encoder, sampling, decoder',
      'Understand the VAE loss: reconstruction loss + KL divergence',
    ],
    keyConcepts: [
      'Latent space: a low-dimensional space where nearby points produce similar images',
      'VAE encoder: maps an image to distribution parameters (mean + variance)',
      'Reparameterization trick: enables backpropagation through sampling',
      'KL divergence loss: regularizes the latent space to be well-structured',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/6.10.mdx',
  },
  {
    id: '6.11',
    moduleId: 'mod-6',
    title: 'Diffusion Models',
    description: 'Understand the diffusion process: adding noise, learning to reverse it, and generating images.',
    order: 11,
    sourceSections: ['17.2'],
    prerequisites: ['6.10'],
    learningObjectives: [
      'Understand the diffusion process: gradually adding noise, then learning to reverse it',
      'Know how a denoising model (U-Net) is trained',
      'Understand the generation process: iteratively denoising from pure noise',
    ],
    keyConcepts: [
      'Forward diffusion: progressively adding Gaussian noise until pure noise',
      'Reverse diffusion: a neural network learns to remove one step of noise at a time',
      'U-Net architecture: the denoising neural network',
      'Diffusion schedule: how much noise to add at each step',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/6.11.mdx',
  },
  {
    id: '6.12',
    moduleId: 'mod-6',
    title: 'Text-to-Image Models and Module 6 Recap',
    description: 'Understand text conditioning for image generation and review the complete NLP landscape.',
    order: 12,
    sourceSections: ['17.3'],
    prerequisites: ['6.11'],
    learningObjectives: [
      'Understand how text conditioning enables text-to-image generation',
      'Know the components: text encoder + diffusion model + optional latent space',
      'Review the complete NLP and generation landscape from Module 6',
    ],
    keyConcepts: [
      'Text-to-image: conditioning the diffusion process on a text description',
      'CLIP text encoder: encodes text prompts into vectors that guide generation',
      'Latent diffusion: performing diffusion in a compressed latent space',
      'Classifier-free guidance: amplifying the text prompt effect during generation',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/6.12.mdx',
  },

  // ---------------------------------------------------------------------------
  // Module 7: Mastery (Chapters 18-20)
  // ---------------------------------------------------------------------------
  {
    id: '7.1',
    moduleId: 'mod-7',
    title: 'Hyperparameter Optimization and Model Ensembling',
    description: 'Understand grid search, random search, Bayesian optimization, and model ensembling.',
    order: 1,
    sourceSections: ['18.1.1', '18.1.2'],
    prerequisites: ['6.12'],
    learningObjectives: [
      'Understand hyperparameter optimization strategies: grid, random, Bayesian',
      'Know how model ensembling improves predictions',
      'Apply these techniques to squeeze out final performance gains',
    ],
    keyConcepts: [
      'Hyperparameters: settings not learned by gradient descent',
      'Random search: more efficient than grid for high-dimensional spaces',
      'Bayesian optimization: using past results to choose next hyperparameters',
      'Model ensembling: combining predictions from multiple different models',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/7.1.mdx',
  },
  {
    id: '7.2',
    moduleId: 'mod-7',
    title: 'Scaling Training -- Multi-GPU and TPUs',
    description: 'Understand data parallelism, distributed training, and TPUs.',
    order: 2,
    sourceSections: ['18.2.1', '18.2.2', '18.2.3'],
    prerequisites: ['7.1'],
    learningObjectives: [
      'Understand data parallelism: distributing batches across multiple GPUs',
      'Know the basic concepts of distributed training and gradient synchronization',
      'Understand what TPUs are and when they offer advantages',
    ],
    keyConcepts: [
      'Data parallelism: each GPU processes a different batch, gradients are averaged',
      'Effective batch size: batch_per_GPU * N',
      'TPU: Google\'s custom hardware optimized for tensor operations',
      'Communication overhead: gradient synchronization takes time',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/7.2.mdx',
  },
  {
    id: '7.3',
    moduleId: 'mod-7',
    title: 'Mixed Precision and Quantization',
    description: 'Understand floating-point precision, mixed-precision training, and post-training quantization.',
    order: 3,
    sourceSections: ['18.3'],
    prerequisites: ['7.2'],
    learningObjectives: [
      'Understand floating-point precision levels (float32, float16, bfloat16, int8)',
      'Know how mixed-precision training speeds up computation',
      'Understand quantization for faster inference',
    ],
    keyConcepts: [
      'Mixed precision: compute in float16, keep master weights in float32',
      'Loss scaling: prevents gradient underflow in float16',
      'Quantization: converting weights to lower precision for faster inference',
      'bfloat16: same range as float32 but fewer mantissa bits',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/7.3.mdx',
  },
  {
    id: '7.4',
    moduleId: 'mod-7',
    title: 'Limitations of Deep Learning',
    description: 'Identify deep learning\'s fundamental limitations and avoid anthropomorphizing models.',
    order: 4,
    sourceSections: ['19.1', '19.2'],
    prerequisites: ['7.3'],
    learningObjectives: [
      'Identify deep learning\'s fundamental limitations',
      'Understand local vs. extreme generalization',
      'Avoid the trap of anthropomorphizing ML models',
    ],
    keyConcepts: [
      'Local generalization: handling variations similar to training data',
      'Extreme generalization: adapting to fundamentally novel situations',
      'Sensitivity to phrasing: same question with different wording can produce different answers',
      'Anthropomorphization risk: attributing human-like understanding to pattern-matching',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/7.4.mdx',
  },
  {
    id: '7.5',
    moduleId: 'mod-7',
    title: 'The Future of AI -- Beyond Deep Learning',
    description: 'Explore intelligence as abstraction acquisition, ARC, and program synthesis.',
    order: 5,
    sourceSections: ['19.3', '19.4'],
    prerequisites: ['7.4'],
    learningObjectives: [
      'Understand Chollet\'s vision: intelligence as abstraction acquisition and recombination',
      'Know about the ARC benchmark for measuring general intelligence',
      'Appreciate program synthesis as a complement to deep learning',
    ],
    keyConcepts: [
      'Intelligence = abstraction acquisition + recombination',
      'ARC Prize: benchmark testing on-the-fly reasoning with novel puzzles',
      'Program synthesis: generating discrete programs that solve tasks',
      'Test-time adaptation: models that can learn during inference',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/7.5.mdx',
  },
  {
    id: '7.6',
    moduleId: 'mod-7',
    title: 'Key Concepts in Review and Staying Current',
    description: 'Consolidate key concepts into a mental map and plan for staying current.',
    order: 6,
    sourceSections: ['20.1', '20.2', '20.3', '20.4', '20.5'],
    prerequisites: ['7.5'],
    learningObjectives: [
      'Consolidate the key concepts across the entire book into a mental map',
      'Know the key network architectures and when to use each',
      'Have a plan for staying current in the fast-moving field of AI',
    ],
    keyConcepts: [
      'Architecture guide: Dense (tabular), ConvNets (images), RNNs (short sequences), Transformers (long sequences)',
      'Universal workflow: define task -> develop model -> deploy and monitor',
      'Limitations to remember: DL is interpolation, not reasoning',
      'Staying current: Kaggle competitions, arXiv papers, Keras ecosystem',
    ],
    estimatedMinutes: 15,
    contentPath: 'lessons/7.6.mdx',
  },
];
